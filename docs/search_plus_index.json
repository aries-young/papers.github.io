{"./":{"url":"./","title":"Part Zero. Introduction ","keywords":"","body":"Introduction 常用终端命令 $ gitbook install ./ $ gitbook build ./ .\\algorithm-notes.github.io\\docs $ gitbook serve Katex 语法 Inline math: {% math %}\\int_{-\\infty}^\\infty g(x) dx{% endmath %} Block math: {% math %} \\int_{-\\infty}^\\infty g(x) dx {% endmath %} 正文引用添加方式 > [!note|iconVisibility:hidden] > NOTE > [!tip|iconVisibility:hidden] > TIPS > [!warning|iconVisibility:hidden] > SUPPLEMENT > [!danger|iconVisibility:hidden] > CORE CONTENT "},"【论文】12-in-1.html":{"url":"【论文】12-in-1.html","title":"12-in-1","keywords":"","body":" 【论文】Lu, Jiasen, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multitask vision and language representation learning. （pdf） What is 12-in-1 ? 针对之前的工作，作者认为将每个任务数据集分裂开进行预训练的方法忽略了语言和视觉之间潜在且统一的联系，fine-tuning 后模型输出的结果是只针对某一特定任务的独立模型，即这个模型无法对另外一个任务给出参考。例如，模型学习到的是 “small red varse”，在 VQA 中我们对于问题 “What color is the small vase ? ” 我们回答 “red”，在 referring expression grounding 中我们将 small red varse 定位到红色小花瓶的区域，但是由于上述的割裂我们在 VQA 上微调的模型就无法反过去定位花瓶的图像区域 因此，作者在 ViLBERT 上进行拓展，将许多任务联合在一起进行训练使模型得到来自不同任务源的 grounding supervision，最终实现将四大任务、 12 个数据集集合在一个模型上进行训练的 12-in-1 主要贡献： 系统地分析了不同 V&L 数据集和任务与多任务联合训练的关系，提出 Clean V&L Multi-Task setup，保证在十二合一的训练中不会训练集测试集交叉的情况 提出了一种融合了 12 个训练集在一个模型上的 single multi-task model，并为解决 12-in-1 的大规模训练提出了 dynamic stop-and-go training scheduler，taskdependent input tokens 和 simple hyper-parameter heuristics 三种 training tricks Task-Groups and Datasets Vocab-based VQA： 考虑三个主流数据集 VQAv2，GQA 和 VGQA Image Retrieval： 考虑 COCO 和 Flick30K 针对于该任务组的 caption datasets Referring Expression： 在给定自然语言描述的情况下，定位描述所指的图像区域。在不同数据集中，这种指代性的描述差异很大，可能是简短的名词短语，也有可能是多轮的对话。作者考虑了 RefCOCO(+/g) 的 pharse grounding，Visual17W 的 pointing questions，以及 GuessWhat 中的对话序列 Multi-modal Veriﬁcation： 给定一张或者多张图像以及一段自然语言描述，判断它们之间的匹配是否正确，或预测它们的语义关系。考虑 NLVR2 和 SNLI-VE。在 NLVR2 中给出两张图像和一段描述，要求两张图像都与描述匹配。在 SNLI-VE 中，匹配程度视为一个分类问题，分为 entailment、contradiction 和 neutral，表示图像内容与描述相符、不符和不完全符合 A Clean V&L Multi-Task Setup 如下图所示，许多 V&L 任务之间的数据集会存在重叠情况，如果只是简单的将 12 个数据集揉在一起的话会导致某一任务从其他任务的数据集中习得一些作弊信息，所以作者提出了 clean V&L multi-task setup 保证所有的测试数据都不会再训练或者验证数据中出现 Multi-Task Learning 12-in-1 的结构是这样的，对于每一个任务有一个 head network，然后它们有一个共享的 trunk ViLBERT，共享的 trunk ViLBERT 的参数记为 $\\thetas$，针对所有下游任务 $\\mathcal T$ 的 head network 的参数记为 $\\left{\\theta_t\\right}{t=1}^{\\mathcal T}$，那么整个网络要学的参数就是 $\\thetas\\cup \\left{\\theta_t\\right}{t=1}^{\\mathcal T}$ 作者对 ViLBERT 的预训练方式做出了两个修改： 遮罩图像区域的时候，作者将其他有明显重叠的区域也进行了遮罩（IoU > 0.4），避免其他区域泄露信息 在跨模态对齐预测的时候不对 unmatching caption 的样本进行采样，以此增强 masked multimodal modelling losss Task Token 不同组的任务的处理是不同的，因此作者引入了 task token [TASK~t~]，那么新的 ViLBERT 的输入格式就是 $\\left{IMG,v_q,\\cdots,v_T,CLS,Task_t, w_1,\\cdots,w_T,SEP\\right}$，由此 12-in-1 可以以一种自底向上的方式利用任务信息 Vocab-Based VQA Output 用 $h{IMG}$ 和 $h{CLS}$ 的元素积的结果作为 overall imagequery representation，然后通过一个两层的 MLP 得到预训练回答 $A$ 的得分 \r P_v(A|I,Q)=\\sigma(MLP(h_{IMG}\\odot h_{CLS}))\r VQA 和 VGQA 共享 MLP 和 answer vocabulary，GQA 单独使用一套 Image Retrieval Output 按照如下公式计算图文对的匹配得分 \r Rel(I,Q)=W_i(h_{IMG}\\odot h_{CLS})\r COCO 和 Flickr30K 共享 $W_i$ Referring Expression Output 根据给定的描述对一组 region proposals 进行重排序，将每个图像区域 $i$ 的最终表征 $h_{v_i}$ 通过 $W_r\\in\\mathbb R^{d\\times1}$ 投影计算匹配得分 \r Rel(v_i,Q)=W_rh_{v_i}\r $W_r$ 在所有 referring expression tasks 之间共享 Multi-modal Veriﬁcation Output 以 NLVR2 为例，输入两张图片 $I_0,I_1$ 以及描述 $Q$，我们将该问题是做一个分类问题，可以给出一个关于图文对 $(I_0,Q)$ 和 $(I1,Q)$ 的 embedding，根据该 embedding 做分类预测 \r P_v(C|I_0,I_1,Q)=softmax\\left(MLP\\left(\\left[\\begin{matrix}\r h_{IMG}^0\\odot h_{CLS}^0\\\\ \r h_{IMG}^1\\odot h_{CLS}^1\r \\end{matrix}\\right]\\right)\\right)\r 其中，$[\\cdot]$ 表示 concatenation 对于 SNLI-VE，输入的是一张图片，我们采用另外一个分开的分类器，形式和上面相似 Large-Scale Multitask Training 如此下来，12-in-1 具有 6 个 task head、超过 4.4m 的参数，融合了 12 个数据集，对于这样的网络的训练是十分困难的。除了网络自身的庞大，数据集的不统一也带来了很多问题，例如， VGQA 的一个 epoch 相当于 RefCOCOg 上的 19.8 个 epoch。同样，分开计算的情况下，RefCOCOg 会迭代 5K 次，但是 VQA 要迭代 84K 次。诸如此类在数据集上的不统一都给模型的训练带来了大问题，因此作者提出了如下的一些 training tricks Pretraining ViLBERT 在 Conceptual Caption 数据集上进行预训练，预训练的自监督任务包括上面提到的两点修改 Round-Robin Batch-Level Sampling 根据论文的描述不太清楚，但简单来说就是在开始多任务训练之前在不同任务之间轮转按照一种分 batch level 的采样策略进行采样，这可以使得在一个 multi-task iteration 中既能完成每个任务的前向传播，也能按顺序更新完参数 Dynamic Stop-and-Go（DSG） 如前所述，如果我简单的将迭代次数设置为所有任务中所需的最大值，那么对于 5K 次迭代的任务就会出现过拟合。但是，如果让这些迭代次数不多的任务提前终止又会出现基础网络漂移的问题，有点像刻舟求剑， 对应 RefCOCOg 数据集的任务在 5K 迭代的时候停止，认为这时表现最佳，但是在 84K 次 VQA 迭代完成后，trunk ViLBERT 的参数早不是 5K 迭代时的参数，同样会导致 RefCOCOg 上表现不好 于是作者提出了一种 “走走停停” 的训练方式，我们跟踪任务 $t$ 的验证损失 $s_t$，如果两个 task epoch 之间的涨幅未超过 0.1% 那么我们就视为该 head network 已经收敛，将其设置为 stop mode（freeze 住），进入 stop mode 后每隔 iter-gap $\\Delta$ 个 epoch 之后才根据该任务的损失更新 headwork 和 trunk ViLBERT。同样，反过来，如果某一次跟踪到的 head network 表现下降了 0.5%，我们认为产生基础网络漂移了，需要将其重新加入整个结构体系的训练中，算法如下图所示 Curriculum Learning 作者研究了一下 curriculum 和 anti-curriculum strategies 对模型性能的影响。所谓 curriculum 就是先在收敛最慢的任务组 G1（Vocab-Based VQA）上训练，然后再开始完整的循环多任务预训练；所谓的 anti-curriculum 就是先在收敛最快的任务组 G3（Referring Expression）上进行训练，然后再开始完整的循环多任务训练。作者最后指出不使用 curriculum learning 反而会有更好的表现 Setting Multi-Task Hyperparameters 论文遵循一个简单的设计思想—— “大家好才是真的好”，换言之，每个任务在自己里面尝试超参数组合，而不跑到别的任务上联合寻找不错的超参数组合，这样子大大减少了搜索联合训练超参数的工作负担 Batch Size：各自任务的批大小应该调整来适合自己任务 Warm-up Duration：记 $N$ 表示所有单个任务训练需要的最大迭代次数，作者采用了一个线性的预热，即使用 $\\eta*N$ 次迭代进行预热。预热时间过短，比较复杂的任务性能会有所下降。在实验中，作者设 $\\eta=0.1$ Loss Scaling：不同的任务使用分离的学习率是很重要的，共享的基础模型的学习率设置为所有单个任务学习率的最小值。为了适应每个数据集的可变学习率，作者用任务目标学习率除以基础模型学习率等到一个比例参数，以此比例参数来缩放每个数据集的任务损失 Experiments 12-in-1 通过在 Visual Genome 数据集上带属性损失进行训练 的ResNeXT-152 based Faster RCNN 来获得图像特征；使用 BERT 的权重初始化 ViLBERT；通过线性预热、学习率线性衰减机制和 AdamW 优化，在 8 块 V100 上训练 5 天经过 40K 迭代得到 下表是和一些 SOTA 的比较，可以发现在全部任务上训练出来的 12-in-1 在参数减小的情况下还是取得了和 UNITER 接近的结果（大部分表现超过了 UNITER~base~，比起 UNITER~large~ 来说略微不足） 下表是 12-in-1 自己内部的一些比较 下图展示了 12-in-1 模型对于一些 V&L 任务的处理 同时论文在附录中也给出了一些失败的例子 "},"【论文】Adam.html":{"url":"【论文】Adam.html","title":"Adam","keywords":"","body":" 【论文】Kingma D , Ba J . Adam: A Method for Stochastic Optimization[J]. Computer ence, 2014.（pdf） 论文首次提出了 Adam 算法——基于一阶导数的随机梯度下降算法 Adam 是对 SGD、AdaGrad 和 RMSProp 算法的优化 Adam 结合 AdaGrad 和 RMSProp 两种算法的优点，对梯度的一阶矩估计和二阶矩估计都进行综合考虑，具体算法如下 算法流程， 计算 t 时刻目标函数对 \\theta 的梯度 计算梯度的一阶矩，即前面梯度与当前梯度的平均 计算梯度的二阶矩，即前面梯度与当前梯度平方的平均 对一阶矩 m_t 进行校正，因为 m_0 初始化为 0，会导致 m_t 偏向于 0 对二阶矩 v_t 进行校正，因为 v_0 初始化为 0，会导致 v_t 偏向于 0 更新参数 \\theta_t，注意此时可将 \\frac{\\alpha}{\\sqrt{\\hat v_t}+\\epsilon} 视为更新参数 \\theta_t 的学习率，\\hat m_t 视为更新参数 \\theta_t 的梯度 注意上述算法可以通过改变计算顺序而提高效率，将循环的最后三行替代为下面两条句 \\alpha_t=\\alpha\\cdot\\sqrt{1-\\beta_2^t}/(1-\\beta_1^t) \\\\ \\theta_t\\leftarrow\\theta_{t-1}-\\alpha_t\\cdot m_t / (\\sqrt{v_t}+\\hat\\epsilon) 代码实现 # ADAM # 以 y=x1+2*x2为例 import math import numpy as np def adam(): # 训练集，每个样本有三个分量 x = np.array([(1, 1), (1, 2), (2, 2), (3, 1), (1, 3), (2, 4), (2, 3), (3, 3)]) y = np.array([3, 5, 6, 5, 7, 10, 8, 9]) # 初始化 m, dim = x.shape theta = np.zeros(dim) # 参数 alpha = 0.01 # 学习率 momentum = 0.1 # 冲量 threshold = 0.0001 # 停止迭代的错误阈值 iterations = 3000 # 迭代次数 error = 0 # 初始错误为0 b1 = 0.9 # 算法作者建议的默认值 b2 = 0.999 # 算法作者建议的默认值 e = 0.00000001 #算法作者建议的默认值 mt = np.zeros(dim) vt = np.zeros(dim) for i in range(iterations): j = i % m error = 1 / (2 * m) * np.dot((np.dot(x, theta) - y).T, (np.dot(x, theta) - y)) if abs(error) m_t 和 v_t 的偏差修正 可以将 v_t=\\beta_2\\cdot v_{t-1}+(1-\\beta_2)\\cdot g_t^2 改写为所有时间步上只包含梯度和衰减率的函数，即 v_t=(1-\\beta_2)\\sum^t_{i=1}\\beta_2^{t-i}\\cdot g_i^2 下面我们用数学归纳法简单证明一下 我们知道梯度的真实一阶矩为 E(g_t)，真实的二阶矩为 E(g_t^2)。现在，我们希望知道的是时间步 t 上指数移动均值的期望 E(v_t) 与真实的二阶矩 E(g_t^2) 之间的差异，这样才好对这两个量之间的偏差进行修正 我们可以简单通过下面代码模拟看一下初始值的影响 import numpy as np import matplotlib.pyplot as plt beta = 0.9 num_samples = 100 np.random.seed(0) raw_data = np.random.randint(32, 38, size = num_samples) x_index = np.arange(num_samples) v_ema = [] v_pre = 0 for i, t in enumerate(raw_data): v_t = beta * v_pre + (1 - beta) * t v_ema.append(v_t) v_pre = v_t v_ema_corr = [] for i, t in enumerate(v_ema): v_ema_corr.append(t / (1 - np.power(beta, i + 1))) plt.plot(x_index, raw_data, label='raw_data') # Plot some data on the (implicit) axes. plt.plot(x_index, v_ema, label='v_ema') # etc. plt.plot(x_index, v_ema_corr, label='v_ema_corr') plt.xlabel('time') plt.ylabel('T') plt.title(\"exponential moving average\") plt.legend() plt.savefig('./ema.png') plt.show() 可以看到不经过修正的指数移动平均值在初始阶段阶段的结果与真实的曲线有很大的偏差，但是这种偏差随着步数的增加会越来越小；当然，经过修正的指数移动平均值在开始就可以很好的跟踪真实变化趋势 一阶矩、二阶矩 由前面可知，一阶矩 E(g_t) 即当前梯度 g_t 的期望（估计一阶矩 m_t），由于当前梯度 g_t 是随机采样得到的估计结果，因此更关注它在统计意义上的期望 二阶矩 E(g_t^2) 即当前梯度的平方 g_t^2 的期望（估计二阶矩），考虑它的意义要分四种情况： 当 v_t 很大且 ||m_t|| 很大时，说明梯度大且稳定。v_t 是梯度平方的指数移动均值自然结果为正，当 v_t 很大，说明过往大部分的梯度与当前梯度的绝对值都不会太小。||m_t|| 指的是当前梯度指数移动均值的绝对值，当 ||m_t|| 很大，则说明过往梯度与当前梯度正负抵消的很少，即过往梯度与当前梯度一般是同号。这样，梯度更新相对稳定，过度平滑，可以看成梯度下降在『走大下坡』，梯度下降方向明确 当 v_t 很大而 ||m_t|| 却很小时，则说明过往的大部分梯度和当前梯度的绝对值都很大，但是出现了很多的正负抵消的情况。此时，梯度更新『处于震荡的状态』，一会儿正，一会儿负，但由于 g_t^2 的指数移动均值很大，说明单个梯度的绝对值很大，可能是向下更新到一个局部的波谷，有进行一波反弹 当 ||m_t|| 但是 v_t 却趋于零，这种情况不可能会出现 当 ||m_t|| 趋于零且 v_t 也趋于零时，『可能达到局部最低点，也可能走到一个极度平缓的地方』，此次要避免陷入平原 梯度更新 Adam 更新规则的一个重要特性时其步长的谨慎选择。假设 \\epsilon=0，时间步 t 下参数空间中的有效步长是 \\Delta_t=\\alpha\\cdot\\hat m_t/\\sqrt{\\hat v_t} 这个有效步长有两个明确的上界： 在 (1-\\beta_1)>\\sqrt{1-\\beta_2} 的情况下，有效步长的上确界满足 |\\Delta_t|\\leqslant\\alpha\\cdot(1-\\beta_1)/\\sqrt{1-\\beta_2} 在其他情况下 |\\Delta_t| 第一种情况只有在极稀疏的情况下才会发生：即梯度除了当前时间步不为 0，其余时间步上梯度都为 0；而在不那么稀疏的情况下，有效步长会变得更小 当 (1-\\beta_1)=\\sqrt{1-\\beta_2} 时，我们有 |\\hat m_t/\\hat v_t|，此时我们可以确定出上确界 |\\Delta| 在更通用的场景中，因为 |E(g)/\\sqrt{g^2}|\\leqslant1，我们有 \\hat m_t/\\sqrt{\\hat v_t}\\approx \\pm1，于是每一个时间步的有效步长在参数空间中的量级近似受限于步长因子 \\alpha， 即 |\\Delta_t|。这可以理解为在当前参数值下确定了一个置信域区间，这个置信域区间提供了一些当前梯度估计没有提供的信息。于是，通过其通常便可以提前知道正确的 \\alpha 取值的右边界 对于多数机器学习模型来说，我们知道好的最优状态是在参数空间内的集合域上有着极高的概率，例如，我们可以在参数上有一个先验分布。\\alpha 确定了参数空间内有效步长的上确界，通常也就可以推断出 \\alpha 的正确量级，而最优解也可以从 \\theta_0 开始通过一定数量的跌倒到达这个正确的量级 我们将 \\hat m_t/\\hat v_t 称作信噪比（SNR），如果 SNR 值较小，那么有效步长 \\Delta_t 将接近于 0，目标函数也将收敛到极值。这是十分令人满意的，因为越小的 SNR 意味着就判断 \\hat m_t 的方向是否符合真实梯度方向这点上有着越大的不确定性。例如，SNR 值在最优解附近通常会趋近于 0，这同时也导致了在参数空间中使用更小的有效步长，这类似于一种自动退火的机制 对于不同的梯度范围来说，有效步长 \\Delta_t 是不变的，如果将梯度 g 乘以一个系数 c 进行缩放，那么 \\hat m_t 也要乘以一个一样的系数因子 c，而 \\hat v_t 则乘以系数 c^2，而最终的结果并不产生变化 (c\\cdot\\hat m_t)/(\\sqrt{c^2\\cdot\\hat v_t})=\\hat m_t/\\sqrt{\\hat v_t} Adam 算法的优点 惯性保持 Adam 记录了梯度的一阶矩，即过往梯度与当前梯度的指数移动平均值，是的每一次更新时，上一次更新的梯度与当前更新的梯度不会相差太大，即梯度平滑、稳定的过度，可以适应不稳定的目标函数 环境感知： Adam 记录了梯度的二阶矩，即过往梯度批平方与当前梯度平方的平均，为不同参数产生自适应的学习速率 超参数易控制 \\alpha、\\beta_1、\\beta_2、\\epsilon 具有良好的解释性，且通常无需调整参数或仅需很少的调整 AdaMax 前面我们有 v_t=(1-\\beta_2)\\sum^t_{i=1}\\beta_2^{t-i}\\cdot g_i^2，这可以看成一个关于 g_i 的 L^2 范数，换句话说，权重 \\theta_t 的各维度上的增量是根据该维度上当前和过往梯度的 L^2 范数。我们可以从 L^2 范式的更新规则推广到 L^p 范数的更新规则，但是 p 值越大，推广算法在数值上将变得不稳定。在特例中，作者让 p\\rightarrow\\infty 可以的得到一个极其稳定且简单的算法 在 L^p 范数的情况下，v_t=(1-\\beta_2^p)\\sum^t_{i=1}\\beta_2^{p(t-i)}\\cdot| g_i|^p 于是，我们就得到了算法 2 里面的公式 u_t=max(\\beta_2\\cdot u_{t-1}, |g_t|)，初始化时 u_0=0 在算法 2 里面我们不需要对初始化偏差作出修正。同时，参数更新的范围也有一个更加简洁的界限：|\\Delta_t|\\leqslant\\alpha "},"【论文】AlexNet.html":{"url":"【论文】AlexNet.html","title":"AlexNet","keywords":"","body":"﻿* 【论文】 Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems. 2012.（pdf） 【新颖点】 更深的网络结构 使用层叠的卷积层（卷积 → ReLU → 池化） 使用 Dropout 抑制过拟合 使用数据增强（data augmentation）抑制过拟合 使用 ReLU 替代 sigmoid 网络结构 首先，我们说明一下论文中将网络分为上下两个部分是考虑到对应两块 GPU 用于计算，只有到了特定的网络层之后才需要两块 GPU 进行交互。为了方便理解，我们还是假设使用一块 GPU 进行运算，那么，网络总共有 8 层，其中 5 层卷积（第一和第二卷积层后面连接有 LRN，在此后的网络中有证明 LRN 并非 CNN 中必须包含的层，甚至有些网络加入 LRN 后效果反而降低）、3 层全连接 卷积层 C1： 卷积 → ReLU → 池化 → 归一层，输入为 $224\\times224\\times3$，卷积核的数量为 $96$，对应论文中两片 GPU 就分别计算 48 个核，卷积核大小为 $11\\times11\\times3$，步长为 $4$，padding 为 $0$ 如果按照公式 $wide = (224 + 2 \\times padding - kernel_size) / stride + 1 = 54$ 计算，我们发现 $(224-11)/4$ 并不是整除，那么也就说 kernel 在水平移动时的最后一个感受野并非是 $11\\times11$。不过这个细节在 NetScope 中已经做出了修改，里面的输入被调整为 $227\\times227\\times3$，这样子第一层卷积的结果就是 $55\\times55\\times96$ 池化使用 $3\\times3$ 步长为 $2$ 的池化单元（重叠池化），输出为 $(55-3)/2+1=27$ 局部响应归一化使用的超参数为 $k=2, n=5, \\alpha = 10^{-4},\\beta = 0.75$，输出大小仍为 $27\\times27\\times96$ 卷积层 C2： 卷积 → ReLU → 池化 → 归一层，输入为 $27\\times27\\times96$，卷积核的数量为 $256$，分两组，卷积核大小为 $5\\times5$，步长为 $1$，padding 为 $2$，于是输出为 $(72+2\\times2-5)/1+1=27, 27\\times27\\times256$ 池化使用 $3\\times3$ 步长为 $2$ 的池化单元，输出为 $(27-3)/2+1=13$ 局部响应归一化使用的超参数为 $k=2,n=5,\\alpha=10^{-4},\\beta=0.75$，输出大小仍为 $13\\times13\\times256$ 卷积层 C3： 卷积 → ReLU，输入为 $13\\times13\\times256$，卷积核的数量为 $384$，分两组，卷积核大小为 $3\\times3$，步长为 $1$，padding 为 $1$，于是输出为 $13\\times13\\times384$ 卷积层 C4： 卷积 → ReLU，输入为 $13\\times13\\times384$，卷积核的数量为 $384$，分两组，卷积核大小为 $3\\times3$，步长为 $1$，padding 为 $1$，于是输出为 $13\\times13\\times384$ 卷积层 C5： 卷积 → ReLU → 池化，输入为 $13\\times13\\times384$，卷积核的数量为 $256$，分两组，卷积核大小为 $3\\times3$，步长为 $1$，padding 为 $1$，于是输出为 $13\\times13\\times256$ 池化使用 $3\\times3$ 步长为 $2$ 的池化单元，输出为 $(13-3)/2+1=6$ 全连接层 F6： （卷积）全连接层 → ReLU → Dropout，输入为 $6\\times6\\times256$，『该层有 $4096$ 个卷积核，每个卷积核的大小为 $6\\times6$，由于卷积核的尺寸刚好与输入特征图的尺寸相同，即卷积核中的每个稀疏只与特征图尺寸的一个像素值相乘，一一对应，因此该层又是全连接层。』 卷积后的输出结果为 $4096\\times1\\times1$，即全连接层有 $4096$ 个神经元 全连接层 F7： 全连接层 → ReLU → Dropout，有 $4096$ 个神经元 输出层： 输出 $1000$ 个分类的得分 ReLU 作为激活函数 sigmoid 一个非常大的问题就是梯度饱和，当输入的数值较大或者较小时，其激活记过趋于不变，这样就导致其导数的变化非常的小。尤其在深度网络结构中，反向传播时由于很多很小的 sigmoid 导数累积，最终导致结果趋于 0，权值更新很慢 这对 sigmoid 梯度饱和导致训练收敛慢的问题，AlexNet 引入了 ReLU，ReLU 有以下的好处： 计算开销小，sigmoid 正向传播引入了指数、倒数等预算，而 ReLU 只是线性输出 解决了梯度饱和 ReLU 使一部分神经元的输出为 0，这样就造成了网络的稀疏性，减少了参数的相互依存关系 ReLU 如何实现非线性激活？ 假设我们从矩阵变化的角度来分析，将输入记成矩阵 $A$，输出记成矩阵 $B$，神经网络所做的变化记为 $M$，那么存在如下的关系 $B=M\\cdot A$ 对于 ReLU 来说，由于其实分段的，0 的部分可以看成是神经元没有激活，不同神经元激活或不激活，所以每次流经神经元所对应的变化矩阵是不同的 也就是说，假如样本 $A_1$ 和 $A_2$ 都流经网络，对应的变化矩阵是 $M_1$ 和 $M_2$， 对单个训练样本这样的变化是线性的，但是每个训练样本的线性变化是不一样的，即 $M1$ 和 $M_2$ 是不相同。那么，对于整个训练样本空间来说，就是将 非线性变化 $M$ 拆为若干个线性变化 $M_i$ 的表示，即 $M = M_1\\cdot M_2\\cdots M_n$ 数据增强 AlexNet 中对数据做了以下处理： 随机剪裁，对 $256\\times256$ 的图片进行随机剪裁到 $227\\times227$，然后进行水平翻转 测试的时候，对左上、右上、左下、右下和中间分别做了 5 次剪裁，然后翻转，共 10 次剪裁，之后对结果求平均 对 RGB 空间做 PCA，然后对主成分做了一个 $(0, 0.1)$ 的高斯扰动，结果使错误率又下降了 1% 层叠池化 在 LeNet 中池化不是重叠的，即池化窗口的大小和步长是相等的，但是在 AlexNet 中使用的池化却是重叠的，即每次移动的步长小于池化的窗口长度 局部响应归一化 虽然后来的很多网络都没有使用 LRN，甚至直接表示 LRN 对于网络没有任何提升，但是我们这里还是简单的说一下 LRN 加在 ReLU 的后面，能够增加网络的泛化能力。传统的 tanh 和 sigmoid 激活函数的阈值都是有范围的，但是 ReLU 激活函数得到的值域并没有固定的区间，所以需要对 ReLU 得到的结果进行一个归一化处理，也就是 AlexNet 中指出的 Local Response Normalization 局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象（侧抑制），然后根据论文有如下公式：b_{(x, y)}^i=\\frac{a^i_{(x, y)}}{(k+\\alpha\\sum^{min(N-1, i+n/2)}_{j=max(0, i-n/2)}(a_{x, y}^j)^2)^\\beta} $a^i{(x, y)}$ 代表 ReLU 在第 $i$ 个核的 $(x, y)$ 位置的输出，$n$ 表示 $a^i{(x, y)}$ 的邻居个数，$N$ 表示该核的数量，$b_{(x, y)}^i$ 表示 LRN 的结果 怎么理解这个公式呢？ $a^i_{(x, y)}$ 表述特征图中的一个位置 $[a, b, c, d]$，可以理解成第 $a$ 张图片，在通道 $d$ 下高度为 $b$，宽度为 $c$ 的点 我们称 $a,\\ n/2,\\ k,\\ \\alpha,\\ \\beta$ 分别为 input、depth_radius、bias、alpha、beta，其中除了 input 其他都是自定义的，需要特别指出的是公式中 $\\sum$ 求和是沿着通道方向的，即一个点同方向的前 $n/2$ 个通道（最小为第 $0$ 个通道）和后 $n/2$ 个通道（最大为第 $N-1$ 个通道） 我们用下面的代码演示一下 "},"【论文】APGN.html":{"url":"【论文】APGN.html","title":"APGN","keywords":"","body":" 【论文】Liu D, Qu X, Dong J, et al. Adaptive proposal generation network for temporal sentence localization in videos.（pdf） 最近，bottom-up 框架的工作引起了许多人们的注意，其直接预测每一帧作为时间边界的概率。作者认为这种方式比 top-down 的方式效果要差一些，主要是缺少 segment-level interaction 于是，作者提出了 Adaptive Proposal Generation Network (APGN) 保留 segment-level interaction，同时提高定位的效率。首先，在视频上进行一个 foreground-background classification，然后在分出来的 foreground 上回归产生一些 proposals，接着通过一个 proposal consolidation module 增强生成 proposal 的语义信息，最后按照 top-down 的方式在 proposal 回归得到时间边界 APGN 的整个模型框架如下所示，先生成 query-guided video representation，然后，通过二分类模块生成 foreground frames，在 foreground frames 上回归该帧到 segment 时间边界的距离，由此即产生了一些相互独立的具有粗糙语义信息的 proposal。在 proposal consolidation 中将每个 proposal 对应的 proposal-wise feature 和 位置信息、语义信息结合在一起，构成 proposal graph 中的一个节点，由此处理后的 proposal 将具有更细粒度的信息方便 后续的 localization 具体来说，video encoder 采用预训练好的 C3D 来抽取视频特征，然后使用 self-attention module + Bi-GRU 来学习时序上的特征，最终的 video features 记为 $\\mathbf V=\\left{\\mathbf vt\\right}{t=1}^T\\in \\mathbb R^{T\\times D}$；query encoder 使用 Glove Embedding， 同样使用 self-attention module + Bi-GRU 来建模时序关系，最终的 query features 记为 $\\mathbf Q=\\left{\\mathbf qn\\right}{n=1}^N\\in \\mathbb R^{N\\times D}$ video-query interaction 的操作和 QANet 里面的一样，不在重复介绍，只是将最后的 FFN 换成了 Bi-GRU，即 $\\tilde{\\mathbf V}=\\text{BiGRU}([\\mathbf V;\\mathbf V\\odot\\mathbf A;\\mathbf V\\odot\\mathbf B])$ Proposal Generation 输入 query-guided video features $\\tilde{\\mathbf V}$，先做 foreground-background classification 生成 foreground frames，然后根据每一个 foreground frame $vt$ 通过 regression 生成 $(t, l^t_s,l^t_e)$ 的 proposal，$l^t{s/e}$ 是 $vt$ 到 segment 时间边界的距离，最终的 generated proposal 就可以表示为 $\\left{(t, l^t_s,l^t_e)\\right}^{T{fore}}_{t=1}$ Foreground-background classification 的操作和其他工作区分前景背景的方式一样，就是用一个三层的 MLP 预测 0-1 序列，loss 采用 BCE loss Boundary regression 采用三个一层的1D Conv 输出两个 channels，loss 如下所示 \r \\mathcal L_{reg}=\\frac{1}{T_{fore}}\\sum^{T_{fore}}_{i=1}\\left(1-\\text{IoU}((t, l^t_s,l^t_e),(t, g^t_s,g^t_e))\\right)\r Proposal Consolidation 为了能得到 refined proposal features 更好用于 segment localization，在 proposal consolidation 中先将 proposal feature 和 positional embedding、frame-wise semantic features 融合在一起，然后通过图卷积网络建模 proposals 之间的关系 这么做的理由是：如下图所示，proposal 1 和 proposal 2 具有相同的语义元素 \"blue\" 和 \"hops\"，我们需要对它们的位置关系进行建模以更好的区分两个 proposal，同时也需要 refine 两个 proposal 对应的 feature 以区分 \"second time\" 对于 proposal $(t, l^t_s,l^t_e)$，其生成的 segment 可以表示为 $(t-l^t_s, t+l^t_e)$，在聚合该 segment 所有 frame feature 之前，先对每一个 frame-wise feature $\\tilde{\\mathbf v}_t$ cat 一个位置编码，即 $\\tilde{\\mathbf v}'_t=[\\tilde{\\mathbf v}_t'\\mathbf{emb}_t^{pos0}]\\in\\mathbb R^{1\\times(D+d)}$，这里采用的 position embedding 和 transformer 中采用的一样—— sine and cosine functions 基于上面内容，我们可以得到 $t$-th proposal 对应的 proposal feature $\\mathbf p_t$ \r \\mathbf p_t=\\text{MLP}_2(\\text{Pool}(\\text{MLP}_1([\\tilde{\\mathbf v}'_{t-l^t_s}, \\dots, \\tilde{\\mathbf v}'_{t+l^t_e}])))\r 其中，$\\text{Pool}$ 表示 max-pooling 基于 $\\left{\\mathbf pt\\right}^{T{fore}}{t=1}$ 可以构建 proposal graph，图中所有结点采用全连接的方式，对每一个 $(\\mathbf p_t,\\mathbf p_t')$ 的 proposal-pair edge convolution 可以表示为如下形式 \r \\mathbf e_{t,t'}=\\text{ReLu}(\\mathbf p_t\\boldsymbol\\theta_1+(\\mathbf p_t'-\\mathbf p_t\\boldsymbol\\theta_2))\r 其中，$\\boldsymbol\\theta{1,2}$ 是学习参数 然后根据下面公式更新每一个 proposal feature $\\mathbf p_t\\rightarrow\\hat{\\mathbf p}_t$ \r \\hat{\\mathbf p}_t=\\text{MaxPool}(\\mathbf e_t), \\ \\mathbf e_t =\\left\\{\\mathbf e_{t,t'}\\right\\}^{T_{fore}}_{t=1}\r 图卷积网络有 $k$ 个 stacked graph convolutional layers Localization Head 记 refined proposal features $\\hat{\\mathbf P}=\\left{\\hat{\\mathbf p}t\\right}^{T{fore}}{t=1}$，分别输入到两个 MLP 中预测执行度和时间边界，具体如下所示 \r r_t = \\text{Sigmoid}(\\text{MLP}_3(\\hat{\\mathbf p}_t)) \\\\\r (\\delta^t_s, \\delta^t_e)=\\text{MLP}_4(\\hat{\\mathbf p}_t))\r 在 localization head 中，我们还需要计算两个 loss，一个 $\\mathcal L{align}$ 用于对齐 proposal 和 gt \r \\mathcal L_{align}= -\\frac{1}{T_{fore}}\\sum^{T_{fore}}_{i=1}o_t\\log(r_t)+(1-o_t)\\log(1-r_t)\r 其中，$o_t$ 是每一个 proposal 和 gt segment 的 IoU score 另一个 loss 计算 fine-tuned offsets \r \\mathcal L_{b}= \\frac{1}{T_{fore}}\\sum^{T_{fore}}_{i=1}\\text{SL}_1(\\hat\\delta^t_s - \\delta^t_s) + \\text{SL}_1(\\hat\\delta^t_e - \\delta^t_e)\r 最终 APGN 的完整 loss 如下所示 \r \\mathcal L=\\lambda_1\\mathcal L_{class}+\\lambda_2\\mathcal L_{reg}+\\lambda_3\\mathcal L_{align}+\\lambda_4\\mathcal L_{b}\r "},"【论文】BatchNorm.html":{"url":"【论文】BatchNorm.html","title":"BatchNorm","keywords":"","body":"﻿## BatchNorm主要解决的问题 机器学习领域有一个很重要的基础假设：iid（独立同分布），即训练数据和测试数据独立且服从同一分布 但是这一点并不符合真实的实践情况，BatchNorm 指出了下面两种问题： 『Internal Convariate Shift』 这个术语主要描述的是：在每一次迭代更新之后，上一层网络的输出数据经过这一层网络计算之后，数据的分布会发生变化，为下一层网络的学习带来了困难（神经网络本来就是学习数据的分布，要是分布一直在变，学习就很难了），这个现象我们就称为 Internal Covarirate Shift 接着我们还有 『Covariate Shift』 的概念，这个概念和 Internal Covariate Shift 有相似性，但是不是一个内容。Internal Covariate Shift 发生在神经网络内部，而 Covariate Shift 主要发生在输入数据上。Covariate Shift 主要描述的是：由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响。而我们常用的方法是归一化和白化 有了上面两个概念我们就更清楚为什么要 BatchNorm 了，BatchNorm 简单说来就是一种归一化手段，直观上的说，这种手段会减小图像之间的绝对差异，突出相对差异，加快训练速度 BatchNorm 原理 因为 Internal Covariate Shift 的存在，深层神经网络在做非线性变换前的激活输入值随着网络深度增加或在训练过程中，其分布逐渐发生偏移或变动。这些偏移或变动导致非线性函数的取值更加靠近区间的两端，这就导致训练训练收敛变慢，或者出现梯度消失 BatchNorm 为了解决这个问题，强行将越来越偏的分布来回比较标准的分布，这样就使得激活函数输入值落在一个比较敏感的区域，这样即使输入是一个小的变化也会导致损失函数产生一个较大的变动，于是学习又能加速收敛，大大提高了训练速度 前三步的作用很明显就是把输入数据分布归一化为一个正态分布。但是，由于神经网络强大的表达能力就是基于它的高度非线性化，如果失去了这种特性，网络再深也没有用。因此，作者加入了第四步，对正态分布进行一个偏移，从某种意义上来说，就是在线性换个非线性之间做一个权衡，$\\gamma$ 和 $\\beta$ 是两个超参数通过训练得到 BatchNorm 的反向传播 我们给出如下的一个链式法则说明 BatchNorm 的推理过程（Inference） BatchNorm 在训练的时候可以根据 minibatch 里的若干训练样本进行激活数值调整，但是在推理的过程中，很明显输入就只有一个实例，于是 BatchNorm 在训练时和测试时就需要采取不同的行为 既然在推理的时候没有从 minibatch 得到的统计量，我们就用所有训练实例中获得的统计量来进行替代。获取所有训练实例的统计量是很简单的，因为每次做 minibatch 训练的时候都会计算一个小批次 $m$ 个样本的均值和方差，现在需要全局的统计量，只要吧每个 minibatch 的均值和方差都记录下来，然后对这些句子和方差计算对应的数学期望就可以得到全局的统计量，即 E(x)\\leftarrow E_B[\\mu_B]\\\\ var[x]\\leftarrow\\frac{m}{m-1}E_B[\\sigma^2_B] 有了均值和方差，每个隐藏层神经元也有已经有了对应训练好的 scaling 和 shift 参数，就可以在推理的时候对每个神经元的激活数据计算 BatchNorm，在推理过程中，BatchNorm 采用如下的计算方式 y=\\frac{\\gamma}{\\sqrt{Var[x]+\\varepsilon}}\\cdot x+(\\beta-\\frac{\\gamma\\cdot E[x]}{\\sqrt{Var[x]+\\varepsilon}}) 在推理时，我们使用 $\\hat x=\\frac{x-E[x]}{\\sqrt{Var[x]+\\varepsilon}}$ ，代入 $y=\\gamma x+\\beta$ 就可以得到上面的结果 下面是完整的 BatchNorm 网络训练算法 代码实现 训练过程 def Batchnorm_simple_for_train(x, gamma, beta, bn_param): \"\"\" param:x : 输入数据，设shape(B,L) param:gama : 缩放因子 γ param:beta : 平移因子 β param: bn_param : batchnorm所需要的一些参数 eps : 接近0的数，防止分母出现0 momentum : 动量参数，一般为0.9， 0.99， 0.999 running_mean ：滑动平均的方式计算新的均值，训练时计算，为测试数据做准备 running_var : 滑动平均的方式计算新的方差，训练时计算，为测试数据做准备 \"\"\" running_mean = bn_param['running_mean'] #shape = [B] running_var = bn_param['running_var'] #shape = [B] momentun = bn_param['momentun'] #shape = [B] results = 0. # 建立一个新的变量 x_mean=x.mean(axis=0) # 计算x的均值 x_var=x.var(axis=0) # 计算方差 running_mean = momentum * running_mean + (1 - momentum) * x_mean running_var = momentum * running_var + (1 - momentum) * x_var x_normalized=(x - running_mean)/np.sqrt(running_var + eps) # 归一化 results = gamma * x_normalized + beta # 缩放平移 #记录新的值 bn_param['running_mean'] = running_mean bn_param['running_var'] = running_var return results , bn_param 如论文所提，我们在计算统计量时采用了滑动平均的方式（Using moving averages instead, we can track the accuracy of a model as it trains），滑动平均即指数加权平均 测试过程 def Batchnorm_simple_for_test(x, gamma, beta, bn_param): \"\"\" param:x : 输入数据，设shape(B,L) param:gama : 缩放因子 γ param:beta : 平移因子 β param: bn_param : batchnorm所需要的一些参数 eps : 接近0的数，防止分母出现0 momentum : 动量参数，一般为0.9， 0.99， 0.999 running_mean ：滑动平均的方式计算新的均值，训练时计算，为测试数据做准备 running_var : 滑动平均的方式计算新的方差，训练时计算，为测试数据做准备 \"\"\" running_mean = bn_param['running_mean'] #shape = [B] running_var = bn_param['running_var'] #shape = [B] results = 0. # 建立一个新的变量 x_normalized=(x-running_mean )/np.sqrt(running_var +eps) # 归一化 results = gamma * x_normalized + beta # 缩放平移 return results , bn_param BatchNorm 的缺点 BatchNorm 依赖于 minibatch 的大小，当 batch_size 很小的时候计算均值和方差很不稳定。有研究表明对于 ResNet 类模型在 ImageNet 数据集上，当 batch_size 从 16 降低到 8 时开始有非常明显的性能下降，在训练过程中计算的均值和方差不准确，而在测试的时候使用的就是训练过程中保持下来的这些均值和方差 由于这一个特性，导致batch normalization不适合以下的几种场景， batch_size 非常小，比如训练资源有限无法应用较大的 minibatch，也比如在线学习等使用单例进行模型参数更新的场景 RNN，因为它是一个动态的网络结构，同一个 minibatch 中训练实例有长有短，导致每一个时间步长必须维持各自的统计量，这就使得 BN 并不能正确发挥效果。在 RNN 中，对 BN 进行改进也非常的困难 "},"【论文】BERT.html":{"url":"【论文】BERT.html","title":"BERT","keywords":"","body":" 【论文】acob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova.2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. （pdf） Why is BERT 以往的预训练模型的结构会受到单向语言模型（从左到右或者从右到左）的限制，因而也限制了模型的表征能力，使其只能获取单方面的上下文信息 BERT（Bidirectional Encoder Representation from Transformers） 不再采用以往这种单向语言模型（或把两个单向语言模型进行浅层拼接）进行预训练，而是采用新的 masked language model（MLM）来生成深度的双向语言特征 BERT 具有以下主要的优点： 采用 MLM 对双向的 transformer 进行预训练，以生成深层的双向语言表征 预训练后，只需要添加一个额外的输出层进行 fine-tuning，就可以在各种各样的下游任务中取得不错的表现。在这个过程中并不需要对 BERT 进行任务特定的结构修改 什么叫双向 transformer？ 双向表示模型在处理一个词的时候，能够同时利用前面的词和后面的词两部分信息。它不是在给出前面词的条件下预测最可能的当前‘词，而是随机遮掩一些词，并利用所有没有被遮掉的词进行预测（如下图所示） 单向的 transformer 一般被称为 transformer decoder，每个token 只会注意到其左边的 token，而双向的 transformer 则被称为 transformer encoder ，每一个 token 会被所有的 token 注意到 所以，BERT 丢弃了 transformer 的 decoder，选择 encoder 作为基础集成单元。论文中介绍了两种版本： BERT BASE：与OpenAI Transformer的尺寸相当，以便比较性能 BERT LARG：一个非常庞大的模型，它完成了本文介绍的最先进的结果 两个 BERT 模型的结构都比 transformer 更深，许多配置参数也更大： BASE 包含 12 层 encoder block，768 个隐藏层神经元，12 个 attention heads LARGE 包含 24 层 encoder block，1024 个隐藏层神经元，16 个 attention heads BERT 的结构 下面是 BERT 的完整结构 BERT 的输入 BERT 与 transformer 的编码方式一样。将固定长度的字符串作为输入，数据由下而上传递计算，每一层都用到了 self attention，并通过前馈神经网络传递其结果，将其交给下一个编码器（通过上图我们还可以知道，BERT 的最大输入序列长度为 512） BERT 的输入为每一个 token 对应的表征，Fig.1 中粉色块就表示 token，黄色块是 token 对应的表征。另外，为了完成具体的分类任务，作者在输入的每一个序列开头都插入了特定的 classification token（[CLS]），该 token 对应的最后的输出表征会被用来起到聚集整个序列表征讯息的作用 这里插一句 NLP 有四大类任务： 序列标注：如分词、实体识别、语义标注 分类任务：如文本分类、情感计算 句子关系判断：如 entailment、QA、自然语言推理 生成式任务：如机器翻译、文本摘要抽取 由于 BERT 是一个预训练模型，因此模型所输入的序列必须有能力包含一句话（e.g. 文本情感分类，序列标任务）或两句话（e.g. 文本摘要，自然语言推断，问答任务）。那么就需要让模型有能力去辨别哪一部分属于句子 A，或者哪一部分数据句子 B，BERT 采用两种方式： 在序列的 token 中把 special token （[SEP]）插入到每个句子后面，以分开不同的句子的 token 为每一个 token 表征都添加一个可学习的 segment embedding 来指示这个 token 属于句子 A 还是句子 B 关于上图 BERT 输入的解释就是这样（如果输入序列只包含一个句子的话，则没有 [SEP] 和后面的 token） 上面提到了 token 的表征，实际上该表征由三部分组成：token embedding、segment embedding 和 position embedding BERT 的输出 transformer 的特点是有多少个输入就会对应有多少个输出，每个位置返回的输出都是一个隐藏层大小的向量（基础版 BERT 为 768） $C$ 为 classification token（[CLS]）对应最后一个 transformer 的输出，$T_i$ 表示其他 token 对应的输出 对于一些 token 级别的任务（e.g. 序列标注和问答任务），就把 $T_i$ 输入到额外的输出层中进行预测 对于一些句子级别的任务，就把 $C$ 输出到额外的输出层中（以文本分类为例，如下图所示） BERT 的预训练任务 预训练的概念在 CV 中比较成熟，如我们会使用在 ImgeNet 上预训练好的 backbone 完成物体检测的任务（e.g. RCNN 一族） 虽然在 NLP 领域没有像 ImageNet 这样高质量的人工标注数据，但是可以利用大规模数据点的自监督性质来构建预训练任务。因此，BERT 构建了两个预训练任务——masked language model 和 next sentence prediction MLM MLM 是 BERT 突破单向语言模型限制的重要原因 简单来说，MLM 就是以 15% 的概率用 mask token（[MASK]）随机地替换掉训练序列中的 token，然后预测出 [MASK] 位置原来的单词是什么。然而，这种方式也导致了一定的问题——预训练的过程产生的语言表征对 [MASK] 比较敏感，而对其他 token 不是很敏感，又 mask token 并不会出现在下游任务的 fine-tuning 阶段，那么这就会导致与训练和 fine-tunning 的一丢丢不匹配 BERT 采用下面的策略来解决这个问题： 首先，在每个训练序列中以 15% 的概率随机选中一些 token 用于预测， 这些选中的 token 中 80% 会被替换为 [MASK]（e.g. my dog is hairy → my dog is [MASK]） 10% 的会用其他随机的 token 进行替换（e.g. my dog is hairy → my dog is apple） 还有 10% 保留原来的 token 不变（这部分样本可以理解为与前一个替换方式形成了正负样本的比较） 现在，假如 $i\\ th$ token 被选中，那么在后面的时候就用 $T_i$ 去预测原来的 token（p.s. 将 $T_i$ 输入到全连接层，然后用 softmax 输出每个 token 的概率，最后使用交叉熵计算损失） 通过这样的策略就可以使得 BERT 不再只对 mask token 敏感，而对所有的 token 都敏感，于是就能抽取出任何 token 的表征信息 论文提供了 ablation for different masking procedures，其中基于特征方法的 NER 并没有添加任何 adjust representation 的操作，因此它的 mismatch 会更大一些。作者的实验结果如下表格所示，可以发现，完全使用 [MASK] 替换的 feature-based NER 确实存在严重的 mismatch 的问题。同样的，完全使用随机替换的方式也并不是很好 next sentence prediction MLM 任务倾向于抽取 token 层次的表征，对于一些需要理解两个句子之间关系的任务，MLM 就会有一些问题。为了使模型能够理解句子之间的关系，BERT 使用了 NSP——预测两个句子是否连在一起。具体的做法是：我们在语料库中挑选出来句子 A 和句子 B 组成一个训练样本，50% 的情况句子 B 就是句子 A 的下文（标注为 IsNext），50% 的情况是随机挑选的（标注为 NotNext）。接下来就把训练样本输入到 BERT 模型中，根据输出 $C$ 去进行二分类预测 举个例子，0 这样子每个训练样本就能够对应两个任务（MLM 和 NSP），把这两个的损失加在一起就得到了整个预训练的损失 可以明显看出，这两个任务的数据集都是从无标签的文本数据中构建的，这也就是我们前面提到的自监督性质，比 CV 中人工标注的 ImageNet 数据集简单多了 Fine tuning fine tuning 所需要做的就是在预先训练好的模型中添加一个简单的分类层，然后所有的参数根据下游任务联合进行适当调整。如下图所示，论文介绍了如何将 BERT 用于 4 种下游任务 BERT for feature extraction fine tuning 并不是使用 BERT 的唯一方式。我们可以使用预训练的 BERT 来创建语境化词嵌入。这么做是有意义的， 不是所有的任务都可以使用 transformer encoder 的输出，需要根据下游任务添加特定的模块 预先通过一次复杂的计算得到训练数据有效的表征，然后通过表征去完成后面的一些应用和实验，这样是十分节约计算成本的 那么选取哪些向量作为词嵌入呢？这取决于具体的下游任务，论文比较了 6 中词嵌入的选择，如下图所示 关于 BERT 的思考 BERT 有个很严重的问题，速度慢，计算资源消耗大。其效果的提升跟大语料有不可分割的关系 但是 BERT 又是具有里程碑意义的，其里程碑意义就在于：证明了一个非常深的模型可以显著提高 NLP 任务的准确率，而这个模型可以从无标记数据集中预训练得到 Reference The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) BERT的理解 什么是BERT？ 如何评价 BERT 模型？ "},"【论文】Bottom-Up Attention.html":{"url":"【论文】Bottom-Up Attention.html","title":"Bottom-Up Attention","keywords":"","body":"﻿* 【论文】Anderson, Peter, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering.（pdf） Motivation 我们先区分一下两种注意方式， top-down attention，这种注意力方式由非视觉或任务特定上下文所驱动 bottom-up attention，这种注意方式更容易被显著的、突出的、新奇的事物吸引 为什么以往只有 top-down attention 的模型效果不好呢？ 作者认为这些通过某一特定的任务驱动去选择性注意 CNN 某一两层卷积输出特征的方式并不是很关注这些感兴趣区域是如何选出来的。如下图左边所示，这种方式下产生的输入都是一些与感受野大小、形状保持一致的小方格，与实际的图像并没有关联 而人类实际上的注意方式总是会自发地注意到那些比较显著的内容，而后自主地去根据任务要求关注需要的地方。基于这样的启发，作者将 top-down attention 和 bottom-up attention 结合： bottom-up attention 提取出显著图像区域，每个区域有一个自己对应的池化卷积特征。这时的显著图像区域是纯视觉上的特征，并没有受特定任务驱动，对显著区域的关注并不会厚此薄彼。接着，为了对应 image caption 或者 VQA 任务，需要 top-down attention 根据任务特定的上下文预测图像区域的注意力分布，通过对这些区域的 image feature 的加权平均得到 attended feature vector。这就相当于我们现在根据额外的信息学习到了需要更注重哪一块而忽略哪一块，所以重新调整一下图像区域的权重 到此为止，我想应该说清楚了为什么要将 bottom-up attention 和 top downattention 结合，也说清楚了怎么将两种注意力机制结合在一起 Bottom-Up Attention Model 根据前面的描述 bottom-up attention 要做的事情就是提取纯视觉上的显著图像区域。作者通过 Faster RCNN（backbone：ResNet-101) 来产生这样的视觉特征 $V$，将 Faster RCNN 检测的结果经过非最大抑制和分类得分阈值选出一些显著图像区域，这些显著图像区域如下图所示 具体来说，我们记输入的图片为 $I$，那么图片理解和 VQA 模型的输入都是一个大小为 $k$ 的 image feature set $V=\\left{\\mathbf v_1, \\mathbf v_2,\\cdots, \\mathbf v_k\\right}\\ \\mathbf v_i\\in\\mathbb R^D$，其中每个 image feature 编码了一个显著图像区域。当然，$V$​ 可以定义为 bottom-up model 的输出，也可以定义为普通 CNN 模型的输出 作者对 Faster RCNN 的输出分类别按住 IoU 阈值进行非最大抑制。然后，选择那些分类得分超过一个置信度阈值的区域作为显著图像区域。这样就保证了如上图所示的结果，每一个框能对应一个类别。对于区域 $i$，$\\mathbf v_i$ 是卷积特征平均池化后的结果，$D=2048$ 为了能学习到不错的特征表示，作者还对增加了属性预测。将 $\\mathbf v_i$ 和从 ground truth object class 学习到的一个 embedding 结合在一起，然后将它们送入额外的一个属性 softmax 分类器（附件一个 no attributes class），以此来获得区域 $i$ 对应的属性。作者保留了Faster RCNN 对于 RPN 和 object class proposal 的四部分损失。另外，再加上一个多分类损失用于训练属性分类器，如下图所示 加上属性分类器后的结果如下所示 Caption Model 我不打算去细致描述 caption model 论文里面的公式符号，如果有需要请对照论文 VQA Model 同样的关于 VQA 模型我们也不细致去描述公式和符号，如果有需要请对照论文 Experiment 论文给出了两幅图分别展示了 imag caption 和 VQA 任务下的注意力机制 同时论文也展示一些失败的例子，如下面狗跳起来去追飞盘，但是模型理解为了狗摊在草地上。作者的解释是在提取显著图像区域的时候并没有区分狗的头部和腿部 同样，在 VQA 也有类似的失败的例子，显著图像区域并没有细致的区分每个橘子，所以在回答时导致错误 通过以上描述可以发现 bottom-up and top-down attention model 十分依赖显著图像区域提取的好坏 "},"【论文】CLIP-Adapter.html":{"url":"【论文】CLIP-Adapter.html","title":"CLIP-Adapter","keywords":"","body":" 【论文】Gao P, Geng S, Zhang R, et al. CLIP-Adapter: Better Vision-Language Models with Feature Adapters.（pdf） What is CLIP-Adapter CLIP 和 CoOP 分别在 21 年的 3 月份、9 月份挂在 arxiv 上，10 月份又挂了一篇：CLIP-adapter。CoOP 和 CLIP-adapter 都是在 CLIP 基础上增量式的工作，CLIP 使用一个人为设定好的 prompt 直接进行零样本推理，而 CoOP 和 CLIP-adapter 都是使用小样本学习，不同的是： CoOP是初始化一个随机的 prompt，在小样本中学习合适的 prompt，从而更适应这个任务 CLIP-adapter 是在模型中间插入一个随机的可学习的模块，通过 fine-tuning 这个模块来更适应下游任务 如下图最下面所示就是 adapter 的结构，在两个分支上各加入一个可学习的线性层。在小样本训练（few-shot fine-tunning）时，使 CLIP backbone 冻结参数（如果更新整个大模型，由于数据太少很容易过拟合）。另外，为了更好地结合 CLIP 的知识和从 few-shot training examples 及时习得的知识，作者加入一个残差连接结构，将其称为 residual-style blending 记输入的原始图像为 $\\mathbf I$，分类标签的自然语言名称为 $\\left{Ci\\right}^K{i=1}$，图像特征为 $f$，因为 CLIP 最终将两个 encoder representation 直接进行点积，所以作者将 text encoder 的输出视为一个分类权重矩阵 $\\mathbf W_i$， 也就是 $\\mathbf W_i=\\text{BERT(Tokenizer([} H;C_i\\text{]))}$，其中 $H$ 为 prompt 模板 记 Adapter 在两个分支上的 learnable feature adapters 分别为 $A_v(\\cdot)$ 和 $A_t(\\cdot)$，具体采用了一个两层的线性网络，于是可以描述为 \r A_v(f)=\\text{ReLU}(f^T\\mathbf W_1^v)\\mathbf W_2^v\\\\\r A_t(\\mathbf W)=\\text{ReLU}(\\mathbf W^T\\mathbf W_1^t)\\mathbf W_2^t\r 再考虑上 residual-style blending，Adapter 得到的 new image feature $f^$ 和 classifier weight $W^$ 就可以表示成如下，其中 $\\alpha、\\beta$ 为 residual ratio（平衡因子） \r f^*=\\alpha A_v(f)^T+(1-\\alpha)f\\\\\r \\mathbf W^*=\\beta A_t(\\mathbf W)^T+(1-\\beta)\\mathbf W\r 在小样本学习的时候，通过交叉熵损失优化 $A_v(\\cdot)$ 和 $A_t(\\cdot)$ 的权重 \r \\mathcal L(\\theta)=-\\frac{1}{N}\\sum_{n=1}^N\\sum_{i=1}^Ky_i^{(n)}\\log\\hat y_i^{(n)}\r 如果 $i$ 等于 ground-truth category label $\\hat i$ 的话，$y_i=1$，反之为 $0$；$\\hat y_i=p_i$ 表示预测分类 $i$ 的可能性。学习更新的参数为 $\\theta=\\left{\\mathbf W_1^v,\\mathbf W_2^v,\\mathbf W_1^t,\\mathbf W_2^t\\right}$ Experiments 在 11 个数据集上进行小样本学习，可以看到 CLIP 零样本推理的结果，CLIP 小样本微调的结果，CoOP小样本学习的结果，adapter 小样本学习的结果。 从左上角平均结果的图可以看出，adapter 小样本学习的效果最好。对于 CLIP，小样本学习的初始阶段甚至弱于零样本学习，所以人为设定的 prompt 只是我们看上去合理的提示语句，对于模型来说可能并不是一个好的初始化 Reference 多模态中的Prompt范式：从CLIP、CoOp到CLIP-adapter "},"【论文】CLIP-ViL.html":{"url":"【论文】CLIP-ViL.html","title":"CLIP-ViL","keywords":"","body":" 【论文】Shen S, Li L H, Tan H, et al. How Much Can CLIP Benefit Vision-and-Language Tasks?.（pdf） What is CLIP-ViL 最近的工作表明，视觉表征已经成为 VLM 性能提升的一个瓶颈，学习更加强大的 visual encoder 十分重要。而以往单纯通过图像信息（ImageNet / VG）训练的 visual encoder 显然将不足以应对 VL 中更复杂的学习，于是作者把眼光投向了 CLIP，CLIP 不仅基于语言监督学习视觉概念，同时还有很强的 zero-shot 能力，那么用 CLIP 的 visual encoder 作为 VLM 的视觉编码器岂不是能得到更好的效果 基于这个想法，作者提出了两种方法将 CLIP 和 VLM 进行结合： 将 CLIP 直接作为 VLM 的 visual encoder 进行 task-specific fine-tuning，该方式下的模型记为 CLIP-ViL 将 CLIP 和 VL 预训练结合起来，在图文对数据上进行训练 ，然后再迁移到下游任务，该方式下的模型记为 CLIP-ViL~p~ （这里需要说明的是，作者在文中所指的 VL 预训练模型是指 ViLBERT 这种通过 co-attention transformer layer 使得两个模态有很深交互的模型，而 CLIP 仅是在两个 encoder 输出之后做了一个浅层的跨模态交互。这两种结合方式作者都希望模型在能够获得很好视觉表征的情况下，还可以使两个模态有一个更深的交互） CLIP-ViL 如 Fig 1. 所示，典型的预训练包括三个阶段： visual encoder pre-training：在标注好的视觉数据集上训练 visual encoder，如之前 VLM 爱用的 Faster RCNN 或 ViT vision-and-language pre-training：使用 image-caption 数据集和一些代理任务预训练模型 task-specific fine-tunning：针对下游任务进行微调 而 CLIP-ViL 就是将第一步中的 Faster RCNN 或 ViT 替换为 CLIP 的 visual encoder，然后直接进行第三步，没有 vision-and-language pre-training 的工作 对于下游任务，作者选择了 VQA、Image Captioning 和 Vision-and-Language Navigation 在 VQA 任务上，作者选用 Pythia、MCAN 和 CLIP 结合，实验结果如下所示 通过表格的前两行可以发现采用 CLIP 作为 visual encoder 能够使模型的性能有一个提升，尤其适用更大的 CLIP visual encoder 这种提升还会增加。对比表格的后面两行，作者发现如果 CLIP 和在 VG 数据集上进行过 detection pre-training 的模型结合，表现反而会下降，猜测的原因是 CLIP 采用的数据和训练方式都与 ImageNet 上训练的模型不同，这样反而会损害之前在 VG 上的 fine-tunning 下表是 CLIP-ViL 在 Image Captioning 任务上的实验结果，同样结合 CLIP 能够明显比 BUTD 提取的 region feature 和基于 ImageNet 的 grid feature 性能更高 下表是 CLIP-ViL 在 Vision-and-Language Navigation 任务上的实验结果，由于该任务和具身学习相关（embodied AI），不太了解，不作为重点关注 CLIP-ViL~p~ CLIP-ViL~p~ 在 CLIP-ViL 的基础上不 freeze 住 CLIP visual encoder，将其带入 vision-and-language pre-training 进行训练。采用 LXMERT 中的三个预训练代理任务：（1）grounded masked language modeling；（2）text-image matching；和（3）visual question answering 同 LXMERT 使用 MSCOCO Caption、VG Captions、VQA、GQA 和 VG-QA 几个数据集构成的一个预训练数据集（9.18M image-text pair）进行预训练 下表展示了 CLIP-ViL~p~ 在下游任务上的微调结果，可以看出 CLIP-ViL~p~ 在 VQA 和 SNLI-VE上达到 SOTA 的性能 Experiments 这里我们提一个点，在 CLIP-ViL 中 CLIP-ViT-B 的效果很差，作者的猜测是 ViT 对 visual location 并不感知。同样，作者在 CLIP-ViT-B 的 grid feature maps 上做了一个 VG 的检测器，但是发现这种方式下的 AP（Average Precision）只有 0.03，比 ImageNet-Res50 的 3.14 差了很多 （但是我们好奇的是 ResNet 的 CLIP 就有很好的 visual location 感知能力了吗，这种感知能力个人觉得应该不是指检测器一类的对 bbox 的回归，应该是卷积网络自身带有那种位置感知能力） "},"【论文】CLIP.html":{"url":"【论文】CLIP.html","title":"CLIP","keywords":"","body":" 【论文】Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision.（pdf） What is CLIP 作者的 motivation 很简单：现在在 NLP 领域已经有很多工作都是基于大量语料数据的自监督训练，这些模型的效果已经超越了人工标记数据集训练的效果。然而在 CV 领域大部分的工作还是基于预先定义好的标签来进行训练，这种有监督的方式限制了模型的泛化能力和使用性。那么，有没有办法在 CV 上做到一种自监督训练呢？有，那就是 using natural language supervision for image representation learning，但是这部分的研究很少（比较新的一些的模型如 VirTex、ICMLM 和 ConVIRT）。原因主要是表现很差（Li 等人在 17 年的工作中通过 zero-shot的方式仅在 ImageNet 上实现了 11.5% 的准确率，而当时的 SOTA 则达到了 88.4%） 那另外是否还有方法呢？还有一些研究是关于弱监督的（如 Mahajan 18 年的工作1和 Kolesnikov 19 年的工作2），这些方法取得了不错的结果。而弱监督方法和直接使用自然语言做监督的区别在于：弱监督使用的数据相对来说会很大在数百万到数十亿之间，而直接使用自然语言作监督的方法则只用到十到二十万张图片。于是，作者把两种方法结合起来，用了一个包含 400m 个图像文本对的来训练一个简化版的 ConVIRT ，由此得到 CLIP（Contrastive Language-Image Pre-training）从自然语言去监督视觉特征的学习。在进行下游任务时，只需要提供和图片上的 concepts 对应的文本描述就可以进行 zero-shot transfer 通过这段话我们对 CLIP 的工作有了一个初步的认识，但是具体是怎么实现的呢？我们需要从下面几个问题去进一步探讨 400m 这个超大数据集是如何构建的呢？ 如何训练 CLIP？ CLIP 如何实现 zero-shot transfer 的呢？ Creating a Sufficiently Large Dataset 为了覆盖尽可能多的 visual concepts，作者预设了 500k 个词语作为 queries（每个词至少在维基百科中出现过 100 次）在搜索引擎上爬取图文对，每个 query 大概有 20k 个图文对，由此构建了 CLIP 用于训练的 4m 数据集 Selecting an Efficient Pre-Training Method CLIP 像是 GPT-3 的一个 CV 版，输入图片，输出文本描述，以此来借助自然语言的监督学习图像特征。CLIP 主要分为下面 3 个阶段：（1）Contrastive pre-training;（2）Create dataset from label text；（3）use for zero-shot prediction Contrastive pre-training 关于预训练的方法作者做了一个对比：先是使用一个类似 image caption 的预训练任务，大概这个模型如 VirTex（如下图所示），Fig 2. 中的蓝线，学习效率很低，达到同样精度要使用很多图片数据。黄线是基本的训练方法，即预测图片描述的文本单词，可以发现使用 transformer 的方法会慢出 3 倍的样子。这前面的这两种方法试图预测每幅图片所附文字的准确单词，但由于种类繁多，这么做是很困难的。而对比学习能较好地学习表征，以文本整体和图片的配对作为一个目标，进一步提高了效率，即 Fig 2. 中的绿线 Contrastive Learning 可以参考 对比学习（Contrastive Learning） 具体而言，CLIP 中的对比学习可以描述为如下， 给定 batch = $N$ 的图文对输入，CLIP 预测 $N\\times N$ 的概率，对角线视为正样本（$N$ 个），其他的均为负样本（$N^2-N$ 个） text encoder 采用 transformer，image encoder 有两种选择：ResNet 和 ViT，将得到的 encoder representation 直接使用线性映射投影到 multi-modal embedding space，如下图矩阵，利用点积计算图文对之间的 cosine similarity 目标使用 symmetric cross entropy loss，要求匹配的图文对相似度最大，不匹配的图文对相似度最小，训练参数为两个 encoder Create dataset from label text & use for zero-shot prediction 这部分的工作相对比较简单，将不同的 label 填入句子中形成不同的 text encoder representation（如 A photo of a plane 对应于 $T_1$, A photo of a dog 对应于 $T_3$），然后再将 zero-shot 图像也进行编码，跟据对比学习学到的内容，可以预测匹配的输出应该是 A photo of a dog，这样就实现了 one-zero transfer 下图是 CLIP实现的一个伪代码，可以看到相对来说是十分简洁的 Experiments 实验先和其他的零样本学习模型对比，和 Visual N-Grams 相比，CLIP 在三个图片分类数据集上的准确率都有很大提升 作者用实验证明 prompt 的必要性，一个词经常会具有多义性，而将词带入到特定的上下文提示语中，就可以有更清晰的意义，有助于分类。另一方面可以减小和预训练任务之间的差异。实验证明使用 prompt 要更有效 上图蓝线是在（微调和）测试时只使用 label 的文本，绿线是使用了 prompt 和 ensemble，prompt 即给 label 加一段提示语，ensemble 指同时用多个不同的上下文 prompt 语句，对于一个 label 生成多个句子 embedding 再集成 Annotation CLIP 的预训练做了一个 image-caption 的对比学习，学习的内容是两个 encoder，然后用训好的 encoder 去做 few-shot 和 one-shot 的图像分类任务，prompt 是手工设计的，使分类任务输出 label 的形式更接近于预训练学习到通用知识的形式。至于两个 encoder 是否要微调参数取决于 few-shot or one-shot，few-shot 就相当于有训练数据了，接下来就是 tune encoder ~~~~~~~~~~~~~~~~~~~~~~~~~11/16/21~~~~~~~~~~~~~~~~~~~~~~~~~ Create dataset from label text 阶段将所有的标签提前通过 text encoder 生成一个固定的 text_features，然后拿着这个 text_features 和 test image 的 image_features 计算余弦相似度 Linear-probe 说明： Linear-probe 将 pre-training 的模型视为特征提取器，增加一个分类头，只训练这个分类头。Linear-probe 的直觉是一个好的特征应该能区分不同的类，除此之外，fine-tune 效果好有可能是因为架构很适合下游任务，但是 linear-probe 只取决于特征质量 Code Reference 博客引用 多模态系列-1 聊聊这两天刷屏的OpenAI新作，你注意到CLIP了吗 多模态模型补充-CLIP 【论文笔记】CLIP：Learning Transferable Visual Models From Natural Language Supervision CLIP 多模态中的Prompt范式：从CLIP、CoOp到CLIP-adapter 论文引用 1. Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., and van der Maaten, L. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 181–196, 2018. ↩ 2. Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neumann, M., Dosovitskiy, A., et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. ↩ "},"【论文】CLIP2Video.html":{"url":"【论文】CLIP2Video.html","title":"CLIP2Video","keywords":"","body":" 【论文】 Fang H, Xiong P, Xu L, et al. CLIP2Video: Mastering Video-Text Retrieval via Image CLIP.（pdf） video-text retrieval 里有两个问题：（1）如何获得更好的视频视觉特征；（2）视频和语言的夸模态交互。一些 leading 的做法是尝试从一个大型的视频文本数据集中蒸馏出 spatio-temporal features 和跨模态的交互能力，但是基于大数据集的预训练模型由于标注数据获得的困难而受到限制 作者重新思考视频文本检索这个任务，既然视频和文本都是序列的，一个词的意思就可以对应到某张图片或某一段视频帧。例如，我们在一张图片中描述某一物体，那么对应的一个原子操作就应该被放到某一个 short-term segments 的上下文中。于是，视频语言理解任务就可以被分为两个独立的问题，（1）spatial representation of multi-modal image-text training；（2）temporal relationships of video frames and video-language 于是，CLIP2Video 采用两段的 image-text co-learning 框架，分别捕捉视频帧之间的 temporal relations 和视频文本之间的 relations，具体来说，作者设计了如下两个模块： Temporal Difference Block（TDB），捕捉精细时间视频帧的运动 Temporal Alignment Block（TAB），重新对齐 video clips and phrase tokens，增强跨模态的联系 介绍到这，对 CLIP2Video 的 motivation 和 method 都有了一个初步的了解。那么 CLIP 呢？CLIP 的工作体现在哪里呢？ 之前 Clipbert3 的工作说明了将 image-text 的预训练知识转移到 video-text 的学习上是 work 的，作者沿用这种思想使用 CLIP 直接初始化 CLIP2Video 的模型 另外，在 CLIP2Video 之前已经有工作把 CLIP 用于视频文本对检索（Clip4clip），作者认为之前的工作都只在证明 CLIP 模型对预训练的有用性，但是在 CLIP2Video 中进一步研究了如何更好地对视频帧和视频文本的时间关系进行建模 CLIP2Video 不同于图像文本的任务，视觉线索时域的相关性充分折射出一段视频的语义空间，这对视频语言的跨模态学习来说也是十分重要的。于是，作者引入了 TDB 显示激发 motion-sensitive interactions，TAB 对齐文本上下文和关键帧的内容 Temporal Difference Block 先看红色阴影部分，使用 ViT 对每一帧编码得到 video embedding，一帧中的每张图片分为 $N$ 个不重叠的 image patches，pathces 经过线性层得到的 1-D tokens，将 token sequence 和 [CLS] token、positional embedding 组合得到的 transformer 输入序列记为 $\\mathfrak z$，$\\mathfrak z$ 经过一个 $Ls$ 层的 （spatial）transformer 得到 $\\mathfrak z{cls}^{Ls}$，$\\mathfrak z{cls}^{L_s}$ 输入到一个线性映射中得到和 text embedding 相同维度的 frame representation 我们再看 Fig 1. 中绿色的部分，在不考虑时间的情况下，spatial ViT 对每个帧内的相关性进行了建模。为了进一步对不同帧之间的联系进行建模，作者还提出了一个 $L_t$ 层的 temporal transformer 编码 video representation 接着，我们还是回到 Fig 2. 看蓝色的部分。ViT 输出的每个 frame embedding 组合（concate）在一起构成一组长度为 $m$ 的 frame tokens，两个连续帧之间包含反映实际动作的内容偏移（content displacement），所以作者引入 TDB 模块来产生 difference-enhanced token，引导前面讲到的 temporal transformer 编码 motion-related representations 如下图所示，计算时间戳上相邻帧的 transformed difference 来描述视频的运动变化。记相邻两帧的 frame embedding 为 $f_f^{m-1}$ 和 $f_f^{m-2}$，比起直接使用两帧 embedding subtraction，作者提出了一个 difference-level attention $\\delta$ 作用于整个 frame tokens 的 embedding subtraction上，即 $\\delta\\left({f_f^{1}-_f^{0},f_f^{2}-_f^{1},\\dots,f_f^{m-1}-_f^{m-2}}+\\mathbf P\\right)$，其中 $\\delta$ 为一个 1-layer transformer，由于 transformer 的位置不敏感，所以又加入了一个位置编码 $\\mathbf P$ ，这样对连续帧 embedding subtraction 的操作可以对所有片段的 long-term relationship 很好地建模。1-layer transformer 得到的结果经过一个 sigmoid 激活（记为 $\\zeta$）就可以最终的 difference-enhanced token，如蓝色阴影中 $f^t_d$ 的虚线方块所示，数学表达如下 \r \\mathbf {F_d}=2\\zeta\\left(\\delta\\left(\\{f_f^{1}-_f^{0},f_f^{2}-_f^{1},\\dots,f_f^{m-1}-_f^{m-2}\\}+\\mathbf P\\right)\\right)-1\r 另外，$\\mathbf F_d$ 中的系数和减一操作是为了归一化到 $[-1, 1]$。最终，将得到的 difference-enhanced token 带着位置编码（$\\mathbf P$，如蓝色阴影中 $2t-1$ 的绿色方块所示）和类型编码（$\\mathbf T$，$\\pmb d$ 的灰色方块所示）插入到原来的 frame tokens 中就构成了 temporal transformer 的输入 \r \\mathbf {F_te}=\\left\\{f_f^0,f_d^1,f_f^1,f_d^2,f_f^2,\\dots,f_d^{m-1},f_f^{m-1}\\right\\}+\\mathbf P+\\mathbf T\r 最后是 Fig 2. 中的黄色阴影部分，在 temporal transformer 的输出中丢掉 $\\mathbf {F_d}$ 对应的部分，仅保留 frame tokens 对应的 $\\mathbf {F_v}={f_v^0,f_v^1,f_v^2,\\dots,f_v^{m-1}}$ 作为 video embedding，此时的 video embedding 包含空间域和时间域的信息。接着，对所有的 video embedding 做 global average pooling 得到 video representation $f_v^g$ Temporal Alignment Block 文本部分作者直接用 CLIP text encoder 输出的 representation，CLIP 的 text encoder 是一个 12-layer transformer，使用 41952 vocab size 的 BPE tokenize 输入 $\\Phi$，编码的结果用 [CLS] 和 [SEP] 括起来，经过 transformer 得到的 representation 记为 $\\mathbf {F_t}={f_t^{cls},f_t^0,f_t^1,\\dots,f_t^{n-1}}$，[CLS] token 对应的输出作为文本的全局表征 $f_t^g$ 用于和 $f_v^g$ 做 global matching，$\\mathbf {F_t}$ 剩下的 word token representation 用做辅助监督对齐具有运动变化的关键帧 TAB 将不同模态的 token embedding 聚集到共同的中心（shared centers），以此再次强调上下文相关的表征。具体来说，记 $\\mathfrak R={c1, c_2,\\dots,c_k}$ 表示对齐帧和词嵌入的 shared center，通过点击计算模态特征和 shared center 之间的置信度，然后根据这个置信度去为每个 cluster 分配权重 \r w_{ij}=\\frac{\\exp(\\rho_ic_j^T)}{\\sum_{k=1}^K\\exp(\\rho_ic_k^T)}\r 其中，$\\rho_i$ 表示 $i$-th modal feature，$c_j$ 表示 $j$-th shared center，$w{ij}$ 表示归一化的相似度 接着，通过下面公式我们可以得到和中心 $c_j$ 对齐后的 aggregated embedding \r v_j=\\frac{\\sum_{i=1}^{\\eta}w_{ij}(\\rho_i-\\tilde c_j)}{||\\sum_{i=1}^{\\eta}w_{ij}(\\rho_i-\\tilde c_j)||_2}\r 其中，$\\eta$ 表示 modal feature 的最大长度，$\\tilde c_j$ 表示和 $c_j$ 一样大小的可训权重 经过视频、文本和 shared centers 聚类，在每个 modality token 中的整体语义上下文都可以完全对齐到联合空间。为了进一步强调 the weight of motion-related frame tokens toward action-described centers，作者采用二倍的帧率对 frame embedding $\\mathbf {Ff}$ 做了一个稀疏 re-sample，得到的结果记为 $\\mathbf {F{fl}}$，$\\mathbf {F_{fl}}$ 以较大的帧率采样会损失语义上的连续性，但是这同时强调了运动变化的信息，这部分信息有利于重新调整 motion-related center 的权重分布 同样的，为提取时间关系，将 $\\mathbf {F{fl}}$ 过一个 1-layer transformer，输出记为 $\\mathbf {F{dl}}$，$\\mathbf {F{dl}}$ 和 $\\mathbf {F_f}$ 组合在一起构成 $\\mathbf {F{ml}}=[\\mathbf {Ff},\\mathbf {F{dl}}]$，如 Fig 3. 右边输入所示 视频表征和文本表征的对齐可以按照如下公式进行 \r v_j^v=\\frac{1}{\\sigma_v}\\sum_{i=1}^{1.5(m-1)}\\frac{\\exp(f_{ml}^ic_j^T)}{\\sum_{k=1}^{K}\\exp(f_{ml}^ic_k^T)}(f_{ml}^i-\\tilde c_j)\\\\\r v_j^t=\\frac{1}{\\sigma_t}\\sum_{i=1}^{n}\\frac{\\exp(f_{t}^ic_j^T)}{\\sum_{k=1}^{K}\\exp(f_{t}^ic_k^T)}(f_{t}^i-\\tilde c_j)\r 其中，$\\sigma_v$ 和 $\\sigma_t$ 表示 $l_2$ normalization 在 $v_j^v$ 和 $v_j^t$ 分别使用 global average pooling 得到最终的 aligned representation $f_v^a$ 和 $f_t^a$ Loss function 训练 CLIP2Video 采用对称的交叉熵损失，每一个训练 batch $\\Omega$ 中有 $B$ 对视频文本 \r \\mathcal L_{t2v}=-\\frac{1}{B}\\log\\frac{\\exp()}{\\sum_{j\\in\\Omega}\\exp()}\\\\\r \\mathcal L_{v2t}=-\\frac{1}{B}\\log\\frac{\\exp()}{\\sum_{j\\in\\Omega}\\exp()}\\\\\r \\mathcal L_o=\\frac{1}{2}(\\mathcal L_{t2v}+\\mathcal L_{v2t})\r 其中，$$ 表示 cosine similarity、 作者分别采用 $f^g$ 和 $f^a$ 计算一遍 $\\mathcal L_o$ 得到 $\\mathcal L_o^g$ 和 $\\mathcal L_o^a$，最终的损失就表示为 $\\mathcal L_o=\\frac{1}{2}(\\mathcal L_o^g+\\mathcal L_a^g)$ 在推理时，similarity 按照如下公式计算 $=\\frac{1}{2}\\left(+\\right)$ Experiments Reference 1:Portillo-Quintero J A, Ortiz-Bayliss J C, Terashima-Marín H. A straightforward framework for video retrieval using clip[C]//Mexican Conference on Pattern Recognition. Springer, Cham, 2021: 3-12. 2: Luo H, Ji L, Zhong M, et al. Clip4clip: An empirical study of clip for end to end video clip retrieval[J]. arXiv preprint arXiv:2104.08860, 2021. 3: Lei J, Li L, Zhou L, et al. Less is more: Clipbert for video-and-language learning via sparse sampling[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 7331-7341. "},"【论文】CLIP4Clip.html":{"url":"【论文】CLIP4Clip.html","title":"CLIP4Clip","keywords":"","body":" 【论文】Luo H, Ji L, Zhong M, et al. Clip4clip: An empirical study of clip for end to end video clip retrieval.（pdf） 作者将 CLIP 以端到端的方式迁移到 video-language retrieval 中，同时探究了以下几个问题： 图像特征对 video-text retrieval 来说足够吗？ 在大型的 video-text 数据集上 post-pretraining 基于 CLIP 迁移的模型会带来怎样的性能提升？ 对视频帧之间的时间关系哪种机制在实践中更有用？ CLIP4Clip 对超参数的敏感性 对之前关于 video-text 的工作，作者将它们分类两类：pixel-level 和 feature-level，具体来说，pixel-level 是指一些使用原始视频作为输入来训练的模型，最大的挑战是如何减少密集视频输入的高计算过载，ClipBERT 的稀疏采样策略提供了一种思路，使端到端预训练成为可能；而 feature-level 就是指 freeze 住训练好的模型来提取视频特征，这样比较挑剔 feature extractor，很难找到最适合的 除了 ClipBERT 的工作，在 Frozen in Time 的工作中，将图片视为一个 single-frame video，然后设计了一个 curriculum learning schedule 在视频和图像训练集上进行训练，实验的结果表明 curriculum learning schedule 能有效地进行从图片到 multi frames 的学习 虽然这两篇工作放在了论文的 introduction 中，但是很重要，两篇工作说明一点：从图像迁移过来的特征提取器，可以很好迁移到视频特征提取的任务上 CLIP4Clip 正是基于这样的依据才从 CLIP 上进行改造以适应 video-text retrieval 任务。对应于开头的四个问题，论文给出了如下的总结： 单张图像是不足以解决 video-text retrieval 任务 post-Pretraining 是需要的，尤其对于大幅度的零样本预测 在 CLIP 之上，对于小数据集应尽量采取少参数的方式来得到时间关系，如 mean pooling；对于大型数据集则建议引入更多的学习参数，采用 transformer 之类的模型来建模时间关系 模型对超参数十分敏感，作者在文中小心翼翼调参给出了 best setting Introduction 差不多到这就完了，我们针对这四点给出一些详细的解释，比如像 post-pretraining，又或是时间关系建模机制这些 图像特征对 video-text 任务来说是不够的，所以 CLIP4Clip 采用了和 Frozen in Time 很像的结构——将输入的 video 用一连串的帧（images）表示，即 video $v_i=\\left{v_i^1,\\dots,v_i^{|v_i|}\\right}$，$|v_i|$ 表示采样的帧的数量。Video encoder 采用 CLIP 的 ViT-B/32 做 backbone，同样还是采用 [class] token 作为 image representation ，于是通过帧生成的 video representation 就可以表示为 $\\mathbf Z_i=\\left{\\mathbf z_i^1,\\dots,\\mathbf z_i^{|v_i|}\\right}$ ViT 使用的 linear projection 是一种 2D 的projection，即独立地对每个 2D frame patch 进行计算，这忽略了帧之间的时间关系。于是，作者采用了 ViViT 中提出的 3D linear projection 增强对时间特征的提取。具体来说，3D linear projection 采用了核为 $[t\\times h\\times w]$ 的 3D convolution 还是 video encoder 的问题，作者认为直接使用 CLIP 的 visual encoder 是有问题，因为 CLIP visual encoder 并没有习得任何和时间特征相关的内容。于是，作者拿 Howto100M 的 Food and Entertaining 类进行了一个预训练， 这操作就叫做 psot-pretraining，大约有 380k 个视频，在 8 块 V100 上训了 5 个 epoch 大约两周（使用的是 parameter-free type 的结构）。这就是作者说的第二点，关于 post-pretraining，顺便我们解释一下第四点关于 CLIP4Clip 对超参数敏感的问题，因为 CLIP4Clip 放开了 CLIP 的 text encoder 和 visual encoder，而这两个东西是很难 finetune 的，所以造成了超参数敏感的问题 Text encoder 沿用了 CLIP 的，没有改动。那么 CLIP4Clip 中还剩下没讲的就是 similarity calculator，我们可以看到 Fig 1. (b) 展示了三种时间关系的模型，分别是 parameter-free type，sequential type 和 tight type，要分这三个的原因也很简单，因为作者放开 CLIP 进行 finetune，而 CLIP 本身很难 finetune，那么如果引入一大堆要学习的参数，且没有初始化权重，训练时很难进行的，而且反向传播的更新甚至会降低之前 CLIP 的性能 于是，作者根据参数引入量的大小分了三类，这也就是第三点提到的时间关系建模机制的选择。Parameter-free type 使用 mean pooling 操作来融合视频特征，averrage frame 即所有 frame feature 的融合，记为 $\\hat{\\mathbf z}_i=\\text{mean-pooling}(\\mathbf z_i^1,\\dots,\\mathbf z_i^{|v_i|})$， caption representation 记为 $w_j$，那么 similarity function $s(v_i, t_j)$ 就可以定义为余弦相似度 \r s(v_i,t_j)=\\frac{w_j^T\\hat{\\mathbf z}_i}{||w_j^T||||\\hat{\\mathbf z}_i||}\r Sequential type 选择 LSTM 或 transformer 来融合 frame feature，融合后的特征记为 $\\tilde{\\mathbf Z}_i$，可以表示为 $\\tilde{\\mathbf Z}_i=\\text{LSTM}(\\mathbf Z_i)$ 或 $\\tilde{\\mathbf Z}_i=\\text{Transformer-Enc}(\\mathbf Z_i+\\mathbf P)$，其中 $\\mathbf P$ 为 position embedding，后续计算和 parameter-free type 一样计算余弦相似度 Tight type 和之前两个方法不同的是：不再采用 two separate branch 来计算相似度，而是采用一个 transformer 来进行跨模态交互 \r \\mathbf U_i=[w_j,\\mathbf z_i^1,\\dots,\\mathbf z_i^{|v_i|} ]\\\\\r \\tilde{\\mathbf U}_i=\\text{Transformer-Enc}(\\mathbf U_i+\\mathbf P+\\mathbf T)\r 其中，$[\\cdot]$ 表示 concatenate，$\\mathbf P$ 是 position embedding，$\\mathbf T$ 是 type embedding 最后通过两层 MLP 来计算相似度，取 transformer 最后一层输出的第一个 token $\\tilde{\\mathbf U}_i[0,:]$，即 $s(v_i,t_j)=\\text{FC}(\\text{ReLU}(\\text{FC}(\\tilde{\\mathbf U}_i[0,:])))$ CLIP4Clip 训练采用一个 symmetric cross entropy loss \r \\mathcal L_{v2t}=-\\frac{1}{B}\\sum_i^B\\log\\frac{\\exp(s(v_i,t_i))}{\\sum_{j=1}^B\\exp(s(v_i,t_j))} \\\\\r \\mathcal L_{t2v}=-\\frac{1}{B}\\sum_i^B\\log\\frac{\\exp(s(v_i,t_i))}{\\sum_{j=1}^B\\exp(s(v_j,t_i))}\\\\\r \\mathcal L=\\mathcal L_{v2t}+\\mathcal L_{t2v}\r 提取帧的策略时很重要的，需要考虑信息丰富度和计算复杂性之间的平衡，作者采用了 uniform frame sampling strategy 来替代 CIIPBERT 中的 random sparse sampling strategy，sampling rate 为 1 秒 1 帧。作者在消融实验中研究了帧的长度和提取位置对模型的影响 下表展示了 CLIP4Clip 和其他 SOTA 在 MSR-VTT 上的实验结果 下表是 CLIP4Clip 和其他 SOTA 在 MSVD 上的实验结果 下表是 CLIP4Clip 和其他 SOTA 在 LSMDC 上的实验结果 下表是 CLIP4Clip 和其他 SOTA 在 ActivityNet 上的实验结果，这里做一点解释：可能我们第一反应 ActivityNet 和 DiDeMo 都是 video grounding 的数据集怎么用到这里了？作者将 grounding 的每个小段的 text 组合在一起构成了一段描述，做 video-paragraph retrieval 下表是 CLIP4Clip 和其他 SOTA 在 DiDeMo 上的实验结果 下面是 CLIP4Clip 的一些消融实验，这里不再一一介绍 "},"【论文】CoOP.html":{"url":"【论文】CoOP.html","title":"CoOP","keywords":"","body":" 【论文】Zhou K, Yang J, Loy C C, et al. Learning to prompt for vision-language models.（pdf) What is CoOP 作者的 motivation 在于观察到 prompt 的选择对测试结果影响很大 受 AutoPrompt 的启发，CoOp（context optimization）避免人为地设计提示语，把输入 prompt 设计成如下格式： \r t=[V]_1[V]_2\\dots[V]_M[CLASS]\r 其中，$[V]_M$ 是随机初始化的词向量，也是 512 维。注意这里上下文词向量对于不同类别是共享的。在训练阶段，把 $t$ 前向传播，和图像特征计算相似度，计算出对应每一类的概率 $p(y=i|x)$，再用交叉熵优化，梯度反向传播经过 text encoder 从预训练的知识中蒸馏出 task-relevant 的上下文知识，学到合适的 $[V]_M$（注意只更新 $[V]_M$， freeze 住预训练好的 text/image encoder） 具体来说，记 text encoder 为 $g(\\cdot)$，那么 $p(y=i|x)$ 可以描述为 \r p(y=i|x)=\\frac{exp()/\\tau }{\\sum_{j=1}^Kexp()/\\tau}\r 其中，$t_i$ 表示代入 $i$-th class name 的 continuousp rompt，$K$ 表示类别总数，$\\tau$ 表示 temperature paramete，$$ 表示 cosine similarity prompt 有一些其他的变种， 一种是改变 CLASS 的位置，把 [CLASS] 放在句子中间，这样可以学习到更灵活的上下文 另一种是每种 CLASS 学习一组上下文提示语句，即不同类别对应的语句不同（$[V]_1^i[V]_2^i\\dots[V]_M^i\\neq[V]_1^j[V]_2^j\\dots[V]_M^j,\\ i\\neq j$），作者发现这样的方法对一些细粒度的分类很有效 Experiments 使用 11 个分类数据集做小样本学习，涉及到的分类有物体、动作、场景等 从平均结果看，coop 在小样本学习中效果更好，并且当增加用 16 个样本训练时，准确率可以提高 17 个点。 对于数据分布变化的鲁棒性，如表 1，target 数据集是一些 imagenet 数据集变体，在 source 上做小样本训练，在 target 测试，发现 M 越小鲁棒性、泛化性能越好。表 2 表示 coop 比 clip 里 prompt ensemble 效果还要好。表 3 对比了随机初始化和人为设定 prompt 两种情况直接测试的准确率，表明即便随机初始化一个 prompt，测试效果与人为设定也差不多，当然，如果再使用小样本微调一下随机的 prompt，效果就更好了。 下图，左边是 prompt 长度 M 和 class 位置的消融实验；右边是不同视觉 backbone 的影响。 下表就是把学到的词向量通过欧氏距离找到的词表中最近的词，可以看到可解释性很差，但对于模型来说就是有效。这就是『隐式学习』吧，不知道模型学了个啥。 总的来说，虽然效果上看 coop 使用自动生成 prompt 要好过 clip，但一个主要的区别在于 coop 也需要少量的数据学习 prompt，即少样本学习，而 clip 由于直接使用人为设计的提示语句，可以进行零样本学习（不过实验部分也看到 coop 直接用随机的 prompt 也能做零样本学习，就是差点） "},"【论文】CP-Net.html":{"url":"【论文】CP-Net.html","title":"CP-Net","keywords":"","body":" 【论文】Li, K., Guo, D., & Wang, M. (2021). Proposal-Free Video Grounding with Contextual Pyramid Network. （pdf） 作者提出的模型叫 Contextual Pyramid Network (CPNet) ，采用金字塔结构在 3 个 temporal scales ($TT,\\frac{T}{2}\\frac{T}{2},\\frac{T}{4}*\\frac{T}{4}$) 提取 2D contextual correlation maps，2D correlation map 用于建模视频中两个 moment 之间的关系（past→current, current←feature）。模型采用逐步的方式扩大时间维度的感受野，获得整个视频的 temporal contexts，最后使用一个时间维度上的自注意力回归预测时间边界 如图 1 所示，有时视频会包含一些在语义上容易混淆的 units，\"sit up\" 和 \"go down\" 两个动作对应不同的时间边界。针对这种情况，作者尝试通过时间线索来解决定位时的问题。不同于以往匹配 query 和 candidate proposals 的做法，作者通过任意两个片段的细粒度 temporal correlation 来优化 location regression DP-Net 的大致框架如下图所示，分为三个部分 Feature Encoding 别看论文写了一大堆啥玩意，其实就是一个 QANet backbone，一个 shared feature encoder + 一个 co-attention + 一个 combined feature encoder，在 DEBUG 和 VSLNet 中都出现过，没有什么好说 Contextual Pyramid 之前的一篇工作通过 non-local operation 捕捉到 temporal 全局的依赖关系，受此启发，Contextual Pyramid 重点研究每个 2D temporal matrix 上的任意两个时间点之间的关系 一般一个标准的 non-local operation of positions $t_i$ and $t_j$ 表示为 \r \\mathbf y_{t_i}=\\frac{1}{\\mathcal C(\\mathbf x)}\\sum_{\\forall j}f(\\mathbf x_{t_i}, \\mathbf x_{t_j})g(\\mathbf x_{t_j})\r 其中，$\\mathcal C(\\cdot)$ 表示 normalization function，$g(\\cdot)$ 表示一个 feature embedding layer 在 DP-Net 中用一个 correlation matrix $M$ 表示上式中的 $\\frac{1}{\\mathcal C(\\mathbf x)}\\sum{\\forall j}f(\\mathbf x{ti}, \\mathbf x{tj})$ 计算，$M$ 中的元素 $m{i,j}$ 表示 $\\mathbf x{t_i}$ 和 $\\mathbf x{t_j}$ 之间的相互影响。$M$ 被分为上三角部分 $M^U$ 和下三角部分 $M^L$，$M^U$ 表示 past→current，$M^L$ current←feature 为计算 $M$，作者先设计了一个 pairwise correlation function $f(\\cdot)$ \r f(\\mathbf x_{t_i},\\mathbf x_{t_j})=\\theta(\\mathbf x_{t_i})^T\\phi(\\mathbf x_{t_j})\\in\\mathbb R^{T\\times T}\r 其中，$\\theta(\\mathbf x{t_i}),\\phi(\\mathbf x{t_j})$ 表示两个全连接层 接着，对 $f(\\mathbf x,\\mathbf x)$ 的每一行进行 softmax 操作得到 normalized correlation matrix $M$，即 \r M=\\text{softmax}_{\\text{row}}(f(\\mathbf x,\\mathbf x))\\in\\mathbb R^{T\\times T}\r 同时考虑past 和 feature 对 current 的影响， \r \\mathbf x'=\\mathbf M^U\\otimes g(\\mathbf x)+\\mathbf M^L\\otimes g(\\mathbf x)\\in\\mathbb R^{d\\times T}\r 其中，$\\otimes$ 表示矩阵乘法，$g(\\cdot)$ 表示一个全连接层 接着再经过一个卷积层和残差连接得到 $\\mathbf x^$ \r \\mathbf x^*=\\text{Conv}_3(\\mathbf x')+\\mathbf x\\in\\mathbb R^{d\\times T}\r 上面描述了一个完整的 context-aware layer 的操作，用数学公式概括为 $\\mathbf x^=\\text{CAM}(\\mathbf x)$，图 2 中采用了三层的 pyramid，按照 bottom-up pathway 得到 $\\left{\\hat{\\mathbf H_0}, \\hat{\\mathbf H_1},\\hat{\\mathbf H_2}\\right}$，然后再按照 top-down pathway 得到 $\\left{\\hat{\\mathbf H_0^}, \\hat{\\mathbf H_1^},\\hat{\\mathbf H_2^}\\right}$，其中 $\\hat{\\mathbf H^_i}\\in\\mathbb R^{d\\times\\frac{T}{2^i}}$ bottm-up pathway 可以描述为如下形式 \r \\hat{\\mathbf H_0}\\text{CAM}(\\hat{\\mathbf H})\\\\\r \\hat{\\mathbf H_i}=\\text{CAM}(\\text{DownSampling}(\\hat{\\mathbf H_{i-1}}))\r top-down pathway 则描述为 \r \\hat{\\mathbf H_2^*}=\\text{Conv}_1(\\hat{\\mathbf H})\\\\\r \\hat{\\mathbf H_i^*}=\\hat{\\mathbf H_{i-1}}+\\text{DownSampling}(\\hat{\\mathbf H_{i}^*})\r 最后，作者将 $\\hat{\\mathbf H_1^}$ 和 $\\hat{\\mathbf H_2^}$ resize 到 $\\hat{\\mathbf H_0^*}$ 的大小，然后将它们 cat 到一起经过一个全连接网络得到最终的 context feature $\\mathbf Z\\in\\mathbb R^{d\\times T}$ Temporal Self-attentive Regression Temporal Self-attentive Regression 用数学公式描述如下 \r \\mathbf a^z=\\text{softmax}(\\mathbf W^T(\\text{tanh}(\\mathbf W^z\\mathbf Z+\\mathbf b_1))+\\mathbf b_2) \\\\\r \\mathbf Z'=\\sum_{t=0}^T\\mathbf a^z_t\\mathbf Z_t\\in\\mathbb R^d \\\\\r t^s,t^e=\\text{MLP}(\\mathbf Z')\\in[0,1]\r $\\mathbf a^z$ 是一个关于时间维度的 attention solution，MLP 后面跟一个 sigmoid，将时间点归一化到 [0, 1] CP-Net 完整的 loss 包括两个部分 $\\mathcal L{cls}$ 和 $\\mathcal L{reg}$，$\\mathcal L{cls}$ 用于 temporal attention regression $\\mathbf a$ 和 location label 在时间维度上的对齐，$\\mathcal L{reg}$ 则用于最后时间节点的回归 \r \\mathcal L_{cls} = -\\frac{\\sum_{t=0}^T\\hat{\\mathbf a^z_t}\\log(\\mathbf a^z_t)}{\\sum_{t=0}^T\\mathbf a^z_t}\r 如果 $t$ 属于 gt 里面的一帧，那么 $\\hat{\\mathbf a^z_t}=1$，否则置 $\\hat{\\mathbf a^z_t}=0$ \r \\mathcal L_{reg}=R(t^s,\\hat t^s)+R(t^e,\\hat t^e)\r 其中，$R$ 表示 smooth L1 loss "},"【论文】CPT.html":{"url":"【论文】CPT.html","title":"CPT","keywords":"","body":" 【论文】Yao Y, Zhang A, Zhang Z, et al. CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models.（pdf） What is CPT？ CPT 的实验主要在 visual grounding 任务上进行 在一般的 pretrain + fine_tune 的模式中，预训练目标使用 MLM——将文本的单词遮掉，通过图像区域和文本上下文来还原被遮掉的单词。在 finetune 的时候，需要对图像区域增加 classification head 预测是否是对应的区域，这样作者认为会导致预训练与微调之间存在一些差异，这种差异使得需要大量的标记数据来激发预训练模型的 visual grounding 的能力 如此预训练模型适应下游任务的方式不太完美，那么可不可以让下游任务适应预训练模型呢？这就是 CPT 设计思路的核心。借助于 prompt 可以将下游任务的输出形式转化为预训练模型的训练目标，这样就能够减小下游任务与预训练模型之间的差异，进而利用预训练模型学习到的知识就能实现很强的 zero-shot 和 few-shot 预测 CPT 通过在图像和文本中添加基于颜色的共同参照标记，重新构建了 visual grounding task，使之成为一个填空问题。如 Fig 1. (c) 所示，CPT 首先将图片中的实体模块用不同的颜色来进行区分，然后构造类似于 \\ is in [MASK] color 这样的文本用于预训练，最后在 [mask] 上预测对应的是哪个颜色即可 看起来 CPT 还是十分简单的，主要使用了两个 prompt， visual sub-prompt 使用不同的 colored blocks 或者 segmentation masks 将图像区域标记出来 textual sub-prompt 将 query text 放入到 color-based query template 中 visual sub-prompt 记 region proposals 为 $\\mathcal R=\\left{v_1,\\cdots,v_n\\right}$，颜色集合为 $\\mathcal C$，每一个颜色标记 $c_i=(c_v^i,c_w^i)\\in \\mathcal C$，其中 $c_v^i$ 表示 visual appearance（例如，RGB(255, 0, 0)），$c_w^i$ 表示 color text（例如，red） 记 $\\Psi(\\cdot)$ 表示 visual sub-prompt，那么加上颜色标记的 region proposals 就可以表示为 $\\Psi(\\mathcal R;\\mathcal C)$ 关于 visual sub-prompt 有很多的选择，作者使用了纯色的 blocks textual sub-prompt 记 query text 为 $q$，那么可以通过 template $\\mathcal T(\\cdot)$ 将 query text 构建成一个填空的形式 \r \\mathcal T(q)=[\\text{CLS}]\\ q\\ \\text{is in [MASK] color [SEP]}\r 于是，预训练模型就可以根据 prompt 决定哪个图像区域对应的颜色标记更适合填入到 [MASK] 中 training and inference VL-PTMs can be further tuned by CPT using the entropybased objective：$\\mathcal L=-\\sum{(\\mathcal R,q,v^*)\\in\\mathcal D{train}}\\log P(v^*|\\mathcal R,q)$ 这一节还描述了关于颜色集合选取的细节，这里不做详细介绍 Experiments CPT 使用的预训练模型为 VinVL 实验表明 CPT 相比 finetune 拥有更强的 zero-shot 和 few-shot 的能力。在训练样本数较小时，CPT 表现出很强的鲁棒性，反映在选取不同的训练样本和测试样本的情况下测试性能的标准差相比 finetun小。并且，CPT 在全量数据集上进行训练的结果与 finetune 相当，展现出较高的数据利用效率 此外，作者还指出CPT还能够轻易地应用于 object detection、predicate classification 以及 scene graph classification 等任务上 Reference 【新文速递】多模态领域的prompt工作来了！ 清华刘知远提出CPT：基于预训练视觉-语言模型的跨模态Prompt-Tuning "},"【论文】DEBUG.html":{"url":"【论文】DEBUG.html","title":"DEBUG","keywords":"","body":" 【论文】Chujie Lu, Long Chen, Chilie Tan, Xiaolin Li, and Jun Xiao. 2019. DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization（pdf） 【代码】debug模型 作者定义了两种解决 temporal grounding 的方法： 一种是 top-down approach，也就是我们常说的基于 proposal 的工作。有两种常见的 sliding-window-based 和 anchor-based， sliding-window-based 方法通过预先定义好的 multiple temporal scales 采样出一些片段，然后在 candidates 和 query 之间做匹配 anchor-based 方法基于每一帧（The frame is a general description for a frame in a video sequence or an element in a video frame feature sequence）通过 multi-scale temporal anchors 生成 candidates 另外一种是 bottom-up approach，就是逐帧预测的方式，预测每一帧作为时间边界的概率 top-down approach 模型的表现依赖于 proposal 生成的数量以及 temporal scales，但是密集采样生成大量的 candidates 又过于消耗计算资源。所以，我们需要探索 bottom-up approach，但是现有的一些 bottom-up approach 也存在一些问题 ，作者归纳为 3 点 时间边界的预测过于独立，如 Fig. 2 (a) 所示，如果单独预测时间边界的话 frame B 和 frame D 有着相似的视觉特征，所以模型会错误预测 query 对应的 segment 是 A→D，而忽略了 B→C 内容上的变化 如果只是预测两个时间边界，那么在帧数非常大的视频上，正负样本的比例就会失衡，例如在 TACoS 中一个视频平均有 9k 多帧，但是正样本可能就只有两帧，如 Fig. 2 (b) 所示 在一个网络里面同时预测一帧是否和 query 相关、是否是时间边界，这样的做法仍充满挑战 为克服上述问题，DEBUG 将 GT 中的每一帧都视为正样本，对每一个正样本通过一个 classification subnet 预测其和 query 的相关性，再通过一个 regression subnet 回归其位置到 GT 两个时间边界的距离。由于每对时间边界的预测都是基于相同的帧特征，所以将两个预测视为一个整体一起操作可以避免陷入局部最佳状态。另外，作者还提出了一种 temporal pooling 用于改善单一预测带来的不稳定表现 DEBUG 采用 QANet 作为模态交互的 backbone，具体如下所示，结构比较简单我们就不详细去解释 我们重点讲一下 Fig.4 所示的 3 个预测头， classification subnet 预测每一帧和 query 的相关度，由 4 个 1 × 3 conv layers 组成，每一层有 $D$ 个 filters，每一个 filter 后都跟一个 ReLU 激活，然后再是一个 1×3 conv layer，这一层只有 1 个 filter，最终通过 sigmoid 计算 foreground prediction score per location，对于正样本 ground truth classification label 为 $c^_i=1$，负样本为 $c^_i=0$ boundary regression subnet 预测每一个正样本帧到时间边界的双向距离，subnet 的设计和 classification 差不多，只是最终输出是两个值。假设 gt 的时间边界是 $(t_s, t_e)$，正样本在 $i$-th position，那么回归的目标 $\\mathbf t^_i=(l^_i, r^*_i)$ 就可以表示为如下形式 \r l^*_i=i-t_s,\\ r^*_i=t_e-i\r confidence subnet 预测每一帧时间边界回归的置信度，动机是不同帧的预测置信度应该是不同的，例如对终止点的检测，对于在起始点附近的帧要难很多，而对于在终止点附近帧则要容易很多。于是，作者基于每个 segment 的 centerness 对每个帧设置一个 round truth confidence，考虑回归目标 $\\mathcal t^_i=(l^_i, r^*_i)$，其 ground truth confidence score 定义为 \r e^*_i=\\frac{\\min(l^*_i, r^*_i)}{\\max(l^*_i, r^*_i)}\r 考虑所有的帧预测结果 $\\left{(\\hat c_i, \\hat{ \\mathbf t}_i,\\hat e_i)\\right}$ 以及 GT $\\left{(c_i^,\\mathbf t_i^,e_i^*)\\right}$，DEBUG 的损失函数如下 $L{cls},L{conf}$ 都采用 BCE loss，$L_{reg}$ 采用 IoU loss 在推理的时候，作者 rank all segment predictions by the score $\\hat s=\\hat c_i\\times\\hat e_i$，一种简单的方法就是去评分最大的 segment，但是从一帧预测 segment 具有很大的方差。为解决这个问题，作者提出了一个 temporal pooling 融合多个帧预测的结果，直观解释就是用预测的最左的边界和最右的边界作为最终的结果，Pooling Candidates 满足两个条件，1）预测的 segment 要与得分最高的 segment 重叠，2）预测段的分数要大于最高得分乘上 $\\delta^2$ 下面是 DEBUG 的一些实验结果和消融实验分析 "},"【论文】Decoupled Contrastive Learning.html":{"url":"【论文】Decoupled Contrastive Learning.html","title":"Decoupled Contrastive Learning","keywords":"","body":" 【论文】Yeh C H, Hong C Y, Hsu Y C, et al. Decoupled Contrastive Learning. （pdf） 这篇工作说明了一个事情 Contrastive Learning (CL) 比较难训练，难训练的原因是什么，提出了 decoupled contrastive learning (DCL) 损失来解决难训练的问题 CL 的问题可以概括为下面三点， SOTA 的模型都需要一些 unique structures，比如 momentum encoder、 large memory queues，这些东西不是太好理解 基于对比学习的 self-supervised learning (SSL) model 都需要 large batch size 和 huge epoch numbers 进行训练才能达到不错的分数，这部分需要的计算开销大的吓人 模型对超参数和优化器十分敏感，这在之前 finetune CLIP 和 CLIP4Clip 的工作中都有体现 所以，我们希望去找出导致 CL 训练难的问题所在，给出合理的解决方法。作者在文中指出他们通过大量和实验和理论分析，在 CL 的 InfoNCE loss 中发现了一个叫做 negative-positive-coupling (NPC) multiplier 的东西，记为 $q_B$，NPC multiplier 会导致 CL 训练对于 bs 很敏感 具体来说，NPC multiplier 控制着每个样本回传的梯度，在 SSL 分类任务很容易的时候，它会降低模型学习的有效性 下面我们就看一下论文中的分析吗，考虑 SimCLR 模型 输入 batch size $N$，$\\left{x_1,\\dots,x_N\\right}$，记 $\\text x_i^{(1)},\\text x_i^{(2)}$ 表示 $x_i$ 的两个 augmented views，$B$ 是 batch 中所有 augmented views 的集合，即 $B=\\left{\\text x_i^{(k)}|k\\in\\left{1, 2\\right}, i\\in[1,N]\\right}$ SimCLR 的计算流程如 Fig. 2 (a) 所示，对每个 view $\\text xi^{(k)}$ 计算交叉熵损失 $L_i^{(k)}$，总的 loss 可以表示为 $L=\\sum{k\\in\\left{1, 2\\right}, i\\in[1,N]}L_i^{(k)}$ 以，$L_i^{(1)}$ 为例，求导的下面的结果 其中 NPC multiplier $q_{B,i}^{(1)}$ 表示如下 $q_{B,i}^{(1)}$ 在 Equation 2 的每一项中都出现，它能够控制每一部分偏导得到的梯度。回到 Fig. 2 中，我可以看到， 当训练使用的正样本较分散时，负样本可能同样比较分散。此时正样本为 Hard Positive，负样本为 Easy Negative。这使得 NPC 乘数分子分母上的相似度同时减小，得到的小于 1 的 NPC 乘数会减小 Hard Positive 带来的梯度幅度 当训练使用的负样本较紧凑时，正样本可能同样比较紧凑。此时正样本为 Easy Positive，负样本为 Hard Negative。这使得 NPC 乘数分子分母上的相似度同时增大，得到的小于 1 的 NPC 乘数会减小 Hard Negative 带来的梯度幅度 当 bs 较小时，分母上对 bs中负样本相似度的求和会受限于 bs，得到更小的NPC乘数，使得梯度幅度进一步被减小 Fig. 1 量化了 NPC 乘数的这种特性， 从图 (a) 橙色线可以看出，bs 越小，$q_B$ 以较大的离散系数接近 0；bs 越大，$q_B$ 将以较小的离散系数接近 1。蓝色线则量化了 $q_B$ 对 bs 的相对波动情况，bs 越小，NPC 波动越明显 从图 (b) 可以看出尽管所有的分布都具有明显的波动，但是 bs 越小 $q_B$ 越接近 0，而 bs 越大，$q_B$ 的分布越接近 $\\delta(1)$，这和图 (a) 说明的是一个道理 因此，作者认为可以直接把 NPC 乘数从梯度中拿掉，类似于 $q_{B,N}\\rightarrow\\infty$，在 loss 中去掉正样本对就得到了 Decoupled Contrastive Learning Loss ，如下所示 进一步，可以在正样本对相似度上乘上一个权重系数得到 $L_{DCW}$ ， $w$ 是 negative von Mises-Fisher weighting function，其形式如下所示，$L{DC}$ 是 $L{DCW}$ 在 $\\sigma\\rightarrow\\infty$ 时的一个特殊情况 这么做的目的是使在出现相距很远的正样本对（Hard Positive）时，可以增大其训练信号 下面是文中的一些实验和分析，我们暂时不作出详细解读 https://img-blog.csdnimg.cn/1d08d115f2bf42128568493f1427b856.png) "},"【论文】DenseNet.html":{"url":"【论文】DenseNet.html","title":"DenseNet","keywords":"","body":"﻿* 【论文】Huang G , Liu Z , Laurens V D M , et al. Densely Connected Convolutional Networks[J]. 2016.（pdf） 【新颖点】 提出密集连接的方式 DenseNet 优点 缓解了梯度消失的问题，加强了特征传播，鼓励特征重用，大大减少了参数的数量 DenseNet 概述 DenseNet 为了保证网络层之间的最大信息流，将所有层直接彼此连接起来。为了保证前馈特性，每个层从前面的所有层获得额外的输入，并将自己的特征映射传递给后面的所有层 因为不需要重新学习冗余特征图，这种密集连接模式相对于传统的卷积神经网络需要的参数更少。传统的前馈体系结构可以看成是具有一种状态的算法，这种状态从一个层传递到下一个层。每个层从其前一层读取状态并将其写入后续层。每层改变当前的状态，也传递需要保留的信息。但是，这个过程是不可解读，我们不知道哪些信息被改变了，哪些信息没有 DenseNet 显式地将添加到网络的信息和保留信息区分开，密集网层非常窄（例如，每层只有 12 个过滤器），仅向网络的集体知识添加一小组的特征映射，而其余特征保持不练，并且最终分类器会基于网络中的所有特征作出决策 除此之外，DenseNet 还改进了整个网络的信息流和梯度，这使得其更易于训练——每个层都直接访问来自损失函数和原始输入信号的梯度 DenseNet 原理 假设输入一张图片 $X_0$，经过一个 $L$ 层的网络，其中第 $i$ 层的非线性变换记为 $H_i(\\cdot)$，$H_i(\\cdot)$ 可以是多种操作的累加，第 $i$ 层的特征记作 $X_i$ ResNet 传统前馈神经网络将第 $i$ 层的输出 $Xi$ 作为 $i +1$ 层的输入，可以写作 $X_i=H_i(X{i-1})$ 而 ResNet 增加了 shortcut，于是我们记作 $X{i+1}=H_i(X{i-1})+X_{i-1}$，如果你还记得 ResNet 梯度推导中的这个公式 x_L=x_l+\\sum^{L-1}_{i=l}F(x_i, W_i) 可以发现这个累加的操作其实是会阻碍网络中的信息流 Dense Connectivity 于是，进一步优化信息流的传播，DenseNet 提出了不同的连接方式：引入任何层到后续层的直接连接结果，即 X_l=H_l([X_0, X_1,\\cdots,X_{l-1}]) 其中 $[\\cdot]$ 表示 concatenation，将 $X0$ 到 $X{l-1}$ 层所有特征图按通道组合在一起。这里所用到的非线性变换 $H(\\cdot)$ 表示 BN + relu + conv3 的组合操作 Pooling Layer 由于在 DenseNet 中需要对不同层的特征图进行组合操作，所以不同层的特征图应该保持相同的大小。于是，这就不可能在每个层之间加入池化层。因此，采用了 Dense Block 的方式，在每个 Dense Block 之间插入卷积和池化 在同一个 Dense Block 中保持特征图的大小相同，但在不同 Dense Block 之间设置 transition layer 实现下采样 Growth Rate 在 Dense Block 中，假设每一个非线性变换 $H$ 的输出为 $k$ 个特征图，那么第 $i$ 层网络的输入便是 $k_0+(i-1)\\times k$，$k_0$ 表示输入层的通道数。前面说过 DenseNet 不同于传统结构的一个地方在于可以存在很窄的层，如 $k=12$，这个 $k$ 我们就称为网络的 growth rate，它表示了每一层输出特征图的厚度 之所以这么做是因为，在同一个 Dense Block 中每一层都与之前的层连接，我们可以把特征图看做是 Dense Block 的一个全局状态，即集体知识，那么每一层的训练目标就是通过现有的全局状态判断是否需要添加新的更新，每一层的 growth rate 就决定了需要给全局状态更新的信息多少 Bottleneck Layer 虽然 DenseNet 的 growth rate 很小，但是不同层的特征图组合在一起还是会使得通道数大大增加。为了减轻计算难度，还是引入了 $1\\times1$ 卷积层进行降维，经过改善后的非线性变换为 BN + relu + conv1 + BN + relu + conv3 Compression 为了进一步使网络变得紧凑，作者打算减少过渡层上特征图的数量。如果一个 Dense Block 包含 $m$ 个特征映射，那么就可以让紧跟着的过度层生成 $\\left \\lfloor \\theta m \\right \\rfloor$ 个输出特征，$\\theta$ 作为压缩因子在 $[0, 1]$ 内取值，当 $\\theta=1$ 时过渡层的映射特征数保持不变 网络结构 隐性监督 对于密集卷积提高网络精度的一种解释是，各层通过较短的连接从损失函数接收额外的监督 deeply-supervised nets 在每一个隐藏层都加入了一个分类器监强制从中间层学习一些有区分度的特征，这个来自损失函数的监督我们就称为深度监督 而 DenseNet 以隐性的方式执行类似的深度监督：网络顶部的单个分类器通过两个或三个过渡层对所有层提供直接监控，同时所有层之间共享相同的损失函数，因此 DenseNet 的损失和梯度就不会过于复杂 "},"【论文】DETR.html":{"url":"【论文】DETR.html","title":"DETR","keywords":"","body":"﻿* Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko. End-to-End Object Detection with Transformers. （pdf） 什么是 DETR ？ DETR 是第一个提出基于 Transformer 的端到端目标检测，没有了 NMS、anchor 等等操作，其效果和 Faster RCNN 相当（在大物体上超过了 Faster RCNN，在小物体上却不如） RCNN 系列的操作并不是一个直观的物体检测过程，因为最初的方法无法直接获取到检测对象，所以使用了 proposal + classifier 的方法——通过一些密集的方框去覆盖整张图像可能出现物体的部分，然后预测该视野范围内的物体类别以及对该视野范围进行微调 而我们人类更习惯的方式是先确定哪里有物体，然后在进一步去判断这个物体是什么。而 DETR 采用的即是这样一种直观的方法，先确定整张图像的物体情况，然后在调整具体的检测对象视野 下图是 Faster RCNN 和 DETR 的一个比较 DETR 把目标检测看成是一个 set prediction 问题——即给定一个固定小规模的目标查询集，DETR 推理目标和全局图像的关系，然后直接输出最终的预测集，最后再通过二分图匹配确定哪个框对应哪类目标 模型的整体结构 backbone 传统的 CNN 网络，输入的图片大小为 $W_0\\times H_0\\times3$，变换后输出的特征图大小为 $\\frac{W_0}{32}\\times\\frac{H_0}{32}\\times2048$ encoder 首先，通过一个 $1\\times1$ 卷积将 backbone 提取到的特征压缩到 $d\\times H\\times W\\ (H=\\frac{H_0}{32}, W=\\frac{W_0}{32})$。因为 transformer 的输入是一个序列，因此需要再通过 flatten 形成一个 $d\\times HW$ 的特征图。整个过程如下图所示 接着，我们需要关注的是 image feature 展开为 $H\\times W$ 的一维向量，这对 bbox 的预测是很有利的 由于 encoder 对序列顺序不敏感，所以在传入 encoder 之间需要将一个的特征向量和空间位置编码相加。这里的空间位置编码类似于 Faster RCNN 中的 anchor，为了让体现图像在 $x$ 和 $y$ 维度上的信息，spatial positional encoding 计算了两个维度的位置编码，然后连接到一起。为了增强对控件位置的感知，作者不仅在 encoder 输入时会添加位置编码，而且在每个 attention layer 都加入位置编码 另外，作者在实验中可视化了最后一层 encoder 的注意力图，发现 encoder 已经能够很好的区分开每个实物，这样就简化了 decoder 提取物体和定位的工作 decoder decoder 有两个输入一个是 object query（或者上一层 decoder 的输出），另一个是 encoder 的输出。与 transformer 不同的是，在 DETR 中 decoder 是并行解码。decoder 中的 object query 中是 $N$ 个 $d$ 维的向量，与 encoder 中的 spatial position encoding 类似，是学习到的位置编码，在 decoder 中的每个 attention layer 都会加入这些位置编码。object query 通过 decoder 转换为 output embedding。经过 FFN，它们被独立解码为对应的框坐标和类标签，从而得到 $N$ 个最终预测 到这里，我们需要进一步讨论一下 object queries，在 DETR 中它起着很大在作用 需要注意的是，上图的可视化结果是 COCO 验证集上所有图片的 box prediction（如下面论文中对图片的说明） 我刚开始的时候即使看完上面的整个过程，但还不是很理解 object queries 到底是如何训练得到这种对图像特殊的观察模式。其实拆分来看就很好理解了。我们仍然以左上的 object queries 中的一个 embedding 做说明，同样还是将其比作一个提问题的小人，这个小人在刚开始的时候是一个随机向量，这个随机的向量让他的兴趣点关注在了图像的左下角。然后经过第一层 decoder 训练，他的注意力发生了一定的变换，即他学习到了一些东西。接着 transformer 再向上传入第二层 decoder 中，这个小人也就带着他在第一层学到的内容到了第二层，同样第二层的输入会在来自第一层输入的基础上嵌入 object queries，那么这个小人就会带着他学习到的内容更加关注左下角一些其他还没学习到的内容。以此类推，当 Transformer 训练完成后，这个小人对图像左下角的观察就形成了一套完整的独特的观察模式，也就是上面图片左上的可视化结果 上面的可视化结果还有一个奇怪的地方，中为什么每个 slot 中间都有一条红色的竖线，但是没有一条蓝色的横线。作者将这点归因于 COCO 数据集物体分布所导致 我们现在来看一下 DETR transformer 的结构，你会上面的整个过程有更加深刻的理解 transformer 中的这种注意力机制是很神奇的 object queries 中每一个小人对不同的事物感兴趣，包括不同的类别信息和不同的区域，然后这些小人都将输出他们感兴趣的内容的最佳预测。这些小人他们的学习过程是相互作用的，也就是在 transformer 模型中序列中的每个位置可以相互关注，每个位置都可以相互通信和协作聚合图像中的信息。因此，在每一层中，他们都可以相互交流，然后在下层又重复这个过程，然后达成共识某块区域出现了某种物体。这也说明了上面可视化的结果为什么那个小人不是只关注左下一个地方，还会有其他很多关注的地方，具体说来每个都有自己主要关注的内容。更加直观的来说，就是人多力量大，每个小人学习关注自己感兴趣的地方，然后相互交流，提问题的人多了，问题千奇百怪，于是模型对全局信息的抓取就越好 上图有个很奇怪的地方，后面大象被遮住的后腿在 transformer 中也注意到了，至于为什么会注意到，最好的解释是这是模型中的一些小偏差所导致的 但是，综合上面对 encoder 和 decoder 注意力机制的描述可以看到，self-attention 和 encoder-decoder attention 使得模型可以利用 embedding 之间的成对关系进行全局的推理，同时也能够使用整个图像作为上下文联系帮助全局推理的进行 Prediction FFNs 预测头部的 FFN 包含三个部分：（1）一个 ReLU 函数；（2）$d$ 维的隐藏层的 3 层 MLP；（3）一个线性层 一边的 FFN 使用 softmax 预测最终的输出标签；另一边的 FFN 预测固定大小（$N$ 个， e.g. 100）的边界框，其中 $N$ 通常比图像中感兴趣的对象的实际数量要大很多，因此使用了一个额外的标签 $\\phi$ 表示检测到任何对象，即 background Auxiliary decoding losses 作者发现在训练的过程中使用辅助损失是很有帮助的，尤其可以使得模型对每个类输出正确的物体数量。在每个 decoder layer 后面都添加 prediction FNNs 和 Hungarian loss。所有的 prediction FFNs 共享参数。不同 decoder layer 的结果输入 FFNs 之前都先经过一个共享权重的 LayerNorm 进行处理 DETR 的损失函数 基于序列预测的思想，作者将网络的预测结果看作是一个长度为 $N$ 的固定顺序序列 $\\hat y=\\left{\\hat yi\\right}{i=1}^N$，$\\hat y_i=(\\hat c_i,\\hat b_i)$，同时将 ground truth 也视为一个序列 $y$，$y_i=(c_i, b_i)$，$c_i$ 表示该目标所属的真实类别，$b_i\\in[0,1]^4$ 表示一个四元组（ground truth box 的中心点坐标和宽高，它们都是相对图像的比例坐标）。$y$ 的长度往往都小于 $N$，用 $\\phi$ 对该序列进行填充 那么预测任务就可以看作是 $y$ 与 $\\hat y$ 之间的二分图匹配问题，采用匈牙利算法（Hungarian Algorithm）作为二分匹配算法的求解方法，于是定义最小匹配策略如下 \\hat\\sigma=\\underset{\\sigma\\in\\mathfrak{S}}{argmin}\\ \\sum_i^N\\mathcal{L}_{match}(y_i,\\hat y_{\\sigma(i)}) $\\mathcal{L}{match}(y_i,\\hat y{\\sigma(i)})$ 计算 ground truth $yi$ 和序号为 $\\sigma(i)$ 的预测值之间的matching cost，它不仅考虑了分类损失也考虑了预测的 box 和标注 box 的损失。对于序号为 $\\sigma(i)$ 的预测，我们定义 $\\hat p{\\sigma(i)}(ci)$ 表示其属于 $c_i$ 的概率，定义 $\\hat b{\\sigma(i)}$ 表示 predicted box，于是 $\\mathcal{L}{match}(y_i,\\hat y{\\sigma(i)})$ 可以描述为如下的形式 \\mathcal{L}_{match}(y_i,\\hat y_{\\sigma(i)})=-\\mathbb 1_{\\{c_i\\neq\\phi\\}}\\hat p_{\\sigma(i)}(c_i)+\\mathbb 1_{\\{c_i\\neq\\phi\\}}\\mathcal L_{box}(b_i,\\hat b_{\\sigma}(i)) 注意，物体和背景的 matching cost并不依赖于所做的预测，它们之间的 match cost 是一个固定值。同时，我们看到 matching cost 中使用的是 $\\hat p{\\sigma(i)}(c_i)$ 而不同于 Hungarian loss 中的 log-probability，原因是采用 $\\hat p{\\sigma(i)}(c_i)$ 才能使得分类损失和定位损失在同一个数量级上 接下来计算损失函数，对前面的所有配对计算 Hungarian loss \\mathcal{L}_{Hungarian}(y_,\\hat y)=\\sum_i^N\\left[-log\\hat p_{\\sigma(i)}(c_i)+\\mathbb 1_{\\{c_i\\neq\\phi\\}}\\mathcal L_{box}(b_i,\\hat b_{\\hat\\sigma}(i))\\right] 在实际操作中，考虑到物体类和背景类的不平衡（对于 $N=100$，可能会有 $90$ 多个对象都是 background），因此对于 $c_i=\\phi$ 的情况，需要对其 log-probability 除以 $10$ 来降低在损失中的权重 和 Faster RCNN 不同，DETR 不预测两个框之间的偏差 $\\Delta w.r.t$，而是直接预测 box。这样操作起来是比较简单的，但是损失函数的相对范围会有问题——对于 L1 损失，大框的一点偏差将会带来很大的损失，而小框即使偏差很大可能在数量上并不会导致很大的损失。考虑到这点，作者引入了 IoU 损失，它与框大框小并没有任何关联。于是，$\\mathcal L{box}(b_i,\\hat b{\\hat\\sigma}(i))$ 就可以定义为如下形式 \\mathcal L_{box}(b_i,\\hat b_{\\hat\\sigma}(i))=\\lambda_{iou}\\mathcal L_{iou}(b_i,\\hat b_{\\sigma}(i))+\\lambda_{L_1}||b_i-\\hat b_{\\hat\\sigma}||_1 $\\mathcal L_{iou}$ 采用 GIOU，定义为 \\mathcal L_{iou}(b_i,\\hat b_{\\hat\\sigma}(i))=1-\\left(\\frac{|b_{\\sigma(i)}\\cap\\hat b_i|}{|b_{\\sigma(i)}\\cup\\hat b_i|}-\\frac{|B(b_{\\sigma(i)},\\hat b_i)\\setminus b_{\\sigma(i)}\\cup\\hat b_i|}{|B(b_{\\sigma(i)},\\hat b_i)|}\\right) DETR 用于语义分割 如下图所示是论文中关于 DETR 用于语义分割的一个说明，首先通过 DETR encoder 得到注意力图，然后在使用 CNN 将注意力图进行放大，最后根据注意力图对每个像素进行简单的分类 关于 DETR 的思考 训练是否有可能优化？因为 transformer 总是在图片上随机的学习到东西再将它们做联系，因此需要将整张图片保留在显存中直到学习完毕才换出。每块 V100（32 G）的显存只能放下四张图片的 batch，64 个 batch 就用了 16 块 V100 实现。这样的显存占用十分吓人 "},"【论文】Fast RCNN.html":{"url":"【论文】Fast RCNN.html","title":"Fast RCNN","keywords":"","body":"﻿ 【论文】Ross Girshick. Fast R-CNN. （pdf） Why is Fast RCNN RCNN 存在的问题： 训练分多阶段进行 （1）fine-tune ConvNet；（2）fit SVMs to CovNet features；（3）learn bbox regression 训练耗时、占内存 从磁盘读出图片然后再对 object proposal 提取特征 测试时检测速度慢 测试时对每张图片的每个 object proposal 提取特征，每张图片提取 2k 个 object proposal SPPnets 在 RCNN 的基础上添加了一些共享计算的方法（sharing compution）提高了 RCNN 的速度： SPPnets 对整张图像计算特征向量，然后根据这个特征向量为每个 object proposal 做分类 对特征图上对应 proposal 的位置使用最大池化降维得到的一个固定维度的小特征图 对不同大小的输出做空间金字塔在一起形成 proposal 对应的特征 但是，SPPnets 还是存在一些和 RCNN 一样的问题——分阶段寻训练、特征写回磁盘等等 相比较之下，Fast RCNN 有如下的有点： 检测率高 一步训练，多任务损失 训练更新整个网络 不使用磁盘存储特征 Fast RCNN architecture 其工作流程为： 对整张图片做卷积提取特征 每个 object proposal 经过 RoI 池化得到一个固定大小的特征向量 特征向量经过 softmax 预测 top-K 的得分，外加一个背景分类 对每一类做 bbox regression 产生 4 个精确定位值 RoI pooling layer RoI pooling 将 RoI 在特征图上的有效的部分通过最大池化压缩为一个固定大小的小特征图（$H\\times W$，e.g. $7\\times7$），$H,W$ 作为超参数通过学习得到 每一个 RoI 对应特征图上一个矩形窗口，$(r,c,h,w)$ 给出具体的位置，$(r,c)$ 表示左上角的位置，$(h, w)$ 表示宽高。RoI 降维的时候，根据大小为 $(h/H,w/W)$ 的子窗口做最大池化 Fine-tuning for detection Multi-task loss softmax 分支输出 $K+1$ 类的预测得分 $p=(p_0,\\cdots,p_K)$（including background class） bbbox regression 分支输出 $t^k=(t^k_x,t^k_y,t^k_w,t^k_h)$，位置回归只对 $K$ 个物体分类进行，background class 不计算位置回归 于是，多任务损失就可以写做 L(p, u, t^u, v)=L_{cls}(p, u)+\\lambda[u\\geqslant1]L_loc(t^u,v) 其中，$u$ 是 ground-truth class，$t^u$ 是 bbox regression predictive tuple $t^u=(t^u_x,t^u_y,t^u_w,t^u_h)$ ，$v$ 是 ground-truth bbox regression target $(v_x,v_y,v_w,v_h)$，$[u\\geqslant1]$ 为示性函数，$u=0$ 表示背景分类不纳入回归计算，$\\lambda$ 作为比例系数平衡分类和回归的贡献 $L_{loca}(t^u,v)$ 定义为 L_{loca}(t^u,v)=\\sum_{i\\in\\left\\{x,y,w,h \\right\\}}smooth_{L_1}(t_i^u-v) 其中 smooth_{L_1}(x)=\\left\\{\\begin{matrix}\r 0.5x^2& if\\ |x| Mini-batch Fast RCNN 训练时随机梯度采用分层抽样的方法，即抽取 $N$ 张图片，每张图片有 $R/N$ 个 RoI，于是，一个 mini-batch 总共有 $R$ 个样本 其中，75% 的 RoI 与 ground-truth box 的 IoU 大于 0.5，75% 的 IoU 在 $[0.1, 0.5)$ 之间，属于背景分类 Back propagation through RoI pooling layers 记 $xi\\in\\mathbb R$ 表示第 $i$ 个激活到 RoI poolig 的输入，$y{rj}$ 表示第 $r$ 个 RoI 在 第 $j$ 层上的输出。RoI pooling 计算最大池化，$y{rj}=x{i^(r, j)}$，$i^(r, j)=\\underset{i'\\in\\mathcal R(r,j)}{argmax}\\ x{i'}$，$\\mathcal R(r,j)$ 表示 $y{rj}$ max pooling 对应的子窗口 于是， RoI pooling layer 的反向传播就可以定义为下面的形式 \\frac{\\partial L}{\\partial x_i}=\\sum_r\\sum_j[i=i^*(r, j)]\\frac{\\partial L}{\\partial y_{rj}} 这个公式很好理解，还记得 max pooling 的反向传播吗？max pooling 的反向传播可以简单理解为将梯度只沿最大的数回传，其实 RoI pooling 也是这样 通过上面图也很好理解，一个 $xi$ 可能对应几个不同的 $y{rj}$，因为 RoI 之间存在重叠情况 truncated SVD for faster detection 如果是一个普通的分类网络，那么全连接层的计算应该远不及卷积层的计算。但是，针对物体检测任务，Fast RCNN 在 ROoI pooling 后得到的每个region proposal 都要经过几个全连接层，这使得全连接层的计算占网络的计算将近一半，如下图，所以作者采用 SVD 分解来简化全连接层的计算 一层的权重矩阵 $W$ 的大小为 $u\\times v$，可以通过下面公式近似计算 W\\approx U\\sum_tV^T 其中，$U$ 是一个 $u\\times t$ 的矩阵包含 $W$ 的 first $t$ left-singular vectors，$\\sum_t$ 是一个 $t\\times t$ 的对对角矩阵，包含 $W$ 的 top signular value， $V$ 是一个 $v\\times t$ 的矩阵包含 $W$ 的 first $t$ right-singular vectors 于是，通过 SVD 分解，参数量从 $uv$ 降到了 $t(u+v)$，如果 $t \\ll min(u,v)$ 这样做是非常有意义的 Why Fast RCNN works 为什么 Fast RCNN 的检测率更高？ Fast RCNN 对整张图像做卷积提取特征，然后再是在特征图上框出 RoI，这样前向传播有一个更大的感受野，也能更加有效地更新整个网络的参数，这也就是作者说的 feature sharing during training 之前的 RCNN 和 SPPnets 并不是这样做的，它们从不同的图像中取得 RoI 放入网络进行训练。但是，事实上每一个 RoI 都有可能有一个很大的感受野，这个感受野可能覆盖整张图片。于是，这就导致了模型在反向传播时更新效率很低 为什么 Fast RCNN 的检测速度更快？ 每个 RoI 是从已经卷积好的特征图上抽取得到，然后经过 RoI polling 和一些全连接就可以进行分类和回归，同时卷积层使用 SVD 分解也在一定程度上加快了全连接层的计算 下面是 Fast RCNN 的训练过程和测试过程 "},"【论文】Faster RCNN.html":{"url":"【论文】Faster RCNN.html","title":"Faster RCNN","keywords":"","body":"﻿* 【论文】Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.（pdf） 整体框架 Faster RCNN 可以分为 4 个大模块; Conv Layers 用于图片的特征提取，经过一系列的 conv + relu + pooling 的特征映射 Region Proposal Networks 用于推荐候选区域，替代之前在 RCNN 或 Fast RCNN 中的 search selective，通过 softmax 判断 anchor 属于正样本还是负样本，然后利用 bbox regression 修正 anchor 获得精确的 proposal RoI Pooling 和 Fast RCNN 一样，将不同大小形状的 proposal 转换为固定大小的输出 proposal feature Classification 利用 proposal feature 计算 proposal 的类别，同时再次使用 bbox regression 获得加测框的最终精确位置 下面展示了 Faster RCNN 的细节处理，假设输入一张任意大小的图片 $P\\times Q$ 首先，缩放至固定大小 $M\\times N$，然后再将图片送入网络 Conv Layers 包含了 13 个卷积层（kernel_size = 3, padding = 1, stride = 1） + 13 个 ReLU + 4 个 池化层（kernel_size = 2, padding = 0, stride = 2） RPN 首先经过 $3\\times3$ 的卷积，分别生成 positive anchor 和对应的 bbox regression，然后计算出需要的 proposal RoI Pooling 则利用 proposal 对应 feature map 得到 proposal feature，后续送入全连接网络通过 softmax 做分类 Conv Layers Faster RCNN 中对所有卷积都进行了 padding 处理（padding = 1），于是经过 $3\\times3$ 的卷积后输出图的大小并不发生改变（经过卷积后 $(M-3+2\\times1)/1+1=M$） 这样的好处是，每次经过池化层图片大小缩小为原来的一半，即 $\\frac{M}{2}\\times\\frac{N}{2}$，而卷积和 relu 不改变图片大小，于是在经过 Conv Layers 后，图片变为 $\\frac{M}{16}\\times\\frac{N}{16}$，这样在 RoI Pooling 时将非常容易对齐 Anchor Generation Layer Fast RCNN 中会反复提到一个概念——anchor，所谓 anchor 其实就是一组矩形框，9 个矩形有 3 种形状，长宽比大约为 $1:1、1:2$ 和 $2:1$，实际上通过关 anchor 就引入了检测中常用的多尺度方法 在 Conv Layers 输出的 feature map 的每个点都配备这 9 个 anchor 作为初始的检测框，这样获得的检测框存在不准确的问题也不用太担心，后面会有两次机会通过回归修正检测框的位置 我们对论文图片做如下两点解释： 首先，Conv Layers 中的 conv5 的输出维度为 256，也就是有 256 张特征图，选用这一层的特征做 RPN 的输入，那么在每个点上维度都是 256 假设每个点有 $k$ 个 anchor，每个 anchor 分为 positive 和 negative，那么分类得分就是 $2k$；同时，每个 anchor 都有 $(x, y, w, h)$ 4 个偏移量，所以会有 $4k$ coordinates 于是，接下来 anchor generation layer 的操作就是在原图尺度上生成密密麻麻的候选 anchor，论文中原图的大小为 $800\\times600$，在通过 Conv Layer 后，feature map 的大小就是 $50\\times38$（向上取整），那么就会生成 $50\\times38\\times9=17100$ 个 anchor Bounding Box Regression 如下图所示，红色的框为提取的 positive anchor box，绿色的框为 ground truth box，即便红色的框可以识别出飞机，但是由于定位不准确，我们需要 bbox regression 对红色的框进行微调，使得其更加接近于 ground truth 对于一个 box 使用 $(x,y,w,h)$ 表示，分别表示中心点的位置和宽高。下图中还是用红色的框表示 positive anchor box $A=(A_x,A_y,A_wA_h)$，绿色的框表示 ground truth box $G=(G_x,G_y,G_w,G_h)$，我们希望寻得一种关系可以使 $A$ 经过映射得到一个跟 ground truth box 更接近的 regression box $G'=(G'_x,G'_y,G'_w,G'_h)$ 我们先做平移操作 \r G'_x=A_w\\cdot d_x(A)+A_x\\\\\r G'_y=A_h\\cdot d_y(A)+A_y\r 再做缩放 \r G'_w=A_w\\cdot exp(d_w(A))\\\\\r G'_h=A_h\\cdot exp(d_h(A))\r 其实，我们需要学习的参数只有 4 个 $d_x(A),d_y(A),d_w(A),d_h(A)$，在 anchor box 与 ground truth box 之间的差异不太大时，我们可以将其看作是一种线性变换，那么就可以使用线性回归来微调（差异过大时，这时将是一个非线性的问题） 线性变换可以表示为 $Y=WX$，输入是 anchor 对应的 feature map 组成的特征向量 $\\phi(A)$，训练同时传入 $A$ 与 $G$ 之间的差异 $(t_x,t_y,t_w,t_h)$，那么变换的目标函数就可以表示为 \r d_*(A)=W_*^T\\cdot\\phi(A)\r 为了让预测值 $d*(A)$ 与真实值 $t*$ 的差距最小，设计 L1 损失函数 \r Loss=\\sum_i^N|t_*^i-W_*^T\\cdot\\phi(A^i)|\r 函数优化目标为 \r \\hat W_*=\\underset{W_*}{argmin}\\ \\sum_i^N|t_*^i-W_*^T\\cdot\\phi(A^i)|+\\lambda||W_*||\r 实际中使用 smooth-L1 损失 \r smooth_{L1}(x)=\\left\\{\\begin{matrix}\r \\frac{\\sigma^2x^2}{2} & ||x| 清楚了 bbox regression 下面我们要做的就是在 RPN 的第二条路径对 proposal 进行 bbox regression，论文中通过 $1\\times1$ 的卷积实现，卷积的输出为 36，刚好对应一个点 9 个 anchor，每个 anchor 有 4 个回归偏量 Region Proposal Network Faster RCNN 抛弃传统的滑动窗口和 ss（Selective Search）方法生成检测框，直接采用 RPN 生成检测框。RPN 网络分为两条线，一条通过 softmax 分类判断 anchor 是正样本分类还是负样本分类，一条计算 anchor 的 bbox regression 偏移量。后面的 proposal 层负责综合 positive anchor 和回归的偏移量获得 proposal，同时提出太小或超出边界的检测框 RPN 训练的整个损失函数可以表示为 \r L(\\left\\{p_i\\right\\},\\left\\{t_i\\right\\})=\\frac{1}{N_{cls}}\\sum_iL_{cls}(p_i,p_i^*)+\\lambda\\frac{1}{N_{reg}}\\sum_ip_i^*L_{reg}(t_i,t_i^*)\r $p_i$ 表示 positive softmax probability；如果第 $i$ 个 anchor box 与 ground truth box 的 IoU > 0.7 则 $p_i^=1$ ，如果 IoU =0$，至于，IoU 落在 0.3 到 0.7 之间的 anchor 则不参与训练；$t$ 表示 predicted bounding box，$t^*$ 代表 positive anchor 对应的 ground truth box 回归损失乘以了一个系数 $pi^*$ 表示我们只关心 positive anchor 的损失。由于实际计算中，$N{cls}$ 和 $N_{reg}$ 的数量级相差较大，所以引入了平衡因子 $\\lambda$ 如前面所述，$L_{reg}$ 使用 smooth-L1 损失 \r L_{reg}(t_i,t_i^*)=\\sum_{i\\in\\left\\{x,y,w,h\\right\\}}smooth_{L_1}(t_i-t_i^*)Proposal layer\r \r t_x=(x-x_a)/w_a,\\ t_y=(y-y_a)/h_a\\\\\r t_w=log(w/w_a),\\ t_h=log(h/h_a)\\\\\r t_x^*=(x^*-x_a)/w_a,\\ t_y^*=(y^*-y)/h_a\\\\\r t_w^*=log(w^*/w_a),\\ t_h^*=log(h^*/h_a)\r 其中，$x,x_a,x^*$ 分别对应 predicted box，anchor box 和 ground truth box proposal layer proposal layer 负责综合回归偏量和 positive anchor，计算出精确的 proposal。除了回归偏量和 positive anchor 外，proposal layer 还有一个输入 im_info，以及一个参数 feature_stride = 16 feature_stride 记录了 Conv Layers 的缩放程度，即经过 4 次池化，每次缩小为原来的一半，最终的 feature map 为 $\\frac{M}{16}\\times\\frac{N}{16}$ 对于一张任意大小 $P\\times Q$ 的图像都会缩放到固定大小 $M\\times N$，im_fo 记录的则是这一步的缩放信息，即 $im_fo = [M, N, scale_factor]$ proposal layer 具体的工作流程如下： 利用回归偏量对所有的 anchor 做位置修正生成 anchor 按照输入的 positive softmax socre 对 anchor 进行排序，然后提取前 12k 个（测试时 6k）修正位置后的 positive anchor 限定超出图像边界的 positive anchor 为图像边界，防止后续 RoI Pooling 时 proposal 超出图像边界 剔除尺寸非常小的 positive anchor 对剩余的 positive anchor 进行非最大抑制，然后取前 2k 个（测试时300 个） positive anchor 最后取挑出 128 个正样本和 128 个负样本用于训练 值得一提的是这里 proposal layer 的输出是对应 $M\\times N$ 输入尺度的，而不是 feature map 上的尺度 RoI Pooling 为什么需要 RoI Pooling 呢？ 在历史上，网络训练好后输入图像的尺寸也就固定了，对于输入大小不同的图像有两种解决方法：（1）从图像中截取出固定大小的区域；（2）将图像 warp 成固定大小。但是这两种方式都存在一定的问题，第一种破坏了图像的完整结构，第二种破坏了图像的原始形状信息 而 RoI Pooling 的提出正是为了解决这些问题，对大小形状不同的 proposal 进行一个统一化的处理。RoI Pooling 在 Fast RCNN 中就已经提出，大致流程也是相同的： 由于前面传入的 proposal 尺寸对应 $M\\times N$，所以先将其映射回 $\\frac{M}{16}\\times\\frac{N}{16}$ 的 feature map 大小进行讨论 将每个 proposal 对应的区域分为 $pooled_w\\times pooled_h$（e.g. $7\\times7$）大小的网格 最后进行 max pooling 操作 Classification Classification 通过前面的 proposal feature map 经过全连接层和 softmax 计算每个 proposal 对应每个类别的得分。同时，在 Classification 中会再次利用 bbox regression 对每个 proposal 的位置进行精修 Faster RCNN 训练 RPN 和 Fast RCNN 都会要求利用 CNN 网络提取图像特征，所以论文的做法是使 RPN 和 Fast RCNN 共享同一个卷积网络。于是，Fast RCNN 的训练类似于一种弄迭代的过程： 先训练 RPN，然后使用得到的候选区域训练 Fast RCNN 接着使用 Fast RCNN 中关于卷积网络的部分去初始化 RPN，然后再次训练 RPN，这里不跟新关于卷积网络的内容，仅更新 RPN 特有的部分 最后再次训练 Fast RCNN，这里也是不更新关于卷积网络的部分，仅更新 Fast RCNN 特有的层 （现在，在 github 上开源的实现大多采用近似联合训练 approximate joint training，采用端到端的一步训练） 下面两幅图展示了 Fast RCNN 的完整流程 "},"【论文】FEWVLM.html":{"url":"【论文】FEWVLM.html","title":"FEWVLM","keywords":"","body":" 【论文】Jin W, Cheng Y, Shen Y, et al. A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models.（pdf） What is FEWVLM 作者希望通过 low resource 学习到一个相对较小的 VLM 可以很好地实现 zero-/few-shot learning，同时，作者也发现 task-specific prompts 对于 zero-/few-shot learning 非常重要。那么接下来问题就来了，Q1）如何设计一个有效的 prompt 呢？prompt 又是如何影响 new tasks 上的 zero-/few-shot learning ？这些都是作者在本文中的工作。除此之外，作者还围绕 Q2）预训练任务对 zero-/few-shot learning 的影响、Q3）模型表现和数据量的关系这两点展开了深入的研究 我们先介绍 FEWVLM 的结构，然后在详细介绍作者是怎么围绕这三个问题展开工作的。如下图所示，FEWVLM 采用 VL-T5 的模型结构，如果你熟悉 VL-T5 那么自然也熟悉下面这个训练目标 \r \\begin{equation}\r L_{\\theta}=-\\sum_{j=1}^{|y|}\\log P_{\\theta}(y_j|y_{ 对于 Q1），作者将 VQA 和 Captioning 两个任务都视为生成式任务，对于 VQA，通过 sentinel token 来提示答案的生成；对于 Captioning，则通过一个前缀 『a image of』进行提示，这类似于 simVLM Table 1. 详细展示了作者针对两个任务设计的 prompts，对于 Captioning 作者认为即使 picture、photo、image 三个词的意思基本一样，但是对于 zero-/few-shot learning 产生的效果则是不同的 记 prompt template 为 $\\mathcal P$，那么 input text 和 target text 就可以记为 $x, y=\\mathcal P(\\text{input, label})$，然后在 P-tune 时按照公式（1）最小化 negative log-likelihood，在 inference 时，除去 target prompt template 中多余的部分即可得到预测的 label 对于 Q2），作者设计了两种预训练任务—— PrefixLM 和 MaskedLM PrefixLM 随机地将文本分为两部分，一部分作为输入，另一部分作为 decoder 生成的 text target MaskedLM 和以往的 MLM 比较像，以 15% 的概率随机替换为 numbered sentinel tokens，如 FEWVLM 预训练数据集采用 MSCOCO 和 VG，和 VL-T5 不同的是，FEWVLM 并不将 VQA 的数据集纳入到预训练数据集中，确保模型在预训练期间没有针对 VQA 任务的学习 对于 Q3），作者研究了不同大小 few-shot training set 对模型表现的影响 在实验中，FEWVLM 得到了和比它大 246 倍的模型近似的结果，作者也发现， prompts 对 zero-shot learning 有明显的影响，但是对 few-shot learning 则不是很显著 MaskedLM 对 VQA shot-learning 有帮助，PrefixLM 则能增强模型在 Captioning 任务上的表现 当 few-shot training set 增大到一定数量时，模型的表现会急剧增加 "},"【论文】FitCLIP.html":{"url":"【论文】FitCLIP.html","title":"FitCLIP","keywords":"","body":" 【论文】FitCLIP: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks（pdf） 作者的目的是利用 CLIP 本身优越的 zero-shot capabilities 来提高 zero-shot video recognition and retrieval，但是作者认为 CLIP4Clip 或 ActionCLIP 这种全 finetune 的方式又会损伤 CLIP 本身的 zero-shot capabilities，所以提出了一种利用少量 labeled video 和大量 pseudo-labeled video 的方法来 finetune CLIP，使其具有强大的 zero-shot video understanding 能力。新的模型叫做 FitCLIP, 如下图所示 FitCLIP 按照 Teacher-Student fashion 训练模型，teacher model 是 large-scale pre-trained image-language model，也即 CLIP model，student model 是在 video data 上新训好的一个模型，整个过程分为两步： 利用 labeled video-text pairs 和 teacher model 产生的 pseudo-labels 来训练模型 将 teacher model 已有的知识和 student model 的知识融合 Data Subsets 如前所述，用于模型训练的数据分为两部分，一部分是 labeled video-text pairs，这一部分每个视频必有一个对应的人工标注的 text describing；另一部分是 unlabeled video-text candidate pairs，有一组视频和文本描述，对应视频的最佳描述是不存在的 Teacher Model Teacher model 的目的是对 unlabeled video-text candidate pairs 提供一组 soft pseudo labels，具体来说，在视频中取 $N$ 帧输入到 CLIP 的 visual encoder，然后通过 mean-pool 得到一个 visual feature，之后再计算视频特征和文本特征之间的相似度，以此相似度得分作为 target soft pseudo labels Student Model Student model 的目的有两个：一是通过 video-text pairs 进行学习，主要建模video-text 之间的对应关系；二从 teacher knowledge 蒸馏出适用的知识。因此，在训练时采用两种监督：manually labeled video-text pairs 和 soft pseudo-labels from the unlabeled set。至于模型结构、输入视频和输出特征的处理方式和 teacher model 一致 Student’s Training Objective 前面提到 student model 有两种监督， 对于 labeled samples 采用 InfoNCE loss，minimize both the text-to-video and video-to-text contrastive losses，记最终的 labeled loss 为 $(\\mathcal L{v2t} + \\mathcal L{t2v})$ 对于 soft pseudo-labels 计算 student's scores 相对于 teacher 的 cross-entry，如下公式所示，其中 $x_v,x_t$ 表示 teacher model 的 video representation 和 text representation，$z_v,z_t$ 表示 student model 得到的两个特征 \r \\mathcal L_{distill,v2t}=\\sum_{(v, t)\\in B_l}\\frac{e^{x_v\\cdot x_t/\\sigma}}{\\sum_{x\\in T}e^{x_v\\cdot x/\\sigma}}\\log\\frac{e^{z_v\\cdot z_t/\\sigma}}{\\sum_{z\\in T}e^{z_v\\cdot z/\\sigma}}\\\\\r \\mathcal L_{distill,t2v}=\\sum_{(v, t)\\in B_l}\\frac{e^{x_\\cdot x_t/\\sigma}}{\\sum_{x\\in V}e^{x\\cdot x_t/\\sigma}}\\log\\frac{e^{z_v\\cdot z_t/\\sigma}}{\\sum_{z\\in V}e^{z\\cdot z_t/\\sigma}}\\\\\r 最终，FitCLIP 的损失函数为 \r \\mathcal L = (\\mathcal L_{v2t} + \\mathcal L_{t2v}) + \\lambda(\\mathcal L_{distill,v2t} + \\mathcal L_{distill,t2v})\r Fusing Teacher-Student Knowledge 作者采用 elegant weight-space ensembling techniques 来融合 teacher model 保留 visual knowledge 和 student model 学到的 video-specific properties，具体操作和 『Robust fine-tuning of zero-shot models』48 一样（linearly combine the teacher and student weights (by $\\alpha$)） 下面是一些实验结果，后面再做出详细解读 48. Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903, 2021. ↩ "},"【论文】Florence.html":{"url":"【论文】Florence.html","title":"Florence","keywords":"","body":" 【论文】Florence: A New Foundation Model for Computer Vision（pdf） 文艺复兴的佛罗伦萨（Florence） 面对多样化和开放的现实世界，要实现 AI 的自动视觉理解，就要求计算机视觉模型能够很好地泛化，最小化对特定任务所需的定制，最终实现类似于人类视觉的人工智能。计算机视觉基础模型在多样化的大规模数据集上进行训练，可以适应各种下游任务，对于现实世界的计算机视觉应用至关重要。 现有的视觉基础模型，如 CLIP (Radford et al., 2021)、ALIGN (Jia et al., 2021) 和悟道 2.0 等 ，主要侧重于将图像和文本表征映射为跨模态共享表征。 近日来自微软的研究另辟蹊径提出了一种新的计算机视觉基础模型 Florence，将表征从粗粒度（scene）扩展到细粒度（object），从静态（images）扩展到动态（videos），从 RGB 扩展到多模态（caption，depth）。 通过结合 Web-scale image-text data 的通用视觉语言表征， Florence 模型可以轻松地适应各种计算机视觉任务，包括分类、检索、目标检测、视觉问答（VQA）、图像描述、视频检索和动作识别。 此外，Florence 在许多迁移学习中也表现出卓越的性能，例如 fully sampled fine-tunning、linear probing、few-shot transfer 和 zero-shot transfer，这些对于视觉基础模型用于通用视觉任务至关重要。Florence 在 44 个表征基准测试中多数都取得了新的 SOTA 结果，例如 ImageNet-1K 零样本分类任务，top-1 准确率为 83.74%，top-5 准确率为 97.18%；COCO 微调任务获得 62.4 mAP，VQA 任务获得 80.36 mAP。 foundation models，指从大规模广泛数据习得的能够适应（e.g. fine-tuned）许多下游任务的模型 fundation models 重要是因为其惊人的表现和超强的泛化能力 NLP 中有许多这样的 fundation models，例如 BERT、GPT3，但是 CV 中标准的做法还是在标准的数据集（ImageNet）上预训练。最近一些超大规模（web-scale）数据集训练的模型 CLIP、ALIGN、Wu Dao 在前移学习中表现出来很大的进步，但是这些模型都仅限于图像、文本的映射，如分类、检索、标记 为了探索 CV 中的 fundation model，作者从 Fig 1. 给 fundation 一词作出了更细致的说明，如前所述三个坐标轴代表了从粗粒度到细粒度、从静态到动态、从 RGB 到 multiple senses we redefine foundation models for computer vision to be a pre-trained model and its adapters for solving all vision tasks in this Space-Time-Modality space, with transferabil- ity such as zero-/few-shot learning and fully fine tuning, etc 作者期望对于这样的一种 fundation models 的迁移适应性配置应该最小，continuing training, few epochs or few layers for fine tuning without significantly increasing or changing model parameters Data curation 由于大规模数据多样化对基础模型非常重要，因此该研究提出了一个包含 900 million image-text pairs 的新数据集（FLD-900M）用于训练。由于网络爬取数据通常是具有噪音的自由格式文本（e.g. , word, phrase or sentence），为了获得更有效的学习，该研究使用了 UniCL（a unified image-text contrastive learning objective） FLD-900M 包括 900M 张图片，900M个自由格式的文本，9.7M 的 unique queries 以及 7.5B tokens 在 FLD-900M 中有 350M 个图文对并不是一张图片对应唯一一个文本，如果同一文本对应多张图片，那么在对比学习中这些图片都可以被视为正样本。为解决这个问题作者利用了UniCL，让Florence在一个 image-label-description空间中进行训练 Model pretraining 为了从图像 - 文本对中学习良好的表示，该研究使用了包括图像编码器和语言编码器的两塔式（two-tower）架构。对于图像编码器，该研究选择了 hierarchical Vision Transformer。该研究所提架构在继承了 Transformer self-attention 操作性能优势的同时，这些分层架构对图像的尺度不变性进行了建模，并且具有相对于图像大小的线性计算复杂度，这是进行密集预测任务（如，object detection and segmentation）必不可少的属性。 作者将 Swin 的 patch embedding 模块和 patch merging 模块替换为 CvT 中的 convolutional layer 得到 CoSwin 作为 hierarchical vision transformer，记为 CoSwin-H，将 CoSwin 的结果过 global average pooling 得到图像特征，image encoder 和 text encoder 之后采用一个线性映射得到维度相同的图像特征和语言特征 Task adaptations 为了让 Florence 能够更好的适配到下游任务，沿着 Fig 1. 的坐标系，在 space axis 上采用了 dynamic head adapter，在 time axis 上采用了 video CoSwin adapter，在 modality axis 上采用了 METER adapter Florence 旨在通过小样本和零样本迁移学习来有效适配开放世界，并通过很少的 epoch 训练（例如在检索中）进行有效部署 为了获得细粒度的 object-level 表征，作者在预训练好的 image encoder 后接一个 Dynamic head。通过 CoSwin-H 可以得到不同 scale level 的输出特征金字塔，将其 concate 在一起，通过 scaled-down 或 scaled-up 可以得到 $level\\times space \\times chanel$ 的三维张量。而 Dynamic Head 的核心就在于使用三个注意力机制分别作用于这个张量的三个维度（level-wise，spatial-wise，channel-wise）。比起一个注意力机制作用于三个维度，Dynamic Head 的计算资源消耗更少，且效率更高。这三个注意力机制按顺序进行，可以将它们堆叠起来构成一个 three attention layer，如 Fig 3. 所示，Dynamic Head 通过一阶段的 ATSS 框架和损失训练得到 作者还构建了一个 FLOD-9M 的目标检测数据集用于 object detection pre-training，该数据融合了几个抑制的目标检测数据集 COCO、LVIS、OpenImages 和 Object365，同时，作者还在 ImageNet-22K 上生成了一些 pseudo bbox，进一步增大了数据 Training infrastructure 从能源和成本方面考虑，以尽可能低的成本构建基础模型是至关重要的。该研究开发了可扩展的训练基础设施，以提高训练效率。Florence 的 training infrastructure 包括了 ZeRO、activation checkpointing、mixed-precision training、gradient cache 等多项关键技术，从而大大减少了内存消耗，提高了训练吞吐量 "},"【论文】Frozen.html":{"url":"【论文】Frozen.html","title":"Frozen","keywords":"","body":" 【论文】Tsimpoukelli M, Menick J, Cabi S, et al. Multimodal few-shot learning with frozen language models.（pdf） What is Frozen 自回归 transformer 模型在少样本的情况下可以表现出很好的效果，例如：通过提示，可以快速学习新的任务；利用相关的上下文，可以快速检索出相关的知识；给定新单词的意义后，能够正确的使用它们。但是对于视觉或其他不是文本的模态，这种大规模的 language model（LM）就表现得像是 blind 一样（即无能无力） 那么，如何将 transformer LM 的这种能力迁移到多模态场景下呢？作者提出了 Frozen 的方式，给定一个预训练好的 transformer LM，然后通过 image encoder 将图片转换为连续的 embedding 序列输入到 transformer 中，将 transformer 冻住（不更新权重），仅训练 image encoder，如 Fig 2. 所示 更近一步，尽管 Frozen 是在一个图文对输入的情况下训练的，但是其同样可以处理多个图文对的输入，这样就可以输入新的多模态任务样本来提示 Frozen，或者教 Frozen 如何在给定 new visual category name 的情况下正确使用这些新单词（如Fig 3. 所示），如此一来 Frozen 成为了一个 multimodal few-shot learner 总结来说，在这篇了论文中作者的工作大致如下： 提出了 Frozen 的方式来训练带视觉功能的 large LM，这样不仅保留了 LM 自身的能力，也使其能够处理任意的图文对输入序列 Frozen 训练后的模型很好地将 LM 的 rapid task adaptation、encyclopedic knowledge 和 fast concept binding 转移到了跨模态的情形下 需要强调的是，Frozen 的目的不是涨分（在很多情况下其表现远低于 SOTA），其最主要的意义是 few-shot learning，面对一个真正的 open-ended and unconstrained linguistic interpretation of images 的研究 The Frozen Method Architecture 作者在 C4 数据集上训练了一个 7b 参数量的 transformer 作为 PTM text encoder 记为 $g{\\theta}$，将文本离散后的 tokens $\\mathbf y=y_1,y_2\\cdots,y_L$ 一个个转化为连续的 embedding $t_l:=g{\\theta}(y_l)$ 记 transformer 为 $f{\\theta}$，输出为一个 logits 向量——即词汇表上参数化的一个分类分布向量，记为 $p{\\theta}(\\mathbf y)$ \r \\begin{align}\r \\log p_{\\theta}(\\mathbf y)&=\\sum_l\\log p_{\\theta}(y_l|y_1,y_2,\\cdots,y_{l-1})\\\\\r &=\\sum_lf_{\\theta}(t_1,t_2,\\cdots,t_{l-1})_{y_l}\r \\end{align}\r Vision encoder 采用 NF-ResNet-50，记 $v_{\\phi}$ 表示函数将原始图像转换为连续的 transformer 输入序列 Frozen 有点类似于 prefix tunning，简单来说，prefix tunning 训练一个针对于具体任务的连续 bias term，这有点类似于恒定不变的 embedding，但是不同于 prefix tunning 的是 Frozen 会动态生成这个前缀，它不是恒定不变的 bias，而是根据输入条件由 vision encoder 生成的 activation 也就是说，作者把 vision encoder 的输出视为是一个 visual prefix，每个 embedding 的维度为 $D$，总共有 $n$ 个 embedding 输入到 transformer 中，通过实验作者发现 $n=2$ 模型表现最佳 Training 在训练时候仅更新 vision encoder 的参数 $\\phi$，使用 CC 数据集进行训练。和标准的 captioning systems 一样，记下面公式中的 $\\mathbf x$ 为 $v{\\phi}(\\mathbf x)=i_1,i_2,\\cdots,i_n$，冻住参数 $\\theta$，更新 $\\phi$ 来最大化 $p{\\theta}(\\mathbf y)$ \r \\begin{align}\r \\log p_{\\theta,\\phi}(\\mathbf y|x)&=\\sum_l\\log p_{\\theta,\\phi}(y_l|\\mathbf x, y_1,y_2,\\cdots,y_{l-1})\\\\\r &=\\sum_lf_{\\theta}(i_1,i_2,\\cdots,i_n,t_1,t_2,\\cdots,t_{l-1})_{y_l}\r \\end{align}\r 梯度回传时 visual prefix 中的每个 $ik$ 接受梯度 $\\sum\\limits_l \\triangledown{ik}f{\\theta}(i1,i_2,\\cdots,i_n,t_1,t_2,\\cdots,t{l-1})_{y_l}$ transformer 的输入如前所述有些时候会是多个图文对，所以在位置编码上作者采用了相对位置编码，保证 image 相对于 text 在前面，保持 prefix 的特性 "},"【论文】GAN.html":{"url":"【论文】GAN.html","title":"GAN","keywords":"","body":"﻿* 【论文】Goodfellow I J , Pouget-Abadie J , Mirza M , et al. Generative Adversarial Networks[J].（pdf） Generation 生成（generation）就是模型通过学习一些数据，然后生成类似的数据 以前很早就有生成技术，比如自编码器 我们训练一个编码器，然后将图片转换为编码，然后再训练有一个解码器，吧编码转换为一张图片，接着计算得到图片和输入图片之间的 MSE（mean square error），训练完成之后，我们可以只取后半部分 NN Decoder 用来将一个随机的编码转换成图片 显然生成的图片看起来就很假，这里存在一个为，MSE 导致的问题：我们通过 MSE 来衡量输入图片和生成图片之间的相似度，MSE 计算每个像素上的均方差，但是真的 MSE 越小就越相似吗？ 对于上面的两张图片，我们认为左边的是好的生成的，但是在计算 MSE 时的结果确实一样的，因此认为两张图片都是生成好的结果 正是这样的原因，我们就有了 GAN GAN GAN 有两个网络，一个是 generato，一个是 discriminator，通过二者的博弈来达到最好的生成效果 先有一个初代的 generator 生成一些很差的图片，然后也有一个初代的 discriminator 能够准确的区分开生成图片和真实图片，这个 discriminator 可以看做是一个二分类器 接着，我们训练第二代、第三代等等，到最后 discriminator 无法分辨真实图片和生成图片，那么这个网络就拟合了 从最大似然估计说起 最大似然估计要解决的问题 存在一个数据分布 $P_{data}(x)$，这个分布往往是未知的 给定一个有参数 $\\theta$ 定义的数据分布 $P_G(x;\\theta)$ 我们希望求得参数 $\\theta$ 使得 $PG(x;\\theta)$ 尽可能接近 $P{data}(x)$ 最大似然估计的解决方案 从 $P_{data}(x)$ 采样 $m$ 个样本 $\\left{x^1, x^1,\\cdots, x^m\\right}$ 计算采样样本的似然函数 $L=\\mathop{\\Pi}\\limits_{i=1}^mP_G(x^i,\\theta)$ 计算使得似然函数 $L$ 最大的参数 $\\theta$：$\\theta^*=\\underset{\\theta}{argmax}\\ L=\\underset{\\theta}{argmax}\\ \\mathop{\\Pi}\\limits_{i=1}^mP_G(x^i,\\theta)$ 接着，我们有 $\\begin{aligned} \\theta^*&=\\underset{\\theta}{argmax}\\ L \\ &= \\underset{\\theta}{argmax}\\ \\mathop{\\Pi}\\limits{i=1}^mP_G(x^i,\\theta) \\ &=\\underset{\\theta}{argmax}\\ log\\left[\\mathop{\\Pi}\\limits{i=1}^mPG(x^i,\\theta)\\right] \\ &=\\underset{\\theta}{argmax}\\ \\sum\\limits{i=1}^mlog\\left[PG(x^i,\\theta)\\right] \\ &\\approx E{x\\sim P{data}}\\left[logP_G(x^i,\\theta)\\right] \\ &=\\underset{\\theta}{argmax}\\ \\int_xP{data}(x)logPG(x;\\theta)dx \\ &=\\underset{\\theta}{argmax}\\ \\int_xP{data}(x)logPG(x;\\theta)dx-\\int_xP{data}(x)logP{data}(x)dx \\ &=\\underset{\\theta}{argmax}\\ \\int_xP{data}(x)\\left(logPG(x;\\theta)-logP{data}(x)\\right)dx \\ &= \\underset{\\theta}{argmin}\\ \\intxP{data}(x)log\\frac{P{data}(x)}{P_G(x;\\theta)}dx \\ &=\\underset{\\theta}{argmin}\\ KL\\left(P{data}(x)||P_G(x;\\theta)\\right) \\end{aligned}$ 上面推导过程中，加上 $-\\intxP{data}(x)logP{data}(x)dx$ 是为了后面的推导把最大似然函数的式子化简成 KL 散度的表达式，因为其与 $\\theta$ 无关，因此加上这一项并不影响等式，就相当于总体平移了一个常数量，即 $\\underset{\\theta}{argmax}\\left{\\int_xP{data}(x)logP_G(x;\\theta)dx-c\\right}$ $KL(P||Q)$ 衡量 $P,Q$ 两个概率分布的差异，定义为 KL(P||Q)=\\int_xp(x)\\left(logp(x)-logq(x)\\right) GAN 的基本思想 GAN 的本质就是在做一个最大似然估计 确定具体分布的形式 $P_G(x;\\theta)$ 通过最大似然估计求得 $\\theta$，使得 $PG(x;\\theta)$ 近似表达 $P{data}(x)$ 基于 $P_G(x;\\theta)$ 采样进行生成 但是，无论是怎样的概率模型 $P_G(x;\\theta)$ 的表达能力总是有限，于是我们就想到用神经网络来代替传统的概率模型 我们先选取一个简单的先验分布 $P_{prior}$，并从该先验分布中采样 $z$ 作为输入，输入到神经网络 $G$，得到 $G(z)=x$ 生成图像，我们通过这种方式构建了生成分布 $P_G(x;\\theta)$，此时该分部主要由神经网络 $G$ 决定，参数 $\\theta$ 由网络参数定义 我们的目标是 $P{data}(x)$，我们希望我们构建的 $P_G(x;\\theta)$ 与它尽可能接近，我们无法获得 $P{data}(x)$ 的具体表达形式，我们只能获得它的样本 类似最大似然估计，我们通过比较两个分布样本的差异设计损失函数来调节神经网络 $G$ 的参数，从而实现将 $PG$ 向 $P{data}$ 拉近，从而达到用 $PG$ 拟合 $P{data}$ 的效果 如何找到更接近的分布，这就是 GAN 优于最大似然的地方了 GAN 由生成器 $G$ 和判别器 $D$ 组成，判别器 $D$ 的引进就是为了解决如何更接近的问题 对于生成器 $G$，输入 $z\\sim P_{prior}$，输出 $x\\sim P_G$ 对于判别器 $D$，输入 $x\\sim P_G$，输出一个 scalar，$D$ 用于评估两个分布之间大差异 于是，GAN 的最终目标就可以描述为 G^*=arg\\ \\underset{G}{min}\\ \\underset{D}{max}\\ V(G, D) 也就是找到一个 $G$ 使得两个分布之间的最大差异最小 $V(G, D)$ 定义为 V(G, D)=E_{x\\sim P_{data}}[logD(x)]+E_{x\\sim P_G}[log(1-D(x))] 固定 $G$，最优化 $D^*$，有 $\\begin{aligned} V(G, D)&=E{x\\sim P{data}}[logD(x)]+E{x\\sim P_G}[log(1-D(x))] \\ &=\\int_xP{data}(x)logD(x)dx+\\intxP_G(x)log(1-D(x))dx \\ &=\\int_x\\left[P{data}(x)logD(x)+P_G(x)log(1-D(x))\\right]dx \\end{aligned}$ 假设 $D(x)$ 可以表示任何函数，此时再固定 $x$，则对于 $P_{data}(x)logD(x)+P_G(x)log(1-D(x))$ 我们可以视为关于 $D$ 的函数：$f(D)=alogD+blog(1-D)$ 求解 $D^=\\underset{D}{argmax}\\ f(D)$，得 D^*=\\frac{a}{a+b}=\\frac{P_{data}(x)}{P_{data}(x)+P_G(x)} 显然 $0(x) 将 $D^*$ 带回 $V(G,D)$ 得 $\\begin{aligned} \\underset{D}{max}\\ V(G, D)&=V(G, D^*)\\ &= E{x\\sim P{data}}\\left[log\\frac{P{data}(x)}{P{data}(x)+PG(x)}\\right]+E{x\\sim PG}\\left[log\\left(1-\\frac{P{data}(x)}{P{data}(x)+P_G(x)}\\right)\\right] \\ &=\\int_xP{data}(x)log\\frac{P{data}(x)}{P{data}(x)+PG(x)}dx+\\int_xP_G(x)log\\left(\\frac{P{G}(x)}{P{data}(x)+P_G(x)}\\right)dx \\ &=\\int_xP{data}(x)log\\frac{P{data}(x)}{[P{data}(x)+PG(x)]/2}dx+\\int_xP_G(x)log\\left(\\frac{P{G}(x)}{[P{data}(x)+P_G(x)]/2}\\right)dx-2log2 \\ &=-2log2+KL\\left(P{data}(x)||\\frac{P{data}(x)+P_G(x)}{2}\\right)+KL\\left(P{G}(x)||\\frac{P{data}(x)+P_G(x)}{2}\\right)\\ &=-2log2+2JSD(P{data}(x)||P_G(x)) \\end{aligned}$ JSD 表示 JS 散度，是 KL 散度的一种变形 JSD(P||Q)=\\frac{1}{2}KL(P||M)+\\frac{1}{2}KL(Q||M)\\\\ M= \\frac{P+Q}{2} 上面的推导过程证明了 $V(G,D)$ 确实衡量了两个分布之间的差异 那么接下来的任务就是最优 $G$ G^*=\\underset{G}{argmin}\\ V(G, D^*)=\\underset{G}{argmin}\\ JSD(P_{data}(x)||P_G(x)) $0{data}(x)||P_G(x)){data}(x)||P_G(x))=0$ 时，表示两个分布完全相同 接着，我们令 $L(G)=V(G, D^*)$ 为损失函数，然后就通过梯度下降找到最优的 $G$ \\theta_G\\leftarrow\\theta_G-\\eta\\frac{\\partial L(G)}{\\partial\\theta_G} 第一代： 给定 $G_0$ 确定 $D0^$ 使得 $V(G_0,D)$ 最大，此时 $V(G_0, D_0^)$ 表示 $P{data}(x)$ 和 $P_{G_0}(x)$ 的 JS 散度 梯度下降 $\\theta_G\\leftarrow\\theta_G-\\eta\\frac{\\partial V(G,D_0^*)}{\\partial\\theta_G}$，得到 $G_1$ 第二代： 给定 $G_1$ 确定 $D1^$ 使得 $V(G_1,D)$ 最大，此时 $V(G_1, D_1^)$ 表示 $P{data}(x)$ 和 $P_{G_1}(x)$ 的 JS 散度 梯度下降 $\\theta_G\\leftarrow\\theta_G-\\eta\\frac{\\partial V(G,D_1^*)}{\\partial\\theta_G}$，得到 $G_2$ 以此类推 但是，上面还有一个问题：如何确定 $D^*$ 使得 $V(G, D)$ 最大 从 $P_{data}(x)$ 采样 $\\left{x^1, x^2, \\cdots, x^m\\right}$，从 $P_G(x)$ 采样 $\\left{\\tilde x^1, \\tilde x^2, \\cdots,\\tilde x^m\\right}$，因此我们可以将 $\\underset{D}{max}\\ V(G, D)$ 从期望值计算改写为对样本计算（近似估计），\\tilde V=\\frac{1}{m}\\sum_{i=1}^mlogD(x^i)+\\frac{1}{m}\\sum_{i=1}^mlog(1-D(\\tilde x^i)) 这个公式让我们很容易就想到了交叉熵损失，因此，我们不妨将 $D$ 视为一个二分类器，参数为 $\\thetaD$，来自 $P{data}(x)$ 的采样作为正样本，来自 $P_G(x)$ 的采样作为负样本，此时我们的问题就很简单了 交叉熵损失大，$P_{data}(x)$ 和 $P_G(x)$ 的 JS 散度大 交叉熵损失小，$P_{data}(x)$ 和 $P_G(x)$ 的 JS 散度小 于是，$D$ 就可以设计为一个二分类的神经网络，那么为了确定 $D^*$ 可以使用梯度下降优化 训练 这里有一个问题，我们在 $D_0^$ 的位置取到了 $V(G_0, D_0^)$，然后更新 $G_0$ 为 $G_1$，这时 $V(G_1,D_0^))$，但是接下来并不保证会出现一个新的点 $D_1^$ 使得 $V(G1, D_1^) 避免上述情况的方法就是更新 $G$ 的时候，不要更新 $G$ 太多 这句话是个很玄学的话，看下下面的训练算法，理解一下上面的不要更新太多的意思 上图中打红线的意思是在 $\\tilde V$ 中第一项并不包含关于 $G$ 的内容，因此我们可以在计算 $G$ 的梯度下降的时候去掉 改进问题 G loss modification 但是上面 $G$ 的损失函数还是有一点小问题，下面是两个函数的图像 我们发现 $D(x)$ 趋近于 $0$ 的时候，这个函数十分平滑，梯度非常的小。这就会导致在训练的初期，$G$ 想要骗过 $D$ 变化得十分缓慢。但是上面的函数趋势和下面的函数是一样的，都是递减，它的优势在于 $D(x)$ 接近 $0$ 的时候，梯度很大，有利于训练。在 $D(x)$ 越来越大之后，梯度减小，这也很符合实际 因此，我们把 $G$ 的损失函数修改为 V=-\\frac{1}{m}\\sum_{i=1}^mlog(D(x^i)) Weaken your discriminator 接着，还有一个问题，在其他论文中发现，经过许多次训练，损失函数一直都是平的，也就是 $\\underset{D}{max}V(G,D)=0$，JS 散度一直都是 $log2$，$PG$ 和 $P{data}$ 完全没有交集，造成这个的原因是我们无法真正计算期望和积分，只能使用采样的方法近似，如果训练过拟合了，$D$ 还是能够完全把两部分点分开 对于这个问题，我们是否应该让 $D$ 变得弱一点，减弱它的分类能力，但是从理论上讲，为了让它能够有效区分真假图片，我们又希望它的学习能力应该足够的强。解决的办法是添加噪声，让两个分布变得更宽，增大它们的交集，但是随着时间的变化，噪声需要逐渐变小 Mode collapse GNN 还有一个问题是会出现如下图所示的模型坍塌的问题——数据分布是一个双峰的分布，但是学习到的生成分布却只是单峰 造成这种情况的原因是，KL 散度里的两个分布写反了 如果是按照左边的 KL 散度的写法，为了防止出现无穷大，所以有 $P_{data}$ 出现的地方都必须有 $P_G$ 覆盖，这样就不会出现 mode collapse "},"【论文】GLIP.html":{"url":"【论文】GLIP.html","title":"GLIP","keywords":"","body":" 【论文】Li L H, Zhang P, Zhang H, et al. Grounded language-image pre-training.（pdf） "},"【论文】GoogleNet Inception V1.html":{"url":"【论文】GoogleNet Inception V1.html","title":"GoogleNet Inception V1","keywords":"","body":"﻿* 【论文】Szegedy C , Liu W , Jia Y , et al. Going Deeper with Convolutions[J]. 2014.（pdf） 【新颖点】 保证算力的情况下增加宽度和深度 宽度：利用 Inception 结构同时执行多个网络结构 深度：利用辅助分类器防止梯度消失 Inception V1 我们先说一下为什么要提出 Inception 在早期，大家都尽可能的想加深加宽网络，但是一味的增加还是有很多问题： 参数越多，计算复杂复杂度越大 网络越深，越容易出现梯度弥散的问题 于是，Google 研究人员提出了 Inception 的方法——将多个卷积或池化操作放在一起组装成一个网络模块，设计神经网络时以模块为单位去组装整个网络。Inception 的主要思路是怎样用密集成分来近似最优的局部稀疏结构，于是作者首先提出了如下图所示的基本结构 采用不同带下的卷积核意味着不同大小的感受野，最后通过拼接将不同程度的特征进行融合 卷积核采用 1、3 和 5，主要是为了方便对齐。设定卷积步长为 1 之后只要分别设定 padding 为 0、1、2 就好了，卷积之后便可以得到相同维度的特征 介于 pooling 的表项不错，Inception 中嵌入了对应的模块 网络越往后走，特征越家抽象，而且每个特征所设计的感受野也更大了，因此随着层数的增加，$3\\times3$ 和 $5\\times5$ 卷积核的比例也要增加 但是这种叠加不可避免的使得 Inception 模块的输出通道数增加，这就增加了 Inception 模块中每个卷积的计算量，尤其达到网络的后面。为此，作者采用了 $1\\times1$ 卷积核来进行降维，改进后的 Inception 模块如下 值得注意的是 $1\\times1$ 卷积的位置 $1\\times1$ 卷积是在最大池化层之后，而不是之前，因为池化层是为了提取图像的原始特征而存在，一旦它接在 $1\\times1$ 卷积之后就失去了最初的本意 对于 $3\\times3$ 和 $5\\times5$ 卷积，$1\\times1$ 卷积在他们之后，原因是如果反过来则起不到降维的作用 网络结构 Inception V1 是一个 22 层的深度网络，如果考虑池化层的话是 29 层。网络具有三组 Inception 模块，分别为 inception(3a) / inception(3b) inception(4a) / inception(4b) / inception(4c) / inception(4d) / inception(4e) inception(5a) / inception(5b) 三组 Inception 模块被池化层分隔 关于整个网络结构，我们给出下面几点说明， 网络最后采用了 average pooling 来替代全连接层，该想法来自 NIN，事实证明这样可以提高 0.6% 的准确率 虽然移除了尾部过多的全连接层，但是网络中依然使用了 Dropout（在 avg pooling 之后，FC 之前） 为了缓解梯度消失的问题，网络额外增加了 2 个辅助的 softmax 分类器用于向前传导梯度，我们将这两个称为辅助分类器。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类器中，这样就相当于做了模型融合，同时给网络增加了反向传播的梯度信号 我们以 inception(3a) 为例子解释一下网络的参数 辅助分类器 在 inception(4a) 和 inception(4d) 之后分别有两个辅助分类器，它们具有相同的结构： $5\\times5$、步长为 $3$ 的平均池化层 $1\\times1$、输出通道为 $128$ 的卷积层 $1024$ 个神经元的全连接层 $70\\%$ Dropout 的全连接层 softmax 层 在训练期间，两个辅助分类器的损失函数的权重是 0.3，它们的损失被叠加到网络的整体损失上。在测试时，这两个辅助分类器丢弃不用 "},"【论文】Group Normalization.html":{"url":"【论文】Group Normalization.html","title":"Group Normalization","keywords":"","body":"﻿* 【论文】Wu Y , He K . Group Normalization[J].（pdf） BatchNorm 有一个很大的问题——随着 batch_size 的减小，BN 的错误会急剧增加，这是由于小批量不正确统计估计导致的。于是，我们就又有了 Group Normalization 作为 BN 的替代，GN 将通道化成组，然后在每一个组中计算用于归一化的均值和方差 如今，许多的模型由于显存的限制都使用了较小的 batch_size，但是根据上图显示结果，这会导致非常严重的模型性能下降。例如，Fast R-CNN 或者 Mask R-CNN 因为使用了大分辨率的图片，因此每个小批量就只包含 1 - 2 个样本 我们前面还学习过 IN 和 LN 的内容，但是就视觉识别任务而言，它们都未能取得很好的结果 Group Normalization 我们知道，视觉特征在通道上并不是完全独立的。像 SIFT、HOG 和 GIST 这些分类特征都是分组表示的——通道上的每一组都是由某些种类的直方图组成。这些特征在处理上经常也会对每一直方图或者梯度使用 group-wise normalization 另外，一些更高层的特征，如 VLAD 或者 FV 也都是分组特征，每一组可以被看做是一个簇的子向量 同样的，我们也不应该将深度网络特征视为没有结构的向量。例如，对于第一层卷积 conv1，我们希望一个滤波器以及它的水平翻转可以在自然图像上显示出类似的滤波器响应分布，这是很合理的。如果 conv1 刚好能够近似地学习到这一对滤波器，或者通过论文 Exploiting cyclic symmetry in convolutional neural networks 和 Group equivariant convolutional networks 中的设计方式将水平翻转结构化，那么这些过滤器相应的通道就可以组合在一起进行归一化 对于更高层的特征往往会更抽象，其行为也不会很直观。然而，除了 SIFT 和 HOG 这些特征，我们依然还是可以按照频率、形状、照明和纹理等条件进行分组。它们的系数依然是相关的 于是，基于上面这些理由，我们提出了通用的分组归一化方式 Formulation 在 BN、LN、IN 和 GN 中，我们都有如下的公式 \\hat x_i=\\frac{1}{\\sigma}(x_i-\\mu_i) 其中，$x$ 表示特征，$i$ 是序号。对于一张图片，$i=(i_N, i_C, i_H, i_W)$ 是一个四维的向量表示 $(N,C,H,W)$ 下的一个特征。$\\mu_i$ 和 $\\sigma_i$ 的计算公式如下\\mu_i=\\frac{1}{m}\\sum_{k\\in S_i}x_k\\\\ \\sigma_i=\\sqrt{\\frac{1}{m}\\sum_{k\\in S_i}(x_k-\\mu_i)^2+\\epsilon} $S_i$表示进行归一化的像素集合，不同的归一化方法的区别就在于 $S_i$ 的选取方式不同 BN 的 $S_i$ 定义为 $S_i=\\left{k|k_C=i_C\\right}$，意思是通道不变，沿着 $(N, H, W)$ 取一个像素集合 LN 定义为 $S_i=\\left{k|k_N=i_N\\right}$，意思是沿着 $(C, H, W)$ 取一个像素集合 IN 定义为 $S_i=\\left{k|k_N=i_N， k_C=i_C\\right}$，意思是沿着 $(H, W)$ 取一个像素集合 最后，BN，LN 和 IN 都在每个通道上学习一个线性变化以补偿归一化导致的可能的非线性表达性的损失 y_i=\\gamma\\hat x_i+\\beta Group Norm 在 GN 中，$S_i$ 定义为 S_i=\\left\\{k|k_N=i_N, \\left \\lfloor \\frac{k_C}{C/G}\\right \\rfloor=\\left \\lfloor \\frac{i_C}{C/G} \\right \\rfloor\\right\\} 其中，$G$ 表示分组数，是一个预先定义好的超参数，默认 $G=32$。$C/G$ 表示每一组里面的通道数。$\\left \\lfloor \\frac{k_C}{C/G}\\right \\rfloor=\\left \\lfloor \\frac{i_C}{C/G} \\right \\rfloor$ 意味着索引 $i$ 和 $k$ 在同一组通道内。于是，GN 沿着 $(H, W)$ 和 $\\frac{C}{G}$ 一组的通道方向计算 $\\mu$ 和 $\\sigma$ 关于计算方式，GN 还是遵从上面提到的几个公式。GN 同样要学习 $\\gamma$ 和 $\\beta$ 两个超参数 与 LN 的联系 当 $G=1$ 时，GN就退化为 LN，LN 认为一层上的所有通道都保持一样的分布，这点在全连接网络上是没有问题的，但是在卷积上这点很难成立。GN 比起 LN 限制更小，因为我们只假设一组里面的所有通道保持一样的期望和方差，而不是假设所有通道都是。同时，GN 也更加灵活，可以在不同组上学习到不同的分布。这样就使得 GN 比起 LN 拥有更强大的表达能力（如下图所示） 与 IN 的联系 当 $C=G$ 时，GN 就退化为 IN ，但是 IN 只能在特定的一个维度上计算期望和方差，这样的话就会丢失通道间的相关性 Implementation 最后，我们看一下 GN 在 batch_size 方面的鲁棒性 "},"【论文】interBERT.html":{"url":"【论文】interBERT.html","title":"interBERT","keywords":"","body":" 【论文】Lin J, Yang A, Zhang Y, et al. Interbert: Vision-and-language interaction for multi-modal pretraining.（pdf） interBERT 做的事情说起来还是很简单的，把 one-stream 和 two-stream 结合起来，在 corss-modal interaction module 之后再接上两个独立的 two-stream module，整体的框架如下图所示 Extraction Module for Mode Representations 作者认为比较理想的预训练模型输出应该包括 3 部分：visual representations、linguistic representations 和 visual-linguistic representations，这样 VLP model 也可以迁移到下游的单模态任务上。于是，作者提出了一个 two-stream extraction module 用于产生 high-level object representations 和 text representations 具体的做法是用 self-attention 和 FFN 构成 extractor，对图像（或文本）的 cross-modal representations 再做一次模态内的自注意力学习 Pretraining tasks 两种预训练任务： （1）Masked Group Modeling（MGM），分两个子任务 masked segment modeling（MSM）和 masked region modeling（MRM） MSM 类似于 MLM，不过 MSM 以 10% 的概率连续遮掉当前单词以及其后面的 0 到 2 个单词；MRM 类似于 MOC，以 10% 的概率遮掉某一对象以及和该对象 IoU > 0.4 的区域 （2）image-text matching with hard negatives（ITM-hn），和 ITM 不同的是，作者没有随机替换掉 caption，而是选择与原始 caption TF-IDF similarities 小于 0.5 的前 30 个作为替换，生成 hard negatives，如下图所示。这样操作的意义在于让负样本的 caption 与原始的 caption 尽可能在字面上重合，但是两个样本本身的语义却是不一样的，达到一种混淆视听的感觉。实际操作中，20% 的负样本为 hard-negative caption，80% 的负样本是随机替换 后面的内容就不再介绍了，interBERT 在单模态下游任务的表现和 BERT ~base~ 相差不多，在多模态下游任务上涨分不明显，参考意义不是很大 "},"【论文】LayerNorm.html":{"url":"【论文】LayerNorm.html","title":"LayerNorm","keywords":"","body":"﻿* 【论文】Ba J L, Kiros J R, Hinton G E. Layer normalization[J].（pdf） 我们在 BatchNorm 的分析中也指出 BatchNorm 存在两个问题，一是小批量采样太小会导致结果不稳定，二是对于 RNN 网络 BatchNorm 并不适合 于是，本文提出了 『层归一化』，一种独立于 batch_size 的算法，所以无论样本数多少都不会影响参与 LayerNorm 计算的数据 Layer normalization 设 $H$ 是某一层中的隐藏结点的数量，$l$ 表示层数，我们可以计算 LayerNorm 的归一化统计量 $\\mu^l$ 和 $\\sigma^l$，如下 \\mu^l=\\frac{1}{H}\\sum_{i=1}^Ha_i^l \\\\ \\sigma^l=\\sqrt{\\frac{1}{H}\\sum_{i=1}^H(a^l-\\mu^l)^2} 其中 $a^l$ 表示一个中间输出结果的总和，这个总和可以通过权重矩阵 $W^l$ 和隐藏层全部输入 $h^l$ 组成的一个线性变化计算得到，具体可以参考下面a_i^l={w_i^l}^Th^l\\\\ h^{l+1}_i=f(a_i^l+b_i^l) $f$ 被视为一个非线性激活，$i$ 表示该层中的第 $i$ 个神经元。其实说简单点，$a^l$ 就表示层输入经过权重矩阵变换后的值 注意上面的统计量和样本数没有关系，而是和隐藏层的结点数有关，我们甚至可以使 batch_size = 1 于是，我们可以根据商量的统计量进行归一化处理，\\hat a^l=\\frac{a^l-\\mu^l}{\\sqrt{(\\sigma^l)^2+\\varepsilon}} 这是一个非常有意思的事情，我们对归一化的角度变了，BatchNorm 是按照小批量采样的结果进行归一化，而在 LayerNorm 中我们则按照一层神经元变换的结果进行归一化 同样，在 LayerNorm 中我们也需要一组参数来保证归一化操作不会破坏之前的信息。在 LayerNorm 中这组参数叫做增益（gain）和偏置（bias），同 BatchNorm 中的 $\\gamma$ 和 $\\beta$ 还是假设激活函数为 $f$，最终 LayerNorm 的输出为 $h^l=f(g^l\\odot \\hat a^l+b^l )$，整理一下公式于是有 h^l=f\\left (\\frac{g}{\\sqrt{(\\sigma^l)^2+\\varepsilon}}\\odot(a^l-\\mu^l)+b\\right ) Layer normalized recurrent neural networks 在 RNN 中，我们可以非常简单的在每个时间片中使用 LayerNorm，而且在任何时间片我们都能保证归一化统计量是关于 $H$ 个结点信息的统计。对于 RNN 时刻 $t$ 时的结点，其输入是 $t-1$ 时刻的隐层状态 $h^{t-1}$ 和 $t$ 时刻的输入数据 $x_t$，于是有 a^t=W_{hh}h^{t-1}+W_{xh}x^t 接着我们便可以在 $a^t$ 上采取和前面完全相同的归一化过程 h^t=f\\left [\\frac{g}{\\sigma^t}\\odot(a^t-\\mu^t)+b\\right ] \\\\ \\mu^t=\\frac{1}{H}\\sum_{i=1}^Ha_i^t \\\\ \\sigma^t=\\sqrt{\\frac{1}{H}\\sum_{i=1}^H(a_i^t-\\mu^t)^2} "},"【论文】LocVTP.html":{"url":"【论文】LocVTP.html","title":"LocVTP","keywords":"","body":" 【论文】LocVTP: Video-Text Pre-training for Temporal Localization（pdf） 作者做的是一个 Localization-oriented VideoText Pre-training 的工作，在以往只通过 retrieval 任务预训练的基础上做了扩展，以更好对接到下游的 temporal localization 任务 在预训练任务中引入 localization 有两个主要的问题： 预训练数据集关于时间维度的监督从哪里来？ 预训练模型采用怎样的 loss 可以获得关于时间维度的推理？ 作者在 retrieval 预训练的方式上引入了两个新的内容， fine-grained alignment 关于细粒度对齐作者设计了一种 clip-word level alignment，因为并没有 clip-word 这样的大规模数据，作者采用 coarse-grained contrastive learning 得到的共同空间来计算 clip-word pair 的相似度，选择相似度最高的作为正样本。如 Fig. 2 所示，$\\left{\\pmb v^{t_1},\\pmb q^{s_1}\\right}$ 和 $\\left{\\pmb v^{t_2},\\pmb q^{s_2}\\right}$ 都是正样本，clip-word alignment 即是让每对中的两个 semantic embedding 相互靠近，$\\pmb v^{t_1}\\leftrightarrow\\pmb q^{s_1}, \\pmb v^{t_2}\\leftrightarrow\\pmb q^{s_2}$ temporal relation reasoning 针对于时间关系推理，作者设计了一个叫做 context warping 的任务，基于另一个 clip $\\pmb v^{t2}$ 和 相对时间距离 $t_2-t_1$ 来重构 $t_1$ 对应的 feature $\\pmb z{t1}$，$\\pmb z{t_1}=\\text{warp}(\\pmb v^{t_2},t_2-t_1)$，近似于 $\\pmb v^{t_1}$，同时保证与按阿是建立的跨模态关系不变，即 $\\pmb z^{t_1}\\leftrightarrow\\pmb q^{s_1}$ 这样，我们初步可以最上面的两个问题了： 预训练数据集采用一种类似于自监督的方式获得关于时间维度的细粒度特征 采用 clip-word level alignment 和基于时间上下文信息重构 clip 两个方式来预训练模型 LocVTP 的结构如下所示，输入采用稀疏采样，每个视频采样 $T$ clips Coarse-grained Contrastive Learning video feature $\\pmb v=\\left{\\pmb v^t\\right}^T{t=1}$ 和 text feature $\\pmb q=\\left{\\pmb q^s\\right}^{S_q}{s=1}$ 过 average pooling 得到 $\\bar{\\pmb v},\\bar{\\pmb q}$，两者按照对比学习的方式做 video-sentence alignment Fine-grained Contrastive Learning 基于 coarse-grained contrastive learning 训练后，将这时经过 feature encoder $f_q,f_v$ 得到的 $\\pmb v_t$ 和 $\\pmb q^s$ 做点乘计算余弦相似度，取top-$K$ 个 words 作为 video clip $\\pmb v^t$ 对应的 positive words，最终得到的对应 video clip $t$ 的正样本为 于是，fine-grained contrastive loss 记为 Temporal aware Contrastive Learning 作者采用 $\\delta$ 时间距离的 video clip $\\pmb v^{t+\\delta}$ 来重建 $\\pmb v^t$，记 $g(\\cdot)$ 表示 context warping head，其输入为 $\\pmb v^{t+\\delta}$ 和 $\\delta$，重建的 feature $\\pmb z^t$ 可以表示为 其中 $\\delta$ 在 $[-\\delta{max},\\delta{max}]$ 范围内随机采样出来，$\\text{sgn}$ 是符号函数，$\\text{sgn}(\\delta)$ 和 $|\\delta|$ 表示方向和相对时间距离 我们希望重构的 feature 和原始特征十分相似，所以 temporal aware contrastive learning 就可以表示为 最终，LocVTP 的预训练 loss 就可以记为 \r \\mathcal L=\\lambda_c\\mathcal L_c+\\lambda_f\\mathcal L_f+\\lambda_t\\mathcal L_t\r Experiments video feature encoder 采用带 space-time attention 的 ViT-B/16，text feature encoder 采用 DistilBERT 有一点很关键：retrieval 基于稀疏采样，而 grounding 任务一般采样都比较密集，怎么平衡两者？ 视频采样 8 个 clips，每组 16 frames，等距采样，迁移到下游任务时，按照每 16 帧的方式密集采样 "},"【论文】LVTR.html":{"url":"【论文】LVTR.html","title":"LVTR","keywords":"","body":" 【论文】Woo S, Park J, Koo I, et al. Explore and Match: End-to-End Video Grounding with Transformer.（pdf） 【代码】Explore-and-Match 本文提出一种新的范式解决 TVG 任务—— Explore and Match，这种范式也是尝试将 proposal-free 和 proposal-based 的方法结合起来，proposal-free 对应 explore 搜寻 segments，proposal-based 对应 match 匹配 predefined proposals 和 gt Fig. 1 展示了本文方法和 proposal-free、proposal-based 方法的比较，总的来说就是认为 Explore and Match 的方法在原先的两种方法上取长补短，克服单一方法造成的限制 我们先不管 Explore and Match 的具体细节，先来看一下本文提出的 encoder-decoder 模型 LVTR (an end-to-end trainable Language Video Transformer) 模型很简单，一个 feature extractor 模块用于提取 video 和 text 的特征，cross-model encoder 进行跨模态的交互，grounding decoder 部分输入一些 learnable proposals，如此 decoder 在从 encoder 输入交互特征的情况下可以同时一遍预测出对应的 time segment，最后在 Explore and Match 模块与 gt 进行对齐 模型有两个损失 temporal localization loss 和 set guidance loss， Temporal localization loss 用于监督生成准确的 time segments， Set guidance loss 用于监督预测结果和 targets 的匹配，decoder 的 learnable proposals 首先按照 query 的数量分成对应数量的 subsets，然后 set guidance loss 使得每个 subset 匹配自己对应 query 在训练初期，temporal localization loss 占主导地位，目的是以某种方式得到时间近似的 segment，而不是预测对应的 target；训练后期，变成 set guidance loss 占主导地位，将所有 learnable proposals 和自己的 target segments 对齐 关于这部分的理解可以看一下 Fig.2 和 Fig.4，简单来说应该是这样一个逻辑，我们熟悉 Detr 的结构，Detr 就是在 decoder 输入一些 learnable object queries，而这些 learnable object queries 最终就学习到了 OD 的 bbox，这里也是一样的思路，在 decoder 先学习到一些 temporal segments，然后再在这些学到的 temporal segments 中去匹配和 query 对应的 prediction 清楚了什么是 Explore and Match，怎么做 Explore and Match，剩下的都是一些细节 TVG as a set prediction 对于传统的做法，我们都是给定一个视频 $\\mathcal V$ 以及一组 queries $\\mathcal Q=\\left{qi\\right}{i=1}^K$，对应的 gt segments 为 $\\left{yi\\right}{i=1}^K=\\left{(ti,q_i)\\right}{i=1}^K$, 这里取 $K=1$ 但是根据前面说的，LVTR 可以在同时预测一个视频的所有 queries，也即 LVTR 的预测输出为 $\\left{\\hat yi\\right}{i=1}^N=\\left{(\\hat ti, \\hat q_i)\\right}{i=1}^N$，$N>>K$，那么在 matching 的时候就就是在 prediction set $\\left{\\hat yi\\right}{i=1}^N$ 和 gt set $\\left{yi\\right}{i=1}^K$ 之间做 one-to-one matching 为了满足 one-to-one matching，用 $\\varnothing$ 填充 gt set 到 $N$，两个数据集的所有配对情况记为 $\\mathfrak S_N$，目标就是在寻找最佳的匹配模式 $\\hat \\sigma$ 使得匹配损失最小 $\\mathcal C_{match}$ 表示 pair-wise matching cost Set guidance loss 给定 $K$ 个 queries，有 $N$ 个 proposals，划分为 $K$ 个 subsets，$j$-th subset 中的 proposal 根据 set guidance loss 训练后对齐 $j$-th query。定义对应 target query $qi$ 序号为 $\\sigma(i)$ 的 prediction 的概率为 $\\hat p{\\sigma(i)}(q_i)$，set guidance loss 可以定义为如下形式 Temporal localization loss Temporal localization loss 定义为 $\\ell_1$ loss 和 generalized IoU (gIoU) 的组合 Final set prediction loss matching cost 定义为如下形式，将 $K$ 个 $qi\\neq\\varnothing$ 视为正样本，剩下的 $N-k$ 个 $q_i=\\varnothing$ 视为负样本。在 set guidance loss 中，作者并不考虑 negative log-likelihood，而是采用 $1-\\hat p{\\sigma(i)}(q_i)$，常数 $1$ 作者发现并不影响匹配的结果所以就忽略掉 final set prediction loss 定义如下 我们回头再看一下本文提出的 Match and Explore 方法，它保留了 proposal-based 的核心，proposal generation，同时避免了传统 hand-crafted proposal-based 方法的局限——定位最终被视为一种分类问题，而模型的性能取决于手动生成 proposal 的质量。同时，也保留了 proposal-free 方法的优势，动态调整 learnable proposal，使得生成的 proposal 在时间上更加对齐 gt boundary 下面是一些实验的结果，暂时不做细致讨论 "},"【论文】LXMERT.html":{"url":"【论文】LXMERT.html","title":"LXMERT","keywords":"","body":" 【论文】Tan, Hao, and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. （pdf） Why is LXMERT 和 ViLBERT 一样，LXMERT 主要也是为了解决两种模态的之间没有交互的问题。在 LXMERT 中作者通过 3 种编码器的叠加构建了一个大型的 transformer 模型，这三个编码器分别为：对象关系编码器，语言编码器和跨模态编码器。同时，作者设计了 5 种训练任务来使模型具有跨模态的能力：（1）masked crossmodality language modeling；（2）masked object prediction via RoI-feature regression；（3）masked object prediction via detected-label classification;（4）cross-modality matching；（5）image qa Input Embeddings & Encoders 我们解释一下 object-level image embedding 的三个公式：首先，位置信息对于 masked object prediction 任务来说是十分重要的，但是 image embedding layer 和 attention layer 对位置是不敏感的，所以要融合 RoIl feature 和 position feature。同时，考虑到两种特征的差异，先让它们各自经过一个层归一化投影到一个统一的向量上再做加和 实验中设置 $N_L=9,N_R=5,N_X=5$，language encoder 层数多于 object-relationship encoder 的原因是图像经过 Faster RCNN 已经得到一个较高语义的表征，为了平衡两个模态的表征，所以增加对语言特征的处理 Output Representation 如下图所示展示了 LXMERT 的预训练，LXMERT 具有分别针对语言，视觉和跨模态的 3 个输出。对于跨模态输出，我们在 words 之前附加一个特殊标记 [CLS]（下图中间的输出分支的顶部黄色块），该特殊标记对应的特征向量用作跨模态输出 ## Pre-Training Strategies **Masked Cross-Modality LM** 任务设置几乎与 BERT 相同，不过 LXMERT 具有跨模态的模型体系结构，能够通过视觉信息来辅助预测遮罩掉的词。如上图所示，很难从语言信息去推断遮罩掉的词是 carrot，但是通过视觉信息的相互作用就可以很明确知道被遮罩掉的这个词 ##### **Masked Object Prediction** 通过 0.15 的概率随机遮罩图中检测到的物体（将 RoI feature 全部置为 0），模型通过视觉模态和语言模态的结合来推断出被遮蔽物体的特征和标签 这个任务分为两个子任务： - RoI Feature Regression，使用 L2 损失来预测被遮罩对象的 RoI feature $f_j$，这一步不需要语言信息的帮助，可以使模型学习到对象之间的关系 - Detected-Label Classification，结合了视觉信息和语义信息使用交叉熵损失预测被遮罩对象的标签，由于不同数据集对同一对象标注的 ground truth label 存在差异，所以所有的预测标签都采用 Faster RCNN 输出的标签 ##### **Cross-Modality Matching** 为了判断模型文本信息与视觉信息的对齐效果，对每个句子，以 0.5 的概率用一个不匹配的句子替换它。然后，训练一个分类器来预测图像和句子是否相互匹配 **Image Question Answering (QA)** 在预训练的数据集中有将近 1/3 的句子是对图片的提问，当图像和问题匹配时，进一步要求模型预测问题的答案 imag qa 这个代理任务是有点不好理解，需要结合下面实验数据的设置。另外，实验中 LXMERT 训练了 20 个 epochs，考虑到 image qa task 收敛快、训练时学习率小，在最后 10 个 epochs 才引入 ## Experiment 预训练数据融合了 5 个 V&L 数据集，这些数据集来自 MSCOCO 或 VG，除了 COCO 和 VG 中的 captioning datasets，还包括 VQA v2.0，GQA balanced version 和 VG-QA 为了避免泄露测试集，预训练数据集仅包括每个数据集中的 train and dev splits，对于图像问答的数据集，取问题作为图像的描述，回答作为标签，这样就解决了代理任务 image qa 下面是 LXMERT 在 VQA、GQA、NLVR 三个数据集上的测试结果，以及和当时 SOTA 的比较 Our result on VQA v2.0 test-dev is 72.4%；Our result on GQA test-dev is 60.0%；As for NLVR, on the public test set (Test-P)，LXMERT achieves 74.5% Accu and 39.7% Cons Analysis 下图是论文中，LXMERT 和 BERT 的比较，我们先提一点比较重要且直观的：最后两行 pre-train + scratch 表示随初始、pre-train + BERT 表示用 BERT 的参数初始化，通过这两行的比较可以发现不用 BERT 参数初始化的 LXMERT 表现更好。作者给出的解释是：BERT 已经通过代理任务预训练好，但是其只是在语言模态上表现不错，而没有考虑到 LXMERT 中的视觉模态 看到这里你应该有一个疑问，怎么用 BERT 初始化 LXMERT 呢？作者在脚注上给出了解释：Since our language encoder is same as BERTBASE, except the number of layers (i.e., LXMERT has 9 layers and BERT has 12 layers), we load the top 9 BERT-layer parameters into the LXMERT language encoder 我们还要再继续研究一下 Table 3，作者找到一种方式来替代 LXMERT，即 BERT + CrossAtt，具体来说就是：先用 BERT 替代 Bottom-Up and Top-Down (BUTD) attention 中的 GRU，然后再在 BERT + BUTD 的模型后面添加 Cross-Modality Encoder，整个的结构如下所示 有意思的是经过我们一画，发现 BERT + CrossAtt 和 ViLBERT 的结构甚是有些相似，但是 BERT + CrossAtt 在堆叠几层的 Cross-Modality Encoder 后就不再涨分了 这还是蛮好玩的，加入 BERT 我们对问题在语言层面上的理解更深入了，Faster RCNN 抽取出来的 RoI feature 也在一个较高的语义层面上理解视觉上的物体，然后通过 Cross-Modality Encoder 也有了语言模态和视觉模态的交互，刨去这里的实验远没有 ViLBERT 设计的仔细，但是为啥就不涨分了，为啥 LXMERT 在整体表现上还是要比 ViLBERT 好那么一丢丢？ 我认为这和作者在 LXMERT 中设计的 visual-stream 上的自注意力机制是离不开关系的，相当于除了文本-文本自注意、文本-视觉互注意，LXMERT 还引入了视觉-视觉自注意。这个视觉-视觉自注意不同于 Faster RCNN 的处理，它是在多模态交互条件下的自注意，也就是 LXMERT 在回传的时候，Cross-Modality Encoder 会带着文本上学到的信息更新 ObjectRel Encoder 里面对不同物体关系的理解，这样就使得对图像的理解更充分。同时，image qa 代理任务和两个视觉代理任务的引入，在一定程度上也刺激了模型去学习视觉对象和视觉对象之间的关系 下面 LXMERT 作者也在自己的实验中说明了 image qa 代理任务和两个视觉代理任务的有效性。Table 4 和 Table 5 分别表明 image qa 代理任务和视觉预代理任务（RoI-feature regression 和 detected-label classiﬁcation）在预训练中起到的作用 Table 4 中 P20 表示不带 image qa 训练 20 个 epochs 的结果，P10+QA10+FT 表示前面提到的 LXMERT 的完整训练结果，两者比较可以看出 image qa 还是有一定的涨分作用 Table 5 中的第 1 行效果（ No Vision Tasks LXMERT）和 Table 3 中 BERT + 3 Crosstt 的效果近似，换句话说我是这样猜想的，因为没有视觉代理任务，所以对 ObjectRel Encoder 的刺激不够，那么夸张一点，就相当于没有视觉-视觉自注意，那么自然 LXMERT 的表现就退化到了类似于 ViLBERT 或者 BERT + 3 Crosstt。另外，第 2、3 行分别表示单独使用 RoI-feature regression 和 detected-label classiﬁcation，均表明视觉代理任务对模型是有效的，同时使用两个视觉代理任务的效果会更好一些 Visualizing LXMERT Behavior Language Encoder 下图说明了 LXMERT 的 language encoder 和 BERT encoder 有着相同的行为，对于同一句输入 “is it warm enough for him to be wearing shorts？”，第一行两个模型都将注意力放到了下一个词上，第二行两个模型都将注意力放到了上一个词上 Object-Relationship Encoder 作者根据注意力图中 attention scores 比较高的点绘出了如（b）所示的对象关系， 忽略了一些 attention scores 不高的对象。（b）图所示的关系连接精确地描绘出了一个场景图，可以认为 object-relationship encoder 还是学习到了一个不错的对象关系网络 Cross-Modality Encoder 作者做了如下的可视化结果，有 2 点发现： 模型注意力通常集中在句子中的名词和代词上，这些词往往都会提供大量的信息 当句子中出现不可数名词的时候，注意力往往会放到冠词上。这个行为就比较古怪，作者的解释是冠词可能像是一些 special tokens，例如 BERT 里面的 [CLS]、[SEP]，提供了一个总的 attention layers 的信息 "},"【论文】MAE.html":{"url":"【论文】MAE.html","title":"MAE","keywords":"","body":" 【论文】He K, Chen X, Xie S, et al. Masked Autoencoders Are Scalable Vision Learners.（pdf） Introduction 近些年来，深度学习领域出现了一大批能力、容量均不断增长的架构。在不断升级的硬件的支持下，今天的模型已经能够轻松地消化数百万张图像，而且开始向数以亿计的标记图像进发 在自然语言处理中，这种数据需求已经成功地通过自监督预训练来解决。基于 GPT 自回归语言建模和 BERT 掩蔽自编码的解决方案在概念上非常简单：它们遮掉一部分数据，并学习预测遮掉的内容。这些方法可以用来训练包含数千亿参数的可泛化 NLP 模型 掩蔽自编码器（masked autoencoders，MAE）是一种更通用的去噪自编码器，也适用于计算机视觉。其实，自编码与视觉密切相关的研究早于 BERT，在 BERT 成功之后，人们对这一想法更是产生了极大的兴趣。但尽管如此，视觉自编码方法的发展还是落后于 NLP。这篇论文的研究者想知道：what makes masked autoencoding different between vision and language ? 架构差异 在计算机视觉领域，卷积网络是过去十年的主流架构，卷积通常直接在 regular grids 上进行操作，而不需要将 mask tokens、positional embeddings 这些 indicators 整合到卷积网络中。不过，随着 ViT 的推出，这种架构上的差异已经逐渐缩小，应该不会再成为障碍 信息密度差异 语言是人类产生的高度语义化信号，信息非常密集。当训练一个模型来预测每个句子中缺失的寥寥数词时，这项任务似乎能诱发复杂的语言理解。但视觉任务就不同了：图像是自然信号，拥有大量的空间冗余。例如，一个缺失的 patch 可以根据相邻的 patch 恢复，而不需要对其他部分、对象和场景有很多的高级理解 解码器差异 自编码器的解码器的作用是将潜在表征映射回输入，在文本和图像重建任务中其起着不同的作用。在计算机视觉任务中，解码器重建的是像素，因此其输出的语义水平低于一般的识别任务。这与语言相反，语言任务中的解码器预测的是包含丰富语义信息的缺失单词。虽然在 BERT 中，解码器可能是微不足道的（一个 MLP），但作者发现，对于图像，解码器的设计对于学到的潜在表示的语义水平起着关键作用 基于上面的分析，作者提出了一种简单有效、可扩展的 MAE 方法用于视觉表征学习。该 MAE 从输入图像中随机掩蔽掉一些 patch 并重建像素空间中缺失的 patch，它具有非对称的 encoder-decoder 设计—— encoder 仅对 patch 的可见子集（without mask tokens）进行操作，轻量级的解码器则可以从潜在表征和 mask token 中重建原始图像 进一步，masking ratio 取值比较大的时候（e.g. 75%）会产生重要且有意义的自监督任务——一方面模型的准确率达到最优，另一方面 encoder 只需要处理很小一部分的 patch（e.g. 25%），这使得整个预训练时间可以快上 3 倍不止，减少了很多内存消耗，由此使得该 MAE 可以轻松胜任很多大型模型（high-capacity models）的训练。具体来说，ViT-Large/-Huge 在预训练时使用 MAE 能够在 ImageNet-1K 上得到更好的泛化能力，普通的 ViT-Huge model 更是在 ImageNet-1K 上 fine-tunning 时达到了 87.8% 的 top-1 准确率，这胜过之前只用 ImageNet-1K 进行预训练的结果。同时，作者还在 object detection、instance segmentation 和 semantic segmentation 等任务上做了迁移学习，在这些任务上基于 MAE 自监督的训练结果都好于预训练监督学习的结果 Approach Masking 与 ViT 类似，该方法将图像划分为规则的非重叠 patches，然后采样一个 patches 的子集保留下来，其余的都遮掉。采样的规则很简单：遵循均匀分布不带替换的随机采样 pathes，这个规则作者称为 random sampling Random sampling with a high masking ratio 可以避免简单的从可见的相邻 pathces 直接外推（extrapolation）遮掉的像素。而遵循均匀分布采样可以避免过于集中的遮蔽，例如，遮蔽都集中于图像中心。最终，基于 random sampling 规则的遮蔽可以大幅减少 spatial redundancy，得到的高稀疏的输入有助于设计一种更高效的 encoder MAE encoder MAE encoder 和 ViT 的 encoder 基本上是一样的，但只操作于 visible, unmasked patches（Masked patches are removed; no mask tokens are used），这样 encoder 就只需要处理一个很小的 subset 即可，full set 则留给下面 lightweight decoder 进行处理 MAE decoder MAE decoder 的输入是 encoded visible patches + mask tokens，mask token 表示 shared, learned vector that indicates the presence of a missing patch to be predicted 考虑到 patch 在图像中的位置信息，对 full set 的所有 tokens 加入了 positional embedding 注意，MAE decoder 仅在预训练中使用（perform the image reconstruction task），在具体任务时移除 decoder 仅使用 encoder来提取图像表征。那么，decoder 就可以独立于 encoder 设计，论文中中采用了一些 narrower and shallower decoders，如默认的 decoder 对每个 token 就只用到 encoder 10% 的计算消耗，甚至更少。这样非对称的 encoder-decoder 结构进一步缩减了预训练的时间 Reconstruction target MAE 通过预测每一个 masked patch 的像素值来构建图像，decoder 的一个输出向量表示对应一个 patch 的像素值，decoder 最后一层是一个线性层，输出通道的数量等于一个 patch 中像素点的数量。损失函数选用像素空间上的 MSE（mean squared erro），和 BERT 一样只计算 masked pathces 的 loss 同时作者还研究了另外一种重建的方法——预测每个 masked patch 的所有像素的均值和标准差，使用 normalized pixels 作为重建目标能够提升图像表征的质量 Simple implementation MAE 操作起来十分简单，没有任何特别的 sparse operations，可以概括为以下几步： 通过线性层将 positional embedding 和 input patch 整合在一起生成对应的 token 将 token list 进行 randomly shuffle，按照 masking ratio 保留 shuffled list 的前面部分，以此产生 encoder 的输入 将上一步丢掉的（masked） tokens 拿回来，进行 unshuffle（inverting the random shuffle operation）操作还原到最原始的 token list 对齐每个 token 的 targets 接着就是通过 decoder 重建 masked pathces Experiments Reference 大道至简，何恺明新论文火了：Masked Autoencoders让计算机视觉通向大模型 "},"【论文】MCAN.html":{"url":"【论文】MCAN.html","title":"MCAN","keywords":"","body":" 【论文】zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. (pdf) What is MCAN MCAN 主要工作可以总结为下面两点： 模仿 transformer 设计了两个注意力单元作为 Modular Co-Attention（MAC） layer 的基本组成元件—— general attention units 和 self-attention unit， 通过 GA unit 实现问题对图像注意的引导，SA unit 则主要对模态内的交互进行建模 通过 MAC layer 堆叠形成 deep co-attention model 实现对视觉内容和文本内容的细粒度理解 从 MCAN 的工作内容也很容易看出作者的 motivation —— 现有 co-attetnion learning 不足。之前的 co-attention learning 只在浅层模型中实现，即使通过串联形成深层的 co-attention model 性能提升却并不显著。这样的注意力机制只能够学习到不同模态之间粗糙的交互，也不够进行图像和问题关键词的关系推断。于是，作者提出了 MCAN，a deep Modular Co-Attention Network Modular Co-Attention Layer Self-Attention and Guided-Attention Units 受 transformer 中注意力模块单元的启发，作者设计了如下两个类似的注意力单元作为 MCA layer 的基础组成元件 SA unit 作为自注意模块， 输入一组输入特征 $X=[x1;\\cdots;x_m]\\in\\mathbb R^{m\\times d_x}$，multi-head attention 学习 $X$ 中样本对 $$ 之间的关系，接着通过 feed forward layer（FC (4$d$) - ReLU - Dropout (0.1) - FC ($d$) ），最终输出 attended output features $Z\\in\\mathbb R^{m\\times d}$（在 scaled dot-product attention 中通常将键值的维度 $d{key}$ 和 $d_{value}$ 设置为同一个数 $d$ ） GA unit 作为 co-attention 模块，输入两组特征 $X$ 和 $Y=[y_1;\\cdots;y_m]\\in\\mathbb R^{n\\times d_y}$，$Y$ 用于引导 $X$ 的注意学习。$X,Y$ 的形式不固定可以表示任何不同的模态特征，这里multi-head attention 学习的就是两个模态的样本对 $$ 之间的关系 Modular Composition for VQA MCA layer 有三种变体将两个注意力单元结合起来: ID (Y) - GA (X, Y) 中输入的问题特征直接恒等映射到输出特征，在 GA (X, Y) unit 中将每个图像区域 $x_i$ 和每个单词 $y_i$ 结合产生 attended image feature SA (Y) - GA (X, Y) 在 ID (Y) - GA (X, Y)的基础上添加了对问题的自注意力学习，关注问题中单词之间的关系 $\\left{y_i,y_j\\right}\\in Y$ SA (Y) - SGA (X, Y) 在 SA (Y) - GA (X, Y) 的基础上添加了图像区域的自注意力学习，关注图像区域之间的关系 $\\left{x_i,x_j\\right}\\in X$ Modular Co-Attention Networks 整个网络如下图所示，分为了三个模块，下面将分别介绍每个模块的详细内容。由于 Deep Co-Attention Learning 有两种方式，对采用 stacking 结果的 $L$ 层 MAC 的模型称为 MCAN~sk~-L，采用 encoder-decoder 结构的模型称为 MCAN~ed~-L Question and Image Representations 与之前的工作一样使用 Faster RCNN 提取视觉特征，$x_i\\in\\mathbb R^{d_x}$ 表示第 $i$ 个物体。不同的是，作者对检测到物体的概率设定了一个置信度阈值，这样检测到的物体数量就不是固定的，$m\\in[10,100]$。最终图像的特征矩阵为 $X\\in\\mathbb R^{m\\times d_x}$ 将问题拆分为 $n$ 个单词后分别经过 Glove word embedding 和具有 $d_y$ 个隐藏单元的单层 LSTM ，保留所有单词的输出构成问题的特征矩阵 $Y\\in\\mathbb R^{n\\times d_y}$ 为了处理 $m,n$ 不固定的问题，使用 zero-padding 将 $X、Y$ 填充到最大长度 $m=100,n=14$ Deep Co-Attention Learning 首先，deep co-attention model 由 $L$ 层 MAC layer 堆叠而成，即满足 $[X^{(l)},Y^{(l)}]=MCA^{(l)}\\left([X^{(l-1)},Y^{(l-1)}]\\right)$ 这里有两种构成方式，如下图所示 Multimodal Fusion and Output Classiﬁer 记 deep co-attention model 的输出分别为 $X^{(L)}=\\left[x_1^{(L)},\\cdots,x_m^{(L)} \\right]$ 和 $Y^{(L)}=\\left[y_1^{(L)},\\cdots,y_n^{(L)} \\right]$，接着两个输出都会经过一个 attention reduction model 最终输出 $\\tilde x$ 和 $\\tilde y$，该模型由两层的 MLP 构成（ FC (4$d$) - ReLU - Dropout (0.1) - FC (1) ） 以 $X^{(L)}$ 为例， \r \\alpha=softmax(MLP(X^{(L)}))\\\\\r \\tilde x=\\sum_{i=1}^m\\alpha_ix^{(L)}_i\r $\\tilde y$ 同理，然后两者一起输入到一个线性的多模态融合函数中 \r z=LayerNorm(W^T_x\\tilde x+W^T_y\\tilde y)\r 其中，$W^T_x,W^T_y\\in\\mathbb R^{d\\times d_z}$ 表示两个可学习的权重矩阵 最后，使用 binary cross-entropy（BCE）损失在融合特征 $z$ 上训练一个 N-way 分类器 Experiments 下图是论文中一些消融实验的结果 下图展示了 MCA layer 三种变体在 VQA 子任务上的表现 下图可视化了 MCAN~sk~-6 和 MCAN~ed~-6 的注意力分布 下图展示了一些图像和文本 co-attention 的例子，其实这里有一个非常有意思的问题，要区分黑莓手机还是诺基亚，或者要区分击球手还是捕手，这类的 co-attention learning 或多或少依赖属于数据集对他们的区分，或者需要借鉴外部知识 下图是 MCAN 和其他一些比较新的 VQA 模型性能的比较 Referrence 用于视觉问答的深度模块化共同注意网络 《Deep Modular Co-Attention Networks for Visual Question Answering》 "},"【论文】MCB.html":{"url":"【论文】MCB.html","title":"MCB","keywords":"","body":"﻿* 【论文】Fukui, Akira, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Multimodal compact bilinear pooling for visual question answering and visual grounding. （pdf） 对于像视觉问答、视觉定位这样的多模态任务，需要融合不同类型模型的向量得到一个 joint representation 传统的向量融合方式是：点积、点加和全连接，或者加入注意力机制。本文就属于改变特征融合方式的一种，的作者认为这些方法不如向量的外积，但是向量的外积会引起维度的急剧增加，于是作者提出了 MCB（Multimodal Compact Bilinear pooling） 如下如所示，MCB 大致的工作流程为 随机地将图片和文本特征映射到更高维空间里 然后，在快速傅里叶变化（Fast Fourier Transform，FFT）空间中使用元素乘积有效地卷积两个向量 对于开放式问题回答，作者在 VQA 框架中使用了两次 MCB，一次预测空间注意，第二次预测回答 对于一个问答多个答案的情况，作者引入三次 MCB，将编码后的答案和图像空间联系在一起 Multimodal compact bilinear pooling for visual and textual embedding 对于 VQA 或者视觉定位问题，我们需要基于给定的图片 $\\mathbf x$ 和问题 $\\mathbf q$ 来预测最有可能的回答或者位置 $\\hat a$，于是问题可以描述为 \\hat a=\\underset{a\\in A}{argmax}\\ p(a|\\mathbf x,\\mathbf q;\\theta) 其中，$A$ 表示回答集合或者位置集合 对于图像特征 $x=\\Xi (\\mathbf x)$ 和问题语义特征 $q = \\Omega(\\mathbf q)$，我们希望通过 pooling 得到一个不错的融合特征。我们将多模态 pooling 记为 $\\Phi(x, q)$，它应该能很好地对 $x$ 和 $q$ 关系进行编码，同时它还应该很好训练，很容易学得一个分类器 Multimodal compact bilinear pooling Bilinear model 计算两个向量的外积，然后学习一个线性变换 $W$，即 z=W[x\\otimes q] $\\otimes$ 表示外积 $xq^T$，$[\\cdot]$ 表示将矩阵线性化为一个向量 双线性池化允许两个向量的所有元素以乘法方式相互作用。但是，如前面所述，外积存在维度爆炸的问题，假如参与运算的两个向量的维度为 $2048$，$z\\in\\mathbb R^{3000}$，那么我们将会有 $2048\\times2048\\times3000$ 个参数，将近 12.5 billion 因此，需要一种方式将外积映射到更低的维度，并且避免直接计算外积。于是，引入了 Count Sketch Projection Function $\\Psi:v\\in\\mathbb R^n\\rightarrow y\\in \\mathbb R^d$ 定义两个向量，（1）$s\\in\\left{-1, 1\\right}^n$，每个 index 下取 $1$ 或 $-1$； （2）$h\\in\\left{1,\\cdots, d\\right}^n$，将输入 $v$ 的 $i$ 映射到输出 $y$ 上的 $j$ $s,h$ 用标准正态分布进行初始化，并在后面的 count sketch invocations 中保持不变。$y$ 被初始化为零向量。对每一个元素 $v[i]$ 通过 $h$映射到 $j=h[i]$，$s[i]\\cdot v[i]$ 加到 $y[j]$ 上 这样，我们就可以将外积压缩到一个更低维的空间中，从而减小 $W$ 的参数大小 另外，为了避免直接计算外积，可以将两个向量外积的 count sketch 表示为对两个 count sketch 的卷积操作 \\Psi(x\\otimes q,h,s)=\\Psi(x,h,s)\\ast\\Psi(q, h,s) $\\ast$ 表示卷积操作，卷积理论指出，时间域上的卷积等同于其他处理时的元素积 $x'\\ast q'$ 可以写为 $FFT^{-1}(FFT(x')\\odot FFT(q'))$，$\\odot$ 表示元素积，为什么可以这么做参考 Tensor Sketch Algorithm 下面给出了 MCB 工作的示意图和算法流程，算法中我们将 $v_1,v_2$ 代为 $x, q$ Architectures for VQA 使用 ResNet-152 提取图像特征，输入图像大小为 $448\\times448$，从 pool5 提取特征，然后对这个向量（$2048$ 维）进行 L2 正则化 输入问题首先标注维单词，然后对单词进行 one-hot 编码，然后再经过一个 learned embedding layer，embedding 之后接 tanh 激活。再然后经过一个两层的 LSTM（每层有 1024 个单元），每层 LSTM 的输出连接在一起形成一个 2048 维的向量 图片特征向量和问题语义特征一起传入 MCB，MCB 后经过 element-wise signed square-root 和 L2 正则化。这之后，经过一个全连接网络层根据 16000 维的多模态特征预测出 3000 个回答 注意力机制 作者在 MCB pooling 中加入了 soft attention：对于图像特征中的每一个空间网格位置，使用 MCB pooling 将视觉特征的切片和语义特征进行融合。如 Figure 3 所示，在池化之后使用了两层卷积神经网络预测每个网格位置的注意力权重，接着再使用 softmax 进行归一化产生 soft attention map，然后根据权重对视觉特征和 attention map 进行加和产生一个注意力图像特征。作者还尝试生成多个 attention map，实现一个直观上的多次回头观察，这些 attention map 的结合在经过第二个 MCB（Figure 3 中下面的分支的 MCB） 与语义特征融合之前实现 使用 MCB 的预测注意力特征图使模型能够更有效地学习如何通过图像特征和语义特征关注到显著位置 Answer Encoding 对于不固定回答的 VQA（multiple choices），作者引入了一个额外的 answers embedding 如 Figure 4 所示，对于不定长的回答选项，每一个都使用 word embedding 和 LSTM 层（LSTM 层的权重在候选答案中共享）进行编码。然后，使用带注意力机制的 MCB 将编码后的回答选项和原来分支的多模态特征进行融合。最终的 embedding 结果变化为一个分类向量，向量的维度等于回答的个数 Architecture for visual grounding 在视觉定位中，模型的输入是一个问题和带有多个 bounding box 的图片，grounding 的目标是输出符合问题的 bounding box 作者提出的定位方法基于完全监督的 GroundR，做了如下修改， 将 GroundR 的融合部分替换为 MCB 引入视觉特征的线性嵌入 对两种输入模态使用 L2 正则化替代 batch norm "},"【论文】MLB.html":{"url":"【论文】MLB.html","title":"MLB","keywords":"","body":"﻿* 【论文】Kim, Jin-Hwa, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Hadamard product for low-rank bilinear pooling. pdf MCB 有两个问题; 一个问题出在采样方式上，MCB 依赖于 $E[\\left \\langle \\Psi(x, h, s), \\Psi(y, h, s)\\right \\rangle]=\\left\\langle x, y\\right\\rangle$ 使用投影后的特征代替原来的特征，但是要精确计算这个期望是十分困难的一件事情，因此 $h,s$ 在训练和测试的时候都固定不变 正因为 $h, s$ 固定不变，MCB 需要足够大才能最小固定带来的影响，例如 Fukui 在论文中的做法就是选择了一个 16k 维的特征向量 尽管与完全双线性池相比，MCB 的维度减少了两个数量级，但是这种高维特征的计算仍然会导致模型产生瓶颈 作者在本文中提出了 low-rank bilinear pooling using Hadamard product，这种方法可以将三维的权重矩阵转换为二维的权重矩阵，强制将权重矩阵的秩降低 于是，输入的特征向量分别经过一个线性变换，然后再通过哈达玛积乘起来，最终再乘以一个权重矩阵进行线性变换，用公式描述为 \\mathbf W^T_z(\\mathbf W^T_{\\mathbf x}\\mathbf x\\ \\circ\\ \\mathbf W^T_{\\mathbf y}y) 本篇论文的主要贡献是 用 low-rank bilinear pooling 代替 compact bilinear pooling 近似完全双线性池化 提出了 Multimodal Low-rank Bilinear Attention Networks（MLB） Low-rank bilinear model 双线性模型对每对输入特征计算线性变换的二次展开 f_i=\\sum_{j=1}^N\\sum_{k=1}^Mw_{ijk}x_jy_k+b_i=\\mathbf x^T\\mathbf W_i\\mathbf y + b_i 注意到这里的参数量是 $L\\times(N\\times M+1)$，$L$ 是输出的的特征数 Pirsiavash 提出过一个低阶双线性的方法可以减少矩阵权重 $\\mathbf W_i$ 的秩，从而在正则化的时候用到更少的参数 将权重矩阵进行重写 $\\mathbf W_i=\\mathbf U_i\\mathbf V_i^T$，$\\mathbf U_i\\in\\mathbb R^{N\\times d}$，$\\mathbf V_i\\in\\mathbb R^{M\\times d}$，于是， 对 $\\mathbf W_i$ 的秩就有一个限制 $d\\leqslant min(N, M)$ 进一步，$f_i$ 可以重写为 f_i=\\mathbf x^T\\mathbf W_i\\mathbf y + b_i=\\mathbf x^T\\mathbf U_i\\mathbf V_i^T\\mathbf y + b_i= 1^T(\\mathbf U_i^T\\mathbf x\\ \\circ\\ \\mathbf V_i^T\\mathbf y)+ b_i $1\\in\\mathbb R^d$ 表示示性函数。于是，我们就可以用两个三阶的矩阵 $\\mathbf U,\\mathbf V$，计算特征向量 $\\mathbf f$ 为了将权重矩阵的阶数，将示性函数 $1$ 替换为 $\\mathbf P\\in\\mathbb R^{d\\times c}$，$b_i$ 替换为 $\\mathbf b\\in\\mathbb R^c$，然后重定义一下 $\\mathbf U\\in\\mathbb R^{N\\times d}, \\mathbf V\\in\\mathbb R^{M\\times d}, \\mathbf f\\in\\mathbb R^{c}$，于是有 \\mathbf f=\\mathbf P^T(\\mathbf U^T\\mathbf x\\ \\circ V^T\\mathbf y)+\\mathbf b $d,c$ 可以视为两个超参数—— $d$ 决定了融合特征的维度，$c$ 决定了低阶双线性模型输出的维度 如上公式所示，MLB 的流程可以总结为如下： 使用两个无偏置的线性变换编码两个输入向量 然后使用哈达玛积得到融合特征 接着将融合特征通过一个带偏置的线性变换得到最终给定输出维度的输出向量 Low-rank bilinear pooling full model $U,V$ 可以有自己的偏置，于是，我们可以对上面的公式在进行一些修改得到完整的线性回归模型 \\begin{aligned} \\mathbf f &= \\mathbf P^T\\left((\\mathbf U^T\\mathbf x +\\mathbf b_x)\\circ(\\mathbf V^T\\mathbf y +\\mathbf b_y)\\right)+\\mathbf b\\\\ &=\\mathbf P^T(\\mathbf U^T\\mathbf x \\circ\\mathbf V^T\\mathbf y++\\mathbf U'^T\\mathbf y+\\mathbf V'^T\\mathbf y)+\\mathbf b' \\end{aligned} 其中 $\\mathbf U'^T=diag(\\mathbf b_y)\\cdot\\mathbf U^T,\\mathbf V'^T=diag(\\mathbf b_x)\\cdot\\mathbf V^T,\\mathbf b'=\\mathbf b + \\mathbf P^T(\\mathbf b_x\\circ\\mathbf b_y)$ Nonlinear activation 引入非线性激活能够增加模型的表示能力，首先想到的方法是在对输入向量进行线性变换后就使用非线性激活 \\mathbf f=\\mathbf P^T(\\sigma(\\mathbf U^T\\mathbf x)\\ \\circ \\sigma(V^T\\mathbf y))+\\mathbf b 这里 $sigma$ 表示任意的激活函数，不特指 sigmoid 但是这有个问题，两个输入向量来自两种模态的模型，在统计分布上两者不太相同，而且每个输入向量的梯度都直接依赖于哈达玛积的另外一个向量，那么这样就会导致一些不好的结果 那么再在上面表达式哈达玛积的外面套上一层激活函数呢？这也是不合理的，俺么在计算反向梯度的时候回导致激活函数被计算两次 但是，有一种替代的方法可以选择，就是只在哈达玛积外面套一层激活函数，即 \\mathbf f=\\mathbf P^T\\sigma(\\mathbf U^T\\mathbf x\\ \\circ V^T\\mathbf y)+\\mathbf b shortcut Connection 作者也尝试了加入 shortcut connection 构成 残差学习，如 \\mathbf f=\\mathbf P^T(\\sigma(\\mathbf U^T\\mathbf x)\\ \\circ \\sigma(V^T\\mathbf y))+h_x(\\mathbf x)+h_y(\\mathbf y)+\\mathbf b 注意到这个公式是 one-block layered MRN 的一般化形式。作者在自己提出的模型中并没有使用残差学习 Multimodal low-rank bilinear attention networks 假设问题的语义特征为 $\\mathbf q$，图片 $S\\times S$ 大小网格化区域的视觉特征为 $\\mathbf F$ Low-rank bilinear poolling in attention mechanism 作者采用的注意力机制在图片 $S\\times S$ 大小网格化的区域上使用了一个注意力概率分布 $\\alpha$，$\\alpha$ 使用 low-rank bilinear pooling，定义为 \\alpha=softmax\\left(\\mathbf P^T_{\\alpha}\\left(\\sigma(\\mathbf U_{\\mathbf q}^T\\cdot1^T)\\right)\\circ\\sigma(\\mathbf V_{\\mathbf F}^T\\mathbf F^T)\\right) $\\alpha\\in\\mathbb R^{G\\times S^2}$，$\\mathbf P{\\alpha}\\in\\mathbb R^{d\\times G}$，$\\sigma$ 表示 tanh 激活，$\\mathbf U{\\mathbf q}\\in\\mathbb R^{N\\times d}$，$\\mathbf q\\in\\mathbb R^{N}$，$1\\in\\mathbb R^{S^2}$，$\\mathbf V_{\\mathbf F}\\in\\mathbb R^{S^2\\times M}$ 如果上面的 $G>1$，则有一个多次观察的效果。softmax 作用于 $\\alpha$ 的每一行，偏置项因为考虑书写简单而忽略 MLB 注意力视觉特征 $\\hat \\mathbf v$ 通过 $\\mathbf Fi$ 乘以 $\\alpha{g,i}$做线性变换得到。每一个注意力分布 $\\alpha_g$ 对应于每一次观察 $g$，如果 $G>1$ 那么 $\\hat \\mathbf v$ 是 $\\hat\\mathbf v_g$ 的组合 \\hat \\mathbf v=\\overset{G}{\\underset{g=1}{||}}\\sum_{s=1}^{S^2}\\alpha_{g,s}\\mathbf F_s $||$ 表示向量连接 后验概率分布作为 softmax 的输出，其输入是 $\\mathbf q$ 和 $\\hat \\mathbf v$ 的 low-rank bilinear pooling p(a|\\mathbf q,\\mathbf F;\\Theta)=softmax\\left(\\mathbf P^T_{o}\\left(\\sigma(\\mathbf W_{\\mathbf q}^T\\mathbf q)\\right)\\circ\\sigma(\\mathbf V_{\\hat \\mathbf v}^T\\hat \\mathbf v)\\right)\\\\ \\hat a = \\underset{a\\in\\Omega}{argmax}\\ p(a|\\mathbf q,\\mathbf F;\\Theta) 其中，$\\hat a$ 表示一个预测的答案，$\\Omega$ 是候选答案集，$\\Theta$ 表示整个模型的参数 "},"【论文】MTTR.html":{"url":"【论文】MTTR.html","title":"MTTR","keywords":"","body":" 【论文】Botach A, Zheltonozhskii E, Baskin C. End-to-End Referring Video Object Segmentation with Multimodal Transformers. （pdf） "},"【论文】Multimodal Transformer.html":{"url":"【论文】Multimodal Transformer.html","title":"Multimodal Transformer","keywords":"","body":" 【论文】Yu, Jun, Jing Li, Zhou Yu, and Qingming Huang. Multimodal transformer with multi-view visual representation for image captioning. （pdf） Why is MT model 以往流行的方式都是使用 encoder-decoder 结构，在 encoder使用 CNN 提取局部视觉特征，decoder 使用 RNN 根据视觉特征生成 caption，这些方式有如下的一些问题： 它们的注意力机制都只注重对 co-attention 进行建模，即只关注模态内部的交互作用（e.g. object-to-word)，而忽略了对模态间的交互作用（e.g. object-to-object，word-to-word） 并不能充分理解视觉物体之间的复杂关系 基于区域的视觉特征并不能囊括图片中的所有物体，从而导致视觉特征不够有效，无生成准确的 caption 在篇文章中，作者首次提出将 Transformer 引入到 image caption 建立了 MT model。Transformer 的好处在于抛弃了传统的 CNN 和 RNN，完全依赖于注意力机制来获取输入和输出自己爱你的全局关系。MT model 的主要表现如下： MT model 在一个统一的 attention block 中同时捕捉模态内和模态间的交互作用 堆叠 attention blocks 加深模型可以使得 MT model 进行复杂的多模态推理 为了进一步提高 image captioning 的表现，MT model 还引入了 multi-view visual feature，提供更多样、更具区别性的视觉特征 Multimodel Transformer MT model 的结构如下所示，包含一个 image encoder 和 caption decoder。首先，通过 Faster RCNN 提取出来区域视觉特征，然后将其输入到 encoder 中获得 attended visual features。decoder 根据这个视觉特征和前一个单词来预测一下个单词。细节如下图描述 测试时，生成第 $t$ 个单词的 input caption feature 是 $Y{\\leq t}=[y_1, y_2, \\cdots,y{t-1},\\mathbf 0,\\cdots,\\mathbf 0]\\ \\mathbf 0\\in\\mathbb R^{dy}$，该 caption feature 和 image feature 一同送入 decoder 产生预测单词。然后，下一次预测的第 $t+1$ 个单词的时候，input caption feature 就是 $Y{\\leq t+1}$ Image Encoder with Multi-View Visual Representation 在这一节，引入 multi-view image representation 对上述的 image encoder 作出修改。不同于以往，作者采用基于区域的 local multi-view features——将每个目标检测器视为一个角度，每个检测器通过不同 backbone 的 Faster RCNN 得到。考虑到不同检测器得到的特征并不对齐，作者提出了两种 multi-view image encoder，Aligned Multi-View（AMV）image encoder 和 Unaligned Multi-View（UMV）image encoder Aligned Multi-View image encoder1 作者认为统一的边框不是很好，会降低 multi-view features 的多样性，限制 encoded image features 的表征能力。同时，AMV 模型隐式的限制了目标检测器只能是 Faster RCNN 模型（Faster RCNN 要么使用需要计算好的 proposals 作为输入，要么使用自己的 RPN 生成 proposals），这样对于像是 RetinaNet 或者 YOLO 这种 one-stage 的模型就没有办法作为 AMV 的目标检测器 Unaligned Multi-View image encoder 为了解决 AMV 的问题，作者又提出了 UMA 直接整合不同目标检测器的 unaligned multi-view features 值得注意的是，UMV 可以通过堆叠学习到更多不同视角的交互作用 Experiments 我们下面来看一下论文里几张实验的图片 Attentions of MT~sv~（single-view MT model） Encoder 观察上图我们可以发现： 在 Enc(SA)-1 中，最大的注意力值几乎出现在对角线上，说明在 encoder 的第一层中没有学到图像中物体两两的交互作用 在 Enc(Sa)-6 中，最大的注意力值几乎成垂直线（在第 4、9、13 列），对应图中的关键对象（如女孩、滑板） 这表明，所有 attended features 都倾向于与关键对象产生交互作用 Attentions of MT~sv~ Decoder 如上图所示， caption decoder 第一层和第六层的注意力图都反映了 paried words 的相似性。和 Enc(SA)-1 相似，Dec(SA)-1 并没有习得两两单词之间的交互作用。但是在 Dec(Sa)-6 中，我们发现，许多单词开始有了联系 guided-attention（GA）反应了多模态之间的相互联系（word-object）。在 Dec(GA)-1 中，一些词和物并不能很好的配对。但是在 Dec(GA)-6 中，关键对象和它们匹配的词之间的 word-object relationship 相当明显 Attentions of MT~umv~ Encoder 观察上图我们可以发现： 在 Enc(GA)-1 中，来自不同视点的 unaligned objects 自动对齐了，如在 R-101 和 R-152 中的第五个对象，R-101 中的第三个对象和 R-152 中的第 6 个对象 在 Enc(GA)-3 中，开始会涉及到上下文环境，如 R-152 中的第五个对象和 R-101 中的第一个、第四个对象，他们都对应女孩的身体 在 Enc(GA)-6 中，涉及到特定对象的一些关系，以及背景环境的关系会被模型注意到，如 R-152 中的第十三个对象和 R-101 中的第十个对象，它们都属于接街道背景 这表明， UMA image encoder 能够学会对齐来自不同目标检测器的对象，同时能够通过多视点特征探索物体和物体之间的更为复杂的关系，能够提供对图像内容的 fine-grained understanding 同时，作者也在下图展示了一些 image captioning 的预测结果 通过前两行可以明显发现，MT~umv~ 的预测优于 MT~sv~ 的预测。第三行展示了两个 MT~sv~ 优于 MT~umv~ 的例子。最后一行展示了两个模型都预测错误的例子。作者给出的结论是： 尽管 MT~umv~ 预测正确的数量多于 MT~sv~，但是两者之间的性能差距没有本质上的区别，各有各的优势。甚至把两者融合在一起能够形成一个更具多样性的模型 不正确的预测往往会出现在图像上的一些小物体上 1. AMV 里面的 $d_i$ 应该是可以不同的，即 $d_1\\neq d2\\neq\\cdots$ 这种 ↩ "},"【论文】Oscar.html":{"url":"【论文】Oscar.html","title":"Oscar","keywords":"","body":" 【论文】Li X, Yin X, Li C, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks.（pdf） Why is Oscar 根据作者 Chunyuan Li 自己的描述，现有的 VLP（Vision-Language Pretraining）方法只是将图像区域特征和文本特征连接起来作为模型的输入以进行预训练，并不为模型提供任何线索，希望模型能利用 Transformer 的自我注意机制，这是一种使用蛮力来学习图像文本语义对齐的方式 在 Oscar（Object-Semantics Aligned Pre-training）中，作者把同语义下的物体（名词）用作视觉和语言对齐的锚点（定位点），以简化图像和文本之间的语义对齐的学习任务。这样的方式，作者总结为儒学经典《大学》里的『格物以致知』：人接触、感觉、认识事物（物体），然后产生并获得知识 听起来很玄学吧。简单一点来说就是，作者在 MSCOCO 数据集上发现 49.7% 的样本有 1 个物体同时出现在两种模态中，22.2% 的样本有 2 个物体同时出现（如 Fig 2 图片中狗和沙发是主要元素，dog 和 couch 同时也在 caption 中出现，这就叫做在两个模态中同时出现），12.9% 的样本有 3 个物体同时出现，那么加起来的话就是 84.8% 的样本图像和文本是有共同对象出现的。于是，基于这样的发现，所谓的格物致知就是我们把图像中的显著元素标出来，然后对应到 caption 中的文本建立一个直观上的关联，然后让预训练模型围绕标出来的这些显著元素再去学习 我们以 Fig2 的例子来做说明，对于这个样本我们可以看到狗和沙发作为显著元素在图像和文本中同时出现，那么我们就在（b）中通过 Faster RCNN 检测出这两个元素对应对象的标签，并将得到的标签融入到预训练模型的输入中，接下来预训练模型就围绕 dog 和 couch 这两个显著元素学习跨模态的对齐。这两个显著元素也就是 Chunyuan Li 所说的线索 具体来说怎么操作呢？ Oscar 构造如下图所示的三元组输入 Word-Tag-Image triple $(\\pmb w, \\pmb q,\\pmb v)$，下面我们重点讲一下 $\\pmb w, \\pmb q,\\pmb v$ 是怎么来的 输入一张图像有 $K$ 个区域，经过 Faster RCNN 处理得到一个特征元组 $(v',z)$，$v'\\in\\mathbb R^P$ 表示 region feature，$z\\in\\mathbb R^R$ 表示 region position，然后经过一个线性层将 $v'$ 和 $z$ 融合得到 $v$ 同时，拿着 Faster RCNN 输出的标签去 word embedding 中找到对应的嵌入作为 $\\pmb q$（有两点疑问，这个找是怎么做的呢，如果出现近义词怎么处理呢，比如标签给出的是 dog，caption 中的描述是 Husky？我们可不可以分两步去做，第一步自动化地去学习显著元素将它们对齐提供线索，第二再是 VLP） 说到这里，估计你已经对 Oscar 比较了解了。这时，我们就可以说说 Fig 2 (c) ，除了对 MSCOCO 的观察发现，作者的第二个理由认为三元组这样的线索能够引导预训练模型更好的实现跨模态的对齐。我们可以看到 Fig 2 (a) 中 dog 和 couch 的图像区域是有重叠的，在视觉表示空间中很难区分，而在语言表示空间，由于 BERT 通过大量的语料得到物体确切上下文的信息，因而很好进行区分。所以，$\\pmb q$ 的引入对于跨模态交互来说是有利的 下面我们看一下完整的 Oscar 模型。其预训练只使用 6.5M 的图文对样本（UNITER 是 9.18M，IXME 是 9.6M），在 6 个下游任务上进行 fine-tunning，有两个版本 Oscar~B~ 和 Oscar~L~ 分别以 BERT~base~ 和 BERT~large~ 作为 VLP model 在上图中还有两个内容我们没有介绍——Masked Token Loss 和 Contrastive Loss 根据三元组中三个项目的分组方式，作者从两个不同的角度设计了预训练的目标。先说说三元组的分组方式，$\\pmb x$ 表示 modality view，区分文本还是图像；$\\pmb x'$ 表示 dictionary view，区分语言空间还是视觉空间 Dictionary view：通过 Masked Token Loss 衡量模型根据上下文恢复缺失元素（单词或对象标签）的能力，注意这里遮掉的 object tag 是对应的 word embedding 被遮掉。记 $\\pmb h\\triangleq[\\pmb w,\\pmb q]$，Masked Token Loss 记为 $\\mathcal L{MTL}=-\\mathbb E{\\pmb v,\\pmb h}\\sim\\mathcal Dlog(hi|\\pmb h{\\backslash i},\\pmb v)$ Modality view：通过 Contrastive Loss 衡量模型区分原始三元组和污染版本（即原始物体标签被随机采样的标签替换）的能力。记 $\\pmb h’\\triangleq[\\pmb q,\\pmb v]$， Contrastive Loss 记为 $\\mathcal L{C}=-\\mathbb E{\\pmb h',\\pmb w}\\sim\\mathcal Dlog\\left(y|f(\\pmb h',\\pmb w)\\right)$ 总的损失函数定义为 $\\mathcal L{Pre-training}=\\mathcal L{MTL}+\\mathcal L_C$ Oscar 使用的预训练数据集如下表所示 下面是 Oscar 在下游任务的具体表现 另外，作者使用 t-SNE 将 COCO 测试集的图文对的语义特征空间可视化了在二位平面上，比较了带有和不带有物体标签的预训练模型 第一个发现是关于同一个物体的两种不同模态的：借助对象标签，可以大大减少两个模态之间同一对象的距离。例如，Oscar 中 Person 的图片和文本表示比 baseline 方法中的视觉表示和文本表示更接近，这个在图4中用红色曲线表示 第二个发现是不同物体间的：添加物体标签后，具有相关语义的对象类越来越接近（但仍可区分） 而这在 baseline 方法中有些混合，例如图中用棕色曲线表示的动物（zebra, elephant, sheep 等） 下图展示了 Oscar 和 basline 方法在生成 caption 上的一些差异，可以看到 Oscar 能够生成一些更细致的描述，而这些细致的描述离不开 object tags 的引导 Reference 格“物”致知：多模态预训练再次入门 "},"【论文】Pixel-BERT.html":{"url":"【论文】Pixel-BERT.html","title":"Pixel-BERT","keywords":"","body":" 【论文】Huang Z, Zeng Z, Liu B, et al. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers.（pdf） What is Pixel-BERT Piel-BERT 提供了一种新的思路：在像素上实现与文本的对齐。为什么要在像素上对齐呢，作者给出的理由还不得不说很有说服力。如下图所示，在 A 例子中想要通过目标检测模型获取飞机的状态这是比较困难的；B 例子中，即使在一个框中可以检测到地面和女孩，但因为她们的region 是重叠，之后在推理的过程中想要通过两个边界框来判断女孩和地面的实际空间联系会比较困难；C 例子中，视觉特征只有长颈鹿、草、树等等，但很难去推理出这些动物的状态 所以，简而言之，通过这些例子可以看到基于 region 的视觉特征提取器会在语义理解上产生信息隔阂。于是，作者就考虑在像素上实现对齐通过深层多模态 transformer 在统一的端到端框架上来共同学习视觉和语言 embedding。同时，在像素上的对齐减轻了标注 bounding box 的耗费 Pixel-BERT 从结构上来看 Pixel-BERT 属于 single-stream model，采用 BERT 作为跨模态对齐模块 Sentence Encoder 操作和 BERT 中基本一致，记 $w_i$ 表示 token embedding，$p_i$ 表示 position embedding，$s_w$ 表示 semantic embedding，那么 Sentence Encoder 的最终输出为 \r \\hat w_i=LayerNorm(w_i+p_i+s_w)\r 没搞懂 semantic embedding 是什么，反正可以忽略（Since the summation of position and semantic embedding is the mathematic equivalence to one embedding, we will omit the $s_w$ term in our implementation） CNN-based Feature Embedding 对于输入图片 $I$，先使用 CNN backbone（ResNet 或 ResNeXt）去提取特征，然后将提取到的特征沿空间维度进行 flat，flatten feature 记为 $\\mathbf v=\\left{v_1,\\cdots,v_k\\right}$，$k$ 表示像素特征的数量。visual embedding 可以按照如下的公式进行计算 \r \\hat v_i=v_i+s_v\r where $s_v$ is a semantic embedding vector to distinguish the difference with language embedding. Since all pixels share the same $s_v$, this embedding vector can be considered as a bias term to be combined with the CNN backbone Cross-Modality Module 这一块和之前的预训练通用模型就比较相似了，不再做具体的介绍。我们讲一下 Pixel-BERT 是如何形成端到端模型的。作者将 CNN backbone 和 BERT 合在一起构成一个模型，也就是说，Pixel-BERT 的参数是可以反传到 CNN backbone 的，使得 CNN backbone学习到的特征更加适合于跨模态对齐（之前的预训练模型中的 Fater RCNN 是 freeze 住的） Pr-training Pixel-BERT 使用的是两个典型的预训练代理任务：MLM 和 ITM，这里我们就不再做详细的介绍。重点关注一下论文中介绍的 Pixel Random Sampling 为了增强特征学习、防止过拟合，受到 dropout 的启发，Pixel-BERT 在预训练期间会进行 Pixel Random Sampling，即在每个迭代过程中，在抽取像素特征后，会在它们上随机采样一部分形成 visual embedding 作者在论文中指出 Pixel Random Sampling 对于模型训练有两方面的好处： 鼓励模型从不完整的视觉输入上学习语义知识，进而增强稳健性 减少输入元素的数量，这样可以减少计算消耗且加速训练进度 在实验中固定对每张图片从特征图中随机采样一个 100 个 pixels，需要再次强调的是 Pixel Random Sampling 仅在预训练阶段进行，在 fine-tunning 中考虑信息缺失的影响，要求输入在训练和测试阶段都保持一致 Experiment Pixel-BERT 在 MS-COCO 和 VG 上进行预训练，利用 MSCOCO 的 image-level caption annotations 和 VG 的 region-level caption annotations 作为预训练数据 下面是 Pixel-BERT 在 VQA、NLVR 和 Image-Text Retrieval 3 个 下游任务上的表现 下图是在 Pixel-BERT 在 MSCOCO 上的一些注意力分布可视化结果（下图展示的都是一些比较突出的内容，我比较好奇像 long strip 或者 sitting inside 这种比较小的细节 Pixel-BERT 的注意力会怎么样分布） Reference 【论文阅读记录01】《Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal-Transformers》 "},"【论文】Prompting Visual-Language Models for Efficient Video Understanding.html":{"url":"【论文】Prompting Visual-Language Models for Efficient Video Understanding.html","title":"Prompting Visual-Language Models for Efficient Video Understanding","keywords":"","body":" 【论文】Ju C, Han T, Zheng K, et al. Prompting Visual-Language Models for Efficient Video Understanding.(pdf) 作者的目的就是希望将已有的一些比较好的 image-based VL model 用到视频任务上，作者将这个过程定义为 『model adaptation』——具体来说，作者将 CLIP freeze 住，image encoder 上加了一个轻量的 transformer 来进行视频的时间建模，同时为了适应 CLIP 预训练好的 text encoder，作者采用了连续的 prompt 向量来进行提示，最终在少量参数的训练下，新的模型就能适应具体的视频任务 记数据集为 $\\mathcal D=\\left{\\mathcal D{train}, \\mathcal D{val}\\right}$，具体 $\\mathcal D{train}=\\left{(\\mathcal V_1, y_1),\\cdots,(\\mathcal V_n, y_n)\\right}$，其中 $\\mathcal V_i\\in\\mathbb R^{T\\times H\\times W\\times3}$ 表示视频，根据任务的不同长几秒（recognition and retrieval），或几分钟（localisation）；同理，$y_i$ 也根据任务的不同可以是 recognition 中的一个 $\\mathcal C{train}$ label，也可以是 localisation 中的一个长 $T$ 个timestamps 的 dense action label，又或者是 retrieval 中的一个 text description 先说一下 prompts，我们知道在 CLIP 中 text encoder 的处理可以表示为 \r c_{cat}=\\Phi_{text}(\\text{TOKENISER(\"this is a photo of [ }\\underline{\\text{cat}}\\text{ ]\"})\\\\\r c_{dog}=\\Phi_{text}(\\text{TOKENISER(\"this is a photo of [ }\\underline{\\text{dog}}\\text{ ]\"})\r 那么记 $ai\\in\\mathbb R^D$ 表示一个 D-dimension 可学习向量，对 action recognition 任务就可以得到如下的连续提示，注意 $a_i$ 在所有 action categories 之间共享 \r c_{archery}=\\Phi_{text}(a+1, \\cdots,\\text{TOKENISER(\"}\\underline{\\text{archery}}\\text{ \")},\\cdots,a_k)\\\\\r c_{bowling}=\\Phi_{text}(a+1, \\cdots,\\text{TOKENISER(\"}\\underline{\\text{bowling}}\\text{ \")},\\cdots,a_k)\\\\\r 在 action recognition 中需要解释两个概念—— closed-set 和 open-set，在 closed-set 中训练集和验证集的 action categories 是一样的，即 $\\mathcal C{train}=\\mathcal C{val}$；在 open-set 中，$\\mathcal C{train}\\cap\\mathcal C_{val}=\\varnothing$ 对 action localisation 任务，采用 two-stage 的处理方式：先用带一层 transformer 的 CLIP image encoder 来得到每个 snippet 的 frame-wise embeddings，然后再将这些 embedding 送入 proposal 检测器中预测 actioness、centerness 和 boundaries 对 retrieval 任务，将 $\\text{TOKENISER(\"}\\underline{\\text{archery}}\\text{ \")}$ 换为 $\\text{TOKENISER(\"}\\underline{\\text{sentence}}\\text{ \")}$ 接着我们说一下 temporal modeling， 如前说述作者在 CLIP image encoder 的基础上用了一个 light transformer 来建立时间关系，这个 transformer 可记为 $\\Phi_{TEMP}$，那么 image encoder 就被改造为一个 video encoder，表示为 \r v_i=\\Phi_{video}(\\mathcal V_i)=\\Phi_{TEMP}(\\left\\{\\Phi_{image}(I_{i1}),\\dots,\\Phi_{image}(I_{iT})\\right\\})\r 为了标明时间顺序，作者在图像特征里面加入了 temporal positional encoding。$v_i\\in\\mathbb R^{T\\times D}$ 表示 $T$ 帧的 dense feature embedding 对于 recognition 和 retrieval 来说，video-snippet-level feature 可以通过一个 mean pooling 得到，即 $\\bar{v}i=\\Phi{POOL}(v_i)\\in\\mathbb R^{1\\times D}$ 对于 localisation 来说，对 detected action-proposal 做 mean pooling，得到一个 proposal-level feature 最后，作者在训练的时候采用了 NCE loss \r \\mathcal L=-\\sum_i\\left(\\log\\frac{exp(\\bar v_i\\cdot c_i/\\tau)}{\\sum_jexp(\\bar v_i\\cdot c_j/\\tau)}\\right)\r 下面是论文中的一些实验结果， Closed-set Action Recognition Few-shot Action Recognition Open-set Action Recognition Closed-set Action Localisation Open-set Action Localisation Text-Video Retrieval "},"【论文】RCNN.html":{"url":"【论文】RCNN.html","title":"RCNN","keywords":"","body":"﻿* 【论文】Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.（pdf） RCNN 进行物体检测可以大致为三个步骤， 获取输入图像，提取 2000 个类别独立的候选区域 对每个区域利用 CNN 抽取一个固定长度的特征向量 再对每个区域利用 SVM 进行目标分类 候选区域 Region Proposal 是一种传统的区域提取方法，基于启发式的区域提取方法，采用的是 Selective Search 算法 Selective Search 算法的大致流程如下: 将图像分割为小区域 查看现在小区域，合并可能性最高的两个区域，重复知道整张图片合成一个区域位置。我们通常优先合并以下区域：① 颜色直方图相近的；② 梯度直方图相近的；③ 合并后总面积小的；④ 合并后总面积在其 bbox 中所占比例大的。在合并时需要保证合并操作的尺度较为均匀，避免一个大区域陆续吃掉其他小区域，保证合并后形状规则 输出所有曾经存在过的区域，即所谓候选区域 然后我们将裁剪出来的小图片调整为 $227\\times227$ 的大小，方便输入 CNN 中。这里论文中考虑了两种处理方法： 各向异性缩放，也就是不管是否发生就去，直接将截取出来的小图片缩放到 $227\\times227$，如下图 (D) 各向同性缩放，有两种方法 ①先扩充后裁剪：直接在原始图片中，把 bounding box 的边界扩展为正方形，然后进行裁剪，如下图 (B) ②先裁剪后扩充：先把 bouding box 图片裁剪出来，然后用固定的背景颜色填充成正方形图片，如下图 (C) 另外，作者在上面的缩放中还添加了 padding 处理，上图每个示例的第一行是 padding = 0 的结果，第二行是 padding = 16 的结果。最终，作者发现采样各向异性缩放、padding = 16 的精度最高 CNN 提取特征 作者选用了 AlexNet，直接套用其训练好的参数作为初始化参数，然后再经过 fine-tuning 训练，最后得到一个 4096 维的特征向量。这里还需要做一点改变，我们将预训练 CNN 模型的最优一层替换掉，假如要检测的物体有 $N$ 类，我们就让最后一层替换为 $N+1$ 个神经元，这一层直接采用随机初始化参数的方式 fine-tuning 训练很简单，就是把前面 Selective Search 搜索出来的候选框当做输入送入 CNN 中。但是，这里有一个正负样本的区分：我们计算 Selective Search 候选框与人工标注矩形框的 IoU，如果 IoU > 0.5，我们把这个候选框标注成物体类别（正样本），否则我们就把它当做背景类别（负样本） 分类与边界回归 这一步要做两件事情， 对前一步的输出向量进行分类 通过边界回归框回归获得精确的区域信息 分类 对于分类，我们使用 $N$ 个 SVM，每一个 SVM 做一个二分类。于是，我们就很容易判断一个方框内是否包含某一物体了。但是，这里有个问题，要是这个方框只包含了这个物体的一部分呢？ RCNN 还是采用了 IoU 作为阈值，如果一个候选框和人工标注矩形框的 IoU 为什么这里的 IoU 阈值和上面 fine-tunning 的 IoU 阈值不同？ 主要考虑 fine-tuning 过拟合的问题，比较大的阈值能够扩大正样本的数量；而 SVM 用于最终分类，越难分的数据越有利于 SVM 的训练，所有对样本的定义会更家严格 那么，这里为什么使用 SVM 而不是原来 AlexNet 中的 softmax 进行分类呢？ SVM 和 CNN 训练过程对政府样本的定义方式不同，导致 softmax 输出的精度会比 SVM 低一些。例如，上面我们将 IoU > 0.5 的样本都标记为了 CNN fine-tuning 时的正样本，这对 bounding box 的位置限制过于宽松。而 SVM 适用于少样本训练，只有当 bounding box 将整个物体包含进去了，我们才会输出正样本的结果 位置精修 目标检测问题的衡量标准是重叠面积，许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤：对每一类目标，使用一个线性回归器进行精修，输入为 CNN 最后一层的特征，输出为 $x,y$ 方向上的缩放和平移。这里判定候选框和人工标注矩形框的 IoU > 0.6 才算做正样本 在测试阶段，我们还会涉及一个非极大值抑制（NMS）抑制那些冗余的候选框 NMS 流程如下： ① 将所有框的得分排序，选中最高分及其对应的框 ② 遍历其余的框，如果和当前最高分框的重叠面积（IoU）大于一定阈值，我们就将框删除 ③ 从未处理的框中继续选一个得分最高的，重复上述过程 "},"【论文】ReLoCLNet.html":{"url":"【论文】ReLoCLNet.html","title":"ReLoCLNet","keywords":"","body":" 【论文】Zhang H, Sun A, Jing W, et al. Video corpus moment retrieval with contrastive learning.（pdf） 我们先介绍这篇工作中的一个新的任务：video corpus moment retrieval (VCMR)，就是将之前的 retrieval 和 grounding 任务结合在一起，从一堆 untrimmed、unsegmented 的视频中检索 query 对应的一段 temporal moment VCMR 的数学描述如下 记 video corpus 为 $\\mathcal V=\\left{V^1,V^2,\\dots,V^M\\right}$，其中每一个视频记为 $V^k=[fi]^{T-1}{i=0}$，query 记为 $Q=[qi]^{n_q-1}{i=0}$，VCMR 有两个任务： 从 $\\mathcal V$ 中找出和 query 最匹配的 $V^*$ 从 $V^*$ 中回归出 $\\tau^{s/e}$ 经过特征提取后，$Q\\rightarrow\\mathbf Q=[\\mathbf qi]^{n_q-1}{i=0}\\in\\mathbb R^{dw\\times n_q}$，$V\\rightarrow\\mathbf V=[\\mathbf v_i]^{n_v-1}{i=0}\\in\\mathbb R^{dv\\times n_v}$，视频被编码为长度为 $n_v$ 的 feature sequence，$\\tau^{s/e}$ 映射对应 $i^{s/e}$-th visual feature，于是 target segment 就可以表示为 $\\mathbf m^=\\left{\\mathbf v_i|i=i^s,\\dots,i^e\\right}$，进一步 grounding 的任务就可以表示为 $\\mathbf m^=\\arg\\max{\\mathbf m\\in V,V\\in\\mathcal V}p(\\mathbf m|\\mathbf V,\\mathbf Q)p(\\mathbf V|\\mathbf Q)$，再进一步加上 retrieval 任务后 VCMR 就可以表示为 \r \\mathbf V^*=\\arg\\max_{\\mathbf V\\in\\mathcal V}p(\\mathbf V|\\mathbf Q)\\text{ and } \\mathbf m^*\\approx\\arg\\max_{\\mathbf m\\in \\mathbf V*}p(\\mathbf m|\\mathbf V^*, \\mathbf Q)\r 一般在解决 VCMR 任务上会有两种常规的做法： 先分别提取两个模态的特征，然后再对齐 采用细粒度的跨模态交互方法学得 multi-modal representation 本文沿用第一种方法，设计了一个叫 ReLoCLNet（Retrieval and Localization Network with Contrastive Learning）的网络，采用两个对比学习目标来 refine text/video encoder，同时针对 VCMR 任务更好对齐两个模态的特征 VideoCL（video contrastive learning）目标在 video-level 最大化 query 和 candidate video 之间的 mutual information，作用域在不同视频 FrameCL（frame contrastive learning）目标用于在 frame-level 对齐 query 和 moment region，作用域在一个视频内，将 target moment 中的帧视为 foreground（正样本），其余的帧都视为 background 图一展示两种跨模态交互的方式—— unimodal encoding（即我们常说的双塔模型）和 cross-modal interaction learning（即单塔模型），作者认为单塔虽然能获得更好的跨模态交互，但是在 retrieval 需要进行 \"×N\" 的推导，这在效率上试试十分低下的。所以，作者在 ReLoCLNet 中用双塔结构 早期的工作指出将 video retrieval 和 moment localization 作为一个 joint objective 训练会比采用两段式的方式好很多。于是，ReLoCLNet 在训练时使用 VideoCL 和 FrameCL 同时更新网络，在 inference 时，分开对 video 和 query 提取特征保证 retrieval 的高效性 ReLoCLNet 的整个模型架构如下所示 query 端采用两个 transformer blocks 得到 $\\tilde{\\mathbf Q}$，然后通过如下的方式获得 modularized query vector，其中 $m\\in\\left{v, s\\right}$，$\\mathbf q_v$ matching for visual features，$\\mathbf q_s$ matching for subtitle features，因为 TVR 数据集中由于类似于字幕和音轨的数据，这部分数据建模得到的特征记为 $\\mathbf S\\in\\mathbb R^{d_w\\times n_v}$ \r \\boldsymbol \\alpha^q=\\text{Softmax}(\\mathbf W_{m,\\alpha}\\cdot\\tilde{\\mathbf Q})\\in\\mathbb R^{n_q}\\\\\r \\mathbf q_m = \\sum_{i=0}^{n_q-1}\\alpha^q_i\\times \\mathbf q_i\\in\\mathbb R^d\r video 端采用 standard trans + cross trans 的方式建模视频和字幕信息，融合视频和字幕两个模态信息的特征表示为 $\\mathbf H'_v,\\mathbf H'_s$，之后作者又分别引入了一个 standard trans refine encoded cross-modal representations \r \\mathbf H_v=\\text{Transfomer}(\\mathbf H'_v) \\\\\r \\mathbf H_s=\\text{Transfomer}(\\mathbf H'_s)\r Video Retrieval Module 很简单，看图就能明白，分两端—— video 和 subtitle，分别用前面得到的 video representation 和 subtitle representation 跟 query representation 做点乘算相似度，即 $\\boldsymbol\\varphi_m=\\text{norm}(\\mathbf H_m^T)\\cdot\\text{norm}(\\mathbf q_m)$，选相似度最大的 $\\varphi=\\max(\\boldsymbol\\varphi)=\\max([\\varphi^0_m,\\varphi^1_m,\\dots,\\varphi^{n_v-1}_m])$，最终，有字幕输入的话，取 $\\varphi=\\frac{1}{2}(\\varphi_v+\\varphi_s)$，否则 $\\varphi=\\varphi_v$ 训练目标采用 hinge loss，先采样两个负样本对 $\\left{(Q^-i,V)\\right}^N{i=1}$ 和 $\\left{(Q,V^-i)\\right}^N{i=1}$，假设两个负样本对得到的相似度得分是 $\\boldsymbol\\varphi',\\boldsymbol\\varphi''$，hinge loss 可以表示为如下形式，$\\Delta$ 是预先定好的一个 margin value $\\Delta=0.1$ \r \\mathcal L^{VR}=\\max\\left(0,\\Delta+\\frac{1}{N}\\sum\\boldsymbol\\varphi'-\\varphi\\right)+\\max\\left(0,\\Delta+\\frac{1}{N}\\sum\\boldsymbol\\varphi''-\\varphi\\right)\r Moment Localization Module $\\mathbf qm$ 进来先过了一个 FFN 然后再分别和 $\\mathbf H_m$ 算相似度 $\\mathbf S{mq}\\in\\mathbb R^{nv}$，方法和上面类似，然后在相似度得分上直接用一维卷积预测时间边界 \r \\mathbf S_{start}=\\text{Conv1D}(\\mathbf S), \\ \\mathbf S_{end}=\\text{Conv1D}(\\mathbf S)\r 经过 Softmax 后得到时间边界预测概率 $\\mathbf P{start},\\mathbf P_{end}$，在预测时，取联合概率最大的作为时间边界，即 \r (\\hat i^s,\\hat i^e)=\\arg\\max_{a^s,a^e}\\mathbf P_{start}(a^s)\\times\\mathbf P_{end}(a^e)\\\\\r P^{se}=\\mathbf P_{start}(\\hat i^s)\\times\\mathbf P_{end}(\\hat i^e)\r 训练 loss 采用 CE loss，记为 \r \\mathcal L^{ML} = \\frac{1}{2}\\times\\left(\\text{CE}(\\mathbf P_{start}, \\mathbf Y_{start})+\\text{CE}(\\mathbf P_{end}, \\mathbf Y_{end})\\right)\r Video and Frame Contrastive Learning $\\mathbf H'm$ 输入后按照如下方式得到 modularized video representation，基本同 query 端的处理 \r \\boldsymbol \\alpha^m=\\text{Softmax}(\\mathbf W_{m,\\alpha}\\cdot\\mathbf H'_m)\\in\\mathbb R^{n_v}\\\\\r \\mathbf c_m = \\sum_{i=0}^{n_v-1}\\alpha^m_i\\times \\mathbf h'_{m,i}\\in\\mathbb R^d\r 采用 NCE loss 计算 VideoCL score，记为 $\\mathcal I^e{m}$ 还是分两种情况，如果有字幕输入的话，$\\mathcal I^e=\\frac{1}{2}(\\mathcal I^e{v}+\\mathcal I^e{s})$，否则 $\\mathcal I^e=\\mathcal I^e_{v}$，训练 loss $\\mathcal L^{VideoCL}=-\\mathcal I^e$ 对于 FrameCL，输入的 $\\mathbf H'm$ 分为两部分—— positive/foreground video features $\\mathbf H'{m,F}$ 和 negative/background features $\\mathbf H'{m,B}$，如下图所示，有一个 discriminator，作者在这里采用了一个叫 Jensen-Shannon MI(mutual information) estimator 的东西来最大化 query 和 foreground 的 MI，同时最小化 query 和 background 的 MI，其中 $\\text{sp}$ 表示 softplus activation $\\text{sp}(x)=\\log(1+ex)$，$\\mathcal C{\\theta}:d\\times d\\rightarrow\\mathbb R$ 表示 discriminator 最终，FrameCL loss 记为 \r \\mathcal L^{FrameCL}=-\\mathcal I^a\r 最终，ReLoCLNet 的训练 loss 就表示为如下形式，其中 $\\lambda1=1,\\lambda_{2, 3, 4}=0.01$ 保证每个 loss 的量级是相同的 \r \\mathcal L=\\lambda_1\\mathcal L^{VR}+\\lambda_2\\mathcal L^{ML}+\\lambda_3\\mathcal L^{VideoCL}+\\lambda_2\\mathcal L^{FrameCL}\r 在推理的时候，先计算 query 和 $M$ 个 video 的相似度 $\\boldsymbol\\varphi=[\\varphi_1,\\dots,\\varphi_M]$，然后选取 top-$K$ ($K=100$) 个 retrieved video 预测 moment，用 $P^{se}$ 表示 predicted moment 的分数为，VCMR score 就可以表示为 $\\delta=P^{se}\\times r^{\\gamma\\cdot\\varphi}$，其中 $\\gamma$ 作为一个超参数用于平衡 retrieval 和 localization scores "},"【论文】ResNet.html":{"url":"【论文】ResNet.html","title":"ResNet","keywords":"","body":"﻿* 【论文】 He K , Zhang X , Ren S , et al. Deep Residual Learning for Image Recognition[J]. 2016.（pdf） He K , Zhang X , Ren S , et al. Identity Mappings in Deep Residual Networks[J]. 2016.（pdf） 【新颖点】 提出了短路机制解决深度网络退化的问题 提出了残差块 深度网络退化问题 从经验来看，网络的深度对模型的性能至关重要，当网络层数增加后，网络可以进行更复杂的特征模式的提取，但是这却只是经验上的判断。ResNet 的作者通过实验数据说明，56 层的网络不论是训练时错误率还是测试时错误率都高于 20 层网络。这不是因为深层网络存在的梯度消失或爆照问题引起的，我们可以通过 batch norm 等首段来解决梯度消失或爆炸导致的深度网络难训练的问题。但是，网络越深表示能力却越弱，这是一个非常令人诧异的问题，这就是网络的退化 残差学习 针对于网络退化问题，有两种解决思路： 一种是调整求解方法，比如更好的初始化、更好的梯度下降算法等 另一种是调整模型结构，让模型更易于优化 ResNet 的作者从后者入手，将堆叠的基层称为一个块，对于某个块，其可以拟合的函数为 $F(x)$，如果期望的潜在映射为 $H(x)$，与其让 $F(x)$ 直接学习潜在的映射，不如去学习残差 $H(x)-x$，即 $F(x)=H(x)-x$，这样原本的前向路径上就变成了 $F(x)+x$，用 $F(x)+x$ 来拟合 $H(x)$。这样更易于优化，因为相比于 $F(x)$ 学习恒等映射，让 $F(x)$ 学习成为 $0$ 要更加容易（可以通过 L2 正则轻松实现）。这样，对于冗余的块，只需 $F(x)\\rightarrow0$ 就可以得到恒等映射，性能不减 我们理解一下上面这段话 首先什么是恒等映射？ 如果我们在浅层网络上堆叠新的网络层，而这些增加的层什么也不学习，仅仅是复制浅层网络的特征，那么这样新增加层就是恒等映射。当残差为 0 时，堆积层就仅仅做了恒等映射。在这种情况下，我们猜想如此堆叠出来的深度网络应该至少和浅层网络的性能一样，不会出现退化现象。事实上，什么也不学习是神经网络最难做到的东西之一，残差不会为 0，这也就使得堆积层再说输入特征上学习到乐新的特征，从而拥有更好的性能 接着，怎么做残差学习？ 想要让神经网络去拟合潜在的恒等映射函数 $H(x)=x$，这是很难的。所以，我们只要让 $F(x)=0$，就可以构成一个恒等映射 $H(x)=x$。于是，我们的出发点就变成了 $F(x)\\rightarrow0$ 最后，我们做一个简单数学分析解释为什么残差更容易学习？ 设 $xl$ 和 $x{l+1}$ 分别表示第 $l$ 个残差单元的输入和输出，$F$ 表示残差函数，表示学习到的残差，$h(x_l)=x_l$ 表示恒等映射，$f$ 表示 ReLU 激活，于是有下面的关系式 y_l=h(x_l)+F(x_l, W_l) \\\\ x_{l+1} = f(y_l) 从浅层 $l$ 到深层 $L$ 的学习特征为 x_L=x_l+\\sum^{L-1}_{i=l}F(x_i, W_i) 根据链式求导法则，\\frac{\\partial loss}{\\partial x_l}=\\frac{\\partial loss}{\\partial x_L}\\cdot\\frac{\\partial L}{\\partial x_l}=\\frac{\\partial loss}{\\partial x_L}\\cdot\\left ( 1+\\frac{\\partial }{\\partial x_l}\\sum^{L-1}_{i=l}F(x_i, W_i) \\right ) 右式的第一项 $\\frac{\\partial loss}{\\partial x_L}$ 表示损失到达 $L$ 的梯度，括号中 $1$ 表示恒等映射，括号中的另外一项残差梯度则需要经过带有权重的层 清楚理解了残差学习之后，我们要考虑的就是如何设计 $F(x)+x$ 残差块 一个残差块有两条路径 $F(x)$ 和 $x$，$F(x)$ 路径拟合残差，$x$ 路径为恒等映射，称为 shortcut，$\\bigoplus$ 表示元素积，要求 $F(x)$ 与 $x$ 的尺寸相同 我们用书数学公式描述一下残差块，假设残差块的输入为 $x$，输出为 $y$，有 y=F(x,\\left \\{ W_i \\right \\})+x 其中经过一个激活函数，F=W_2\\cdot\\sigma(W_1x) 另外，考虑对维度保持一致，可以在 shortcut 做一个线性变化 $W_s$，此时残差块的表示为 y=F(x,\\left \\{ W_i \\right \\})+W_sx 值得一提的是，残差块中至少要有两个权重层，否则会出现下面的情况，起不到任何作用 y=F(x,\\left \\{ W_i \\right \\})+x=W_1x+x=(W_1 + 1)x 在论文中，残差路径可以分为两种，一种有 bottleneck 的结构，即下图右中的 $1\\times1$ 卷积层用于先降维再升维，主要目的是降低计算的复杂度。另外一种是没有 bottleneck 的结构 因此 shortcut 路径大致也分为两种，取决于残差路径是否改变了特征图的数量和尺寸。一种是将 $x$ 原封不动地输出，另外一种则需要经过 $1\\times1$ 卷积来升维或者降维，主要目的是将输出与 $F(x)$ 路径的输出保持大小一致 网络结构 ResNet 是参考 VGG19，在其基础上进行修改，通过短路机制添加残差单元实现的 ResNet 直接使用步长为 2 的卷积做下采样，而不再在残差块之间引入 pooling 层 使用 global average pooling 代替了全连接层得到最终特征 每个卷积层之后都有 batch norm，图上并未标出这点 另外，ResNet 的一个重要设计原则是：当特征图大小降为一半的时候，特征图的数量增加一倍，如 conv3_1、conv4_1 和 conv5_1，这保证了网络层的复杂性。上图中虚线表示特征图数量发生了变化，shortcut 中通过 $1\\times1$ 卷积保持大小的一致性 对于 18 层 ResNet 或者 34 层 ResNet 更多使用两层的残差块，当网络更深时，则更多使用 bottleneck 的残差块 error surface 对比 ResNet-20（no short）的 error surface 还没有很复杂，优化不会十分困难；但是在增加到 56 层后，error surface 变得异常复杂，此时将很难做到很好的优化 引入 shortcut 后，error surface 变得平缓很多，梯度的可预测性变得很好，更易于优化 残差块的分析与改进 在 Identity Mappings in Deep Residual Networks 一文中进一步研究了 ResNet ，通过 ResNet 反向传播的理论分析调整了残差块的结构 和之前不同的是，在这片文章中将 shortcut 视为主干路径，而将残差路径视为旁路 新的残差块具有更强的泛化能力，也更能避免退化问题，即使在堆叠大于 1000 层之后，网络的性能仍在变好。具体的变化在于， 通过保持 shortcut 路径的纯净，可以让信息在前向传播和反向传播中更加平滑传递。为此，非必要情况下将不在 shortcut 引入 $1\\times1$ 卷积操作，同时将上图灰色路径上的 ReLU 移到了残差路径上 在残差路径上，将 BN 和 ReLU 同一放在权重前作为预激活 "},"【论文】ROSITA.html":{"url":"【论文】ROSITA.html","title":"ROSITA","keywords":"","body":" 【论文】Cui Y, Yu Z, Wang C, et al. ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross-and Intra-modal Knowledge Integration.（pdf） What is ROSITA ROSITA 提出一种新颖的基于模态内和模态间知识协同引导的多模态预训练方法，其核心动机在于如何增强多模态预训练过程中的跨模态细粒度语义对齐，作者给出的解决思路是 『抑制模态内上下文的干扰，同时消除模态间潜在的噪声干扰』 如果只看这一段话，很难去理解 ROSITA 的工作，但是这段话却又是关键所在，不得不放在最前面。接下来，我们先看看一些和 ROSITA 类似的工作，一步步理解什么叫做模态内/间知识，再理解作者为什么提出这样的解决思路 与 ROSITA 类似的，前面值得关注的工作有 OSCAR 和 ERNIE-ViL， OSCAR 引入图像区域的预测标签作为锚点（anchor point）去隐式地和文本单词对齐 而 ERNIE-ViL 则显示地从文本构建一副场景图，然后在预训练目标中对场景图中的一些 keywords 进行强调 容易明白，所谓的模态内知识其实就是一种 object-2-object relationship（or word-2-word relationship）。与 ROSITA 不同的是，OSCAR 和 ERNIE-ViL 都只是通过 intra-modal knowledge 增强两个模态的语义的对齐 OSCAR models the intra-modal knowledge in the image modality while ERNIE-ViL models the intra-modal knowledge in the text modality 于是，自然而然地就会想到一个问题：是否我们可以将两个模态的 intral-modal knowledge 都同时使用起来呢，最好还能结合上 cross-modal knowledge？ 那什么又是模态间知识呢？如 Fig 1. 所示，ROSITA 构建的场景图会包含一些叫做 anchor objects 的内容（可以简单理解为句子或图像中出现的一些显著元素），每个 anchor object 构建了一组模态间/内的知识关系， intra-modal knowledge 指 anchor object 在自己模态内的上下文关系，也就是上面提到的 object-2-object relationship or word-2-word relationship（如蓝色连接线所示） 如粉色连接线所示，anchor object 会在另外一个模态中有一个语义相似的内容，而这样的模态间对应关系就称为 cross-modal knowledge（如，图像区域的 grass 对应文本中描述中的单词 steppe） 如此 ，模态间/内知识的概念就清楚了。那么，如何利用这张场景图实现模态内干扰的抑制和模态间干扰的消除呢？作者提出了一个叫 structural knowledge masking（SKM）的策略使用场景图作为先验知识来完成 MLR（or MRM）预训练任务。SKM 策略下的遮罩不再是根据一个固定的数值随机遮罩，而是依据场景图每条连接边上的权值对不同的节点计算不同的遮罩概率，这个权值衡量图中两个节点的相似度。对于模态内权重较大的边，则意味着连接的两个节点比较相似，如果遮掉边上的一个节点，那么另外一个节点被遮掉的概率也会很大；同理，对于模态间权重较大的边，那么如果遮掉一个节点，那么另外一个节点被遮掉的可能性就应该跟小。这样就实现了模态内干扰的抑制和模态间干扰的消除 那作者提出『抑制模态内上下文的干扰，同时消除模态间潜在的噪声干扰』这样解决方案的依据是什么呢？ 例如 sky 这个词，往往成对出现的另外一个词是 blue，如果只是遮掉 sky 而不遮掉 blue 的话，模型就可以仅通过文本上下文的关系就预测出被遮掉的词是 sky，这样 VLP model 就并没有去学习两个模态之间的联系 同样，模态间相似度很小的两个内容其实并不相关，也不需要学习去怎么对齐，所以可以通过先验知识去减少这种噪声的影响 其实上面两点依据很容易想到，如果你长期在 VLP 领域工作的话。但是，用什么方法实现就是 ROSITA 工作比较亮眼的地方了。到这结束，我想你已经理解了 ROSITA，下面我们关注一下它具体的工作 Knowledge Extraction Unified Scene Graph Construction 记 unified scene graph 为 $G=$，$V$ 表示 region or word 节点，$E$ 表示边集，$S$ 表示边连接两个节点的相似度（简单理解就是边的权重） 首先，分别构建图像场景图和文本场景图， 图像场景图选择两个检测区域的 IoU 作为它们的相似度，记入 $S$ 文本场景图的生成则直接看成是一个黑盒，采用现成的模型，将边连接两个节点在数据集中共同出现的频率作为相似度（主要是一些 object-attribute or object-relation pairs） 接下来要做的事情就是如何构建模态间的知识关系，考虑到没有 region-2-word 这样的 alignment supervision，作者创建了一种 pseudo semantic alignments between region-word pairs：通过 Glove 对齐 region tags 和 semantic similar object words，并计算两者之间的一个相似度。对于相似度得分超过 0.5 的 region-word piar 就构建一对边连接，记入 $E$ 和 $S$ Knowledge Representation 首先，从场景图中选出所有的 anchor objects，一般是一些存在模态间关系的节点，可以使 region 也可以是 word，attribute words 和 relation words 不会被选做 anchor objects 选出所有的 anchor objects 后，就可以得到 knowledge entries 了：给定 anchor object $v\\in V$，对应的 knowledge entry 就是场景图的一个子图 $g(v)\\subseteq G$，定义如下 \r g(v)=G_{cross}(v)\\cup G_{intra}(v)\\cup G_{intra}(G_{cross}(v))\r the ROSITA Framework Image and Text Feature Representations 记 $i$-th region 的视觉特征为 $f_i\\in\\mathbb R^{2048}$，位置特征为 $p_i\\in\\mathbb R^5$，然后采用两个线性映射将视觉特征和位置特征结合在一起形成一个 $d$-dimensional image representation $x_i$ \r x_i=W^T_ff_i+W^T_pp_i\r 对于每一个单词 $w_i$，其绝对位置为 $i$，$d$-dimensional position-aware text representation $y_i$ 定义为 \r y_i=WordEmbed(w_i)+IdxEmbed(i)\r Network Architecture 这一块的内容基本同 UNITER 就不再做详细描述 Structural Knowledge Masking 如前所述，为充分结合 unified scene graph，ROSITA 采用了 SKM 策略来选择性地遮掉 knowledge entry 中的一些 tokens，对应 VLP 中常见的预训练代理任务 MLM 和 MRM，在 ROSITA 中有 SKMLM 和 SKMRM 先说明一下下面会出现的数学符号，记 $\\mathcal R=\\left{r_1,\\cdots,r_m\\right}$ 为 regions set，$\\mathcal W=\\left{w_1,\\cdots,w_n\\right}$ 为 words sequence，$g(v_i)=$ 为 anchor object $v_i$ 的 knowledge entry， 其中 $\\hat V=\\left{v_1,\\cdots,v_N\\right}$，$\\hat S\\in\\mathbb R^{N\\times N}$ 对每一个 knowledge entry，SKM 确定将 anchor object 遮掉，选择性地遮掉其 intra-/cross-model contexts，这里的选择性并不是采用一个统一的概率（如 BERT 的 MLM 以 15% 的概率遮掉 word token），而是根据相似度以不同的概率进行遮罩——对于 intra-modal contexts，越大的相似度意味着越高的遮罩概率；对于 cross-modal contexts，越大的相似度意味着越小的遮罩概率。这也就是文章最开头说的 『抑制模态内上下文的干扰，同时消除模态间潜在的噪声干扰』 另外，有一点问题需要解决，knowledge entry 中并不所有节点都与 anchor object 连接，那么这部分节点的遮罩概率怎么计算呢？作者采用了转移概率 $T=[t_1,\\cdots,t_N]\\in [0,1]^N$，计算公式如下描述 \r T=\\frac{1}{2}\\hat S\\pi(i)+\\frac{1}{2}\\hat S\\hat S\\pi(i)\r $\\pi_i\\in\\left{0, 1\\right}^N$ 是一个 one-hot vector（$i$-th element = $1$），公式中的两项分别对应 $v_i$ 的 one-hop and two-hop contexts transmission probabilities 之后，按照下面的公式将转移概率 $T$ 转换为遮罩概率 $P=[p_1,\\cdots,p_N]$，其中 $\\alpha$ 是一个权重因子，用于平衡模态间/内的遮罩概率 \r p_j=\\left\\{\\begin{array}{l}\r 1,& \\textrm{if}\\ v_j\\ \\textrm{is\\ the\\ anchor\\ object}\\\\ \r \\alpha t_j,& \\textrm{if}\\ v_j\\ \\textrm{is\\ within\\ the\\ intra-model\\ contexts}\\\\ \r (1-\\alpha)(1-t_j), & \\textrm{if}\\ v_j\\ \\textrm{is\\ within\\ the\\ cross-model\\ contexts}\r \\end{array}\\right.\r 到这里我们就介绍完了 SKM 策略 下面我们看一下 SKMLM 和 SKMRM，记 $M_w$ 和 $M_r$ 分别表示 mask indices of the words and regions SKMLM，$vi$ 是一个 object word，损失函数记为 $\\mathcal L{SKMLM}(\\theta)=-\\mathbb E{(\\mathcal W,\\mathcal R)\\sim D}\\log P{\\theta}(\\mathcal W{M_w}|\\mathcal W{\\backslash Mw},\\mathcal R{\\backslash M_r})$ SKMRM，$vi$ 是一个 region，损失函数记为 $\\mathcal L{SKMRM}(\\theta)=-\\mathbb E{(\\mathcal W,\\mathcal R)\\sim D}f{\\theta}(\\mathcal R{M_r}|\\mathcal W{\\backslash Mw},\\mathcal R{\\backslash Mr})$，其中 $f{\\theta}(\\cdot)$ 表示联合的 regression-based loss 和 classification-based loss 除了 SKMLM 和 SKMRM 外，ROSITA 还是保留了传统的 MLM、MRM 和 ITM 三个任务，保留 MLM 和 MRM 的原因是 SKMLM 和 SKMRM 都只集中在对 key tokens 的处理上，而传统的 MLM 和 MRM 能够保证其他 tokens 也被照顾到 Experiments "},"【论文】SimMIM.html":{"url":"【论文】SimMIM.html","title":"SimMIM","keywords":"","body":" 【论文】Xie Z, Zhang Z, Cao Y, et al. SimMIM: A Simple Framework for Masked Image Modeling.（pdf） 像是 NLP 中的 masked language modeling task 或者 CV 中的 masked image modeling task 都可以统一称为 masked signal modeling，意思是遮掉一部分的输入信号，尝试去预测这些被遮掉的内容 但是为什么在 NLP 领域能比较轻松的就通过 MLM 实现自监督学习，而 CV 中却很难有很好的表现呢？作者给出了下面三条归因： 图像有很强的局部联系性，每个像素都和它周围的像素有很高的联系性，有时并不需要语义推理，而是通过简单复制周围的像素就可以预测出缺失的部分 视觉里面的信号很原始且低级（raw and low-level），而 text tokens 往往具有比较高级的概念。我们不禁会想：是否这些低级的信息能被用于高级的视觉识别任务呢？ 视觉信息是连续的，而 text tokens 是离散的。不确定基于分类的 MLM 方法是否能很好的解决连续信号的问题 "},"【论文】simVLM.html":{"url":"【论文】simVLM.html","title":"simVLM","keywords":"","body":" 【论文】Wang Z, Yu J, Yu A W, et al. SimVLM: Simple visual language model pretraining with weak supervision.（pdf） What is SimVLM 简而言之，SimVLM 就是嫌弃之前的 VLP 工作预训练过于复杂，认为这种复杂化给模型的性能造成了瓶颈。于是，SimVLM 寻思着做下面三件事情： 可以无缝地插入到 pretraining-finetuning 中 丢弃复杂的预训练目标 在跨模态的设置中可以实现 text guided zero-shot generalization 最终，形成的 SimVLM 模型不仅简单（既不需要目标检预训练，也不需要辅助损失），而且还获得了比以往工作更好的性能。在实验结果上看，SimVLM 优于现有的 VLP 模型，并在 6 个 VL benchmarks 上实现了 SOTA 性能，而无需额外的数据或任务特定的设置。此外，它在视觉语言理解中获得了更强的泛化性能，支持 zero-shot image captioning 和 open-ended VQA SimVLM 的工作表明，通过一个简单的预训练框架，模型可以获得强大的视觉语言理解能力 那么，这样一个强大的模型是如何实现的呢？大概来说，SimVLM 分为下面几个部分： Objective：使用 Prefix Language Modeling（PrefixLM）作为唯一训练目标进行端到端训练，这样 SimVLM 不仅可以像 GPT-3 一样自然地生成文本，也可以像 BERT 那样以双向的方式处理上下文信息 Architecture：SimVLM 采用 ViT 的结构，直接将原始图片作为输入，这样的模型适合大规模的数据训练，也很容易与 PrefixLM 相结合 Data：这些设置减轻了对检测检测的需求，允许模型利用大规模的弱标记数据集，这对 zero-shot generalization 来说有更好的效果 SimVLM background bidirectional Masked Language Modeling（MLM）一直是文本表示学习中最流行的自监督训练目标函数之一。如 BERT 证明的那样，它基于 denoising autoencoder 的想法，训练模型以恢复文本中损坏的 token 具体来说，给定一个文本序列 $\\mathbf x$，随机采样一个 token子集 $\\mathbf xm$，将其中的 token 替换为 [MASK] 以此构建一个受损序列 $\\mathbf x{\\backslash m}$，训练目标是通过最小化 negative log-likelihood 从 $\\mathbf x_{\\backslash m}$ 重建 $\\mathbf x_m$ 在 VLP 中，输入图文对通过视觉特征来预测被遮掉的词 除此之外，还有 unidirectional Language Modeling（LM），最大化自回归分解下序列 $\\mathbf x$ 的可能性 与 MLM 相比，LM 预训练也被证明对多个 NLP 任务是非常有效的。更重要的是，它使模型具有强大的生成和泛化能力 proposed objective: prefix language modeling 带有 LM 损失的预训练能够引入较强的 zero-shot 能力，受此启发，作者提出 PrefixLM 在 pretrain 获得 vision-language representation PrefixLM 不同于 LM 的地方在于：它允许对前缀序列（$\\mathbf x{{\\geq T_p}$）进行 autoregressive factorization 在预训练过程中，从输入序列中截断一个长度为 $T_p$ 的 tokens sequence 作为前缀序列（$T_P$ 可以随机选择），按照如上数学符号，训练目标可以表示为： SimVLM 将图像视为文本描述的前缀：对输入的一个图文对，将长度为 $T_i$ 的 image feature 放在文本序列前，然后让模型采样一个长度为 $T_p$（$\\geq T_i$）的前缀序列，即该前缀包括完整的 image patches 和一部分的文本描述，接着输入到 transformer 的 encoder 中。在 decoder 中，对文本数据做 LM 预测描述剩下的部分是什么。SimVLM 根据公式 （3）一个词一个词的计算 loss 这样，PrefixLM 不仅可以像 MLM 那样可以具有双向注意的上下文 representation，同时也可以像 LM 一样 perform text generation architecture 理解了 PrefixLM 这一部分就没啥好说的了，基本都是一些关于 ViT 和 transformer 的架构细节 简单来说，SimVLM 在用于机器翻译的 transformer 上通过 ViT 的思想加入了视觉信息，利用 decoder 的自回归能力根据前缀的提示生成想要的文本描述（如下图所示） datasets 作者使用 1.8b 的噪声图像-文本对 ALIGN 训练集1进行预训练，这使模型具有更好的 zero-shot 泛化能力。由于 PrefixLM 训练目标的模态无关性，作者另外还采用了一个只包含文本的语料库（Colossal Clean Crawled Corpus，C4 数据集）以补偿文本数据中的噪声影响 与之前的两阶段和多个辅助目标组成的 VLP 方法相比，SimVLM 只需一个 PrefixLM loss 就可以进行端到端的预训练，大大减轻了工作的复杂程度 Experiments fine-tunning Visual question answering 使用 VQA v2 数据集进行微调。将原始图像和相应的问题分别作为编码器和解码器的输入，然后训练一个特定于任务的线性分类器来预测答案，分类器作用于 decoder 输出的 last question token（大概是 Fig 2. 中 \\ 对用的 token） Visual entailment 使用 SNLI-VE 数据集进行微调。微调方法类似于VQA，将图像和句子分别输入编码器和解码器，然后训练分类器来预测三种关系 Visual reasoning NLVR2 数据集给出一个描述和两张图片，询问描述和两张图片是否分别匹配。在 SimVLM 中，作者创建两个输入对，每个输入对由一个图像和文本描述组成，并使用上面相同的设置为两者生成输出embedding。然后将这两个 embedding 连接起来进行最终的预测 Image captioning 使用 CoCo 和 NoCaps 数据集进行微调。Image captioning 需要模型来生成输入图像的自然语言描述。对于 SimVLM，首先可以直接在编码器中编码图像，然后使用解码器生成字幕 Multimodal translation Multimodal translation 的目标是将源语言中的图像描述转换为目标语言，因此可以利用图像输入作为 grounding 信号。作者用 PrefixLM，将源句和图像输入给编码器，编码器的结果将被解码器翻译成目标语言 下面是 SimVLM 在各任务上和一些 SOTA 方法的比较 Reference 博客引用 谷歌发布最新零样本学习看图说话模型，多类型任务直接上手 《SimVLM》拒绝各种花里胡哨！CMU&Google提出弱监督极简VLP模型，在多个多模态任务上性能SOTA 论文引用 1:Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021. "},"【论文】Tip-Adapter.html":{"url":"【论文】Tip-Adapter.html","title":"Tip-Adapter","keywords":"","body":" 【论文】Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling（pdf） What is Tip-Adapter 作者认为 CLIP-Adapter 引入的 residual feature adapter 增加了额外的训练，为了保留 CLIP training-free 的特性，作者提出了 Tip-Adapter（Training-Free CLIP-Adapter），希望在 CLIP-Adapter 的基础上预训练完之后不再需要任何的训练就可以进行 few-shot learning 因此，Tip-Adapter 采用了一个 key-value cache model 来产生 adapter 的权重，这样就可以避免了通过反向传播训练 adapter，如下图所示 作者发现，Tip-Adapter 这样做不仅能够达到和 fully-tuned CLIP Adapter 同样的表现，而且如果不冻住 adapter 的第一层线性层的话 Tip-Adapter 也只需要 20 个 epochs 就可以完成 few-shot learning，具有很快的收敛特性，比 CLIP-Adapter 的 200 个 epochs 快出了很多很多 Training-free CLIP-Adapter Tip-Adapter 采用 CLIP-Adapter 的结构，详细的关于模型结构的内容不再做详细的介绍，主要关注 cache model 的构建 给定一个 $K$-shot $N$-class 的 few-shot training set，图片记为 $IK$，标签记为 $L_N$，将 $I_K$ 经过 CLIP image encoder 得到的视觉特征记为 $\\mathbf F{train}\\in\\mathbb R^{NK\\times C}$，其中 $C$ 表示一张图片经过 image encoder 得到的特征的维度，将 $LN$ 经过 one-hot 编码的结果记为 $\\mathbf L{train}\\in\\mathbb R^{NK\\times N}$，其中一个标签经过 one-hot 编码的结果为一个 $N$-dimensional vector \r \\mathbf F_{train}=\\text{VisualEncoder}(I_K)\\\\\r \\mathbf L_{train}=\\text{OneHot}(L_N)\r 接着，将 $\\mathbf F{train}$ 视为 cache model 的 keys，$\\mathbf L{train}$ 视为 values，由此就构成了 Tip-Adapter 的 key-value cache model，存储从 few-shot training set 得到的额外知识。将这些额外的知识转化为 adapter 的权重就可以按照 residual-style blending 的方式更新 CLIP 的 prior knowledge，从而实现 training-free 的 few-shot learning 对于一张测试图片的视觉特征 $f{test}$，将其视为 cache model 的 query，按照如下公式计算 query 和 keys 的匹配程度 $A\\in\\mathbb R^{1\\times NK}$ \r A=\\exp(-\\beta(1-f_{test}\\mathbf F_{train}^T))\r $\\beta$ 是一个超参数。通过 $A\\mathbf L{train}$ 可以检索到对应的 value 于是，最终 Tip-Adapter 的 predicted logits 可以描述为 \r \\begin{align}\r \\text{logits} &=\\alpha A\\mathbf L_{train}+f_{test}W_c^T\\\\\r &=\\alpha\\varphi(f_{test}\\mathbf F_{train}^T)\\mathbf L_{train}+f_{test}W_c^T\r \\end{align}\r $W_c$ 是 CLIP text encoder 输出的分类权重，$\\alpha$ 是同 CLIP-Adapter 中的 residual ratio，$\\varphi(x)=exp(-\\beta(1-x))$ 通过这个公式，我们很清楚 Tip-Adapter 做预测的依据来自两部分，一部分是 CLIP 知识给出的对测试图片的预测，一部分是 adapter 给出的 cache model 中匹配的标签。在 $\\text{logits}$ 的展开公式中，我们也可以很清楚的看到 $\\alpha$ 作为平衡因子，如果 domain gap 很大的话我们会选择较大的 $\\alpha$ 值以获取更多的 few-shot knowledge 这里，我们回过头来看一下为什么作者将 cache model 的输出称为 adapter 的权重呢？如果熟悉 CLIP-Adapter 的话，不难理解下面这个公式 \r \\text{logits} =\\alpha\\cdot\\left[ \\varphi(f_cW^T_1 + b_1)W^T_2 + b_2\\right]\\cdot W_c^T+f_cW_c^T\r $\\varphi(f_cW^T_1 + b_1)W^T_2 + b_2$ 指 adapter 学到的特征（$\\varphi$ 是两层 MLP 的激活函数），$f_c$ 是 CLIP image encoder 输出的特征 对比 Tip-Adapter 的公式，可以发现 CLIP-Apdater 和 Tip-Adapter 之间的一些联系 \r \\begin{align}\r & W_1 =\\mathbf F_{train},\\ W_2 =\\mathbf L^T_{train},\\ b_1 = 0,\\ b_2 = 0 \\\\\r &\\varphi(x) = \\exp(−β(1 −x)),\\ \\text{where} x \\in[0,1]\r \\end{align}\r Tip-Adapter 采用了 cached weight，从而避免 SGD 训练，实现了 non-parametric 和 training-free CLIP-Adapter 使用了一个低维的 MLP 来 fine-tune 目的是避免对 few-shot 数据集的过拟合，同样 Tip-Adapter 选用 $NK$ 容量的 cache model 不仅能够减轻过拟合，还能获得和高维线性层相同的感知能力 $\\exp(−β(1 −x))$ 在 Tip-Adapter 中可以视为一种新的激活函数，由于输入的 $x$ 是标准化特征空间中的距离，因此输出限制在 $[0,1]$ 中，这样的激活使得 Tip-Adapter 有更好的性能 最后，作者还给出了一种 fine-tune Tip-Adapter 的方法，将 $W2=\\mathbf L{train}$ 和两个 encoder freeze 住，通过 SGD 更新 $W1$，理由是 $\\mathbf L{train}$ 中 cache 的 ground truth annotations 作为一种 few-shot knowledge 应该被保留，以帮助模型更好的完成 few-shot learning，训练 $W_1$ 则可以更好的匹配测试图片和 few-shot training set Experiments "},"【论文】Tips and Tricks for Visual Question Answering_ Learnings from the 2017 Challenge.html":{"url":"【论文】Tips and Tricks for Visual Question Answering_ Learnings from the 2017 Challenge.html","title":"Tips and Tricks for Visual Question Answering_ Learnings from the 2017 Challenge","keywords":"","body":"﻿* 【论文】eney, Damien, Peter Anderson, Xiaodong He, and Anton Van Den Hengel. Tips and tricks for visual question answering: Learnings from the 2017 challenge.（pdf） 主要思想 论文提出一些细节上优化提升 VQA 的方法，主要包括如下： sigmoid output：在结果预测时，允许有多个答案，对每个答案采用 sigmoid 方法预测 use soft scores as ground truth targets：预测时采用回归预测，预测概率，而不是传统的分类 gated tanh activations：激活函数采用 tanh image features from bottom-up attention：图像特征提取采用目标检测的方法 pretrained representations of candidate answers：初始化输出层的权重 large mini-batches and samrt shuffling：训练过程中使用 large mini-batch 和混洗的方式 模型架构 模型前面采用了常见的 joint embedding of input question and image，后半部分对候选答案使用了一个多标签分类器 Question embedding 使用 GloVe 提取问题语义特征，整个数据集中只有 0.25% 问题超过了 14 个单词，因此文章考虑一个问题最多包含 14 个单词，超出的全部丢弃。使用 Wikipedia / Gigaword 语料库上预训练好的 GloVe （Global Vectos for Word Representation）来进行词特征的向量化，每个单词背转化为一个 300 维的向量。对于少于 14 个单词的语句，采用 end-padded 补齐。于是，一个问题就可以被表述为 14×300 的矩阵，送入 GRU（Recurrent Gated Unit），GRU 的 hidden_size = 512，我们提取最有一个状态作为我们的问题语义向量 $q$ Image features 视觉特征的注意力模型 bottom-up attention 是指通过 faster-rcnn 的检测区域提取不同类别物体的特征，这种方式也可以叫做给予显著性的 attention 机制。图像通过 faster-rcnn 后，获得一个 $K\\times2048$ 的矩阵，$K$ 代表图片有多少个位置，每个位置由一个 $2048$ 维的向量表示，这个向量是 $K$ 对应区域的图像特征编码，$K$ 是从 faster-rcnn 的 top-K 中确定的 实验中作者既选择了一个固定值 $K=36$，也设定 $K$ 也可以是一个自适应的数值，允许 $K$ 随图像的复杂度而变化，最大上限是 100，这种情况下 VQA v2 产生的 $K$ 的均值是每张图片 60 个区域 这种注意力机制不受训练影响，应该属于 hard attention。faster-rcnn 在 visual genome 数据上训练，同时加入更多标签，使 faster-rcnn 能够提取到更精细的区域与特征，最后实验证明，bottom-up attention 能够使模型性能提升 6% Image attention 文中的模型采用 question-guided attention（作者将这一步成为 top-down attention，区别于 bottom-up attention）机制，该机制通过问题的语义特征指导视觉特征 对图片上的每个位置 $i=1,\\cdots,K$，图像特征 $vi$ 和问题语义特征 $q$ 结合在一起，共同经过一层非线性映射 $f_a$，然后再经过一个线性层得到一个标量注意力权重 $\\alpha{i, t}$ a_i=w_af_a([v_i, q])\\\\ \\alpha = softmax(a)\\\\ \\hat v=\\sum_{i=1}^K\\alpha_iv_i其中，$w_a$ 实现学习参数的向量。所有位置的注意力权重经过 softmax 函数进行归一化。然后，所有位置的图像特征以归一化的注意里权重为权值，然后加和得到一个大小为 2048 的向量 $\\hat v$ 注意到这种注意力机制是 one-glimpse attention，区别于其他复杂的注意力机制（stacked、multi-head 以及 bidirectional attention） Multimodal fusion 多模态特征融合 joint embedding 采用对应位置相乘的方式（hadamard product），问题语义特征 $q$ 和图像特征 $\\hat v$ 经过非线性层处理后通过 Hadamard product 结合起来 h=f_q(q)\\circ f_v(\\hat v) Output classifier 候选答案集合在文中被称为输出词汇表，从训练集中出现 8 次以上的所有正确答案中预先选出来，最终得到一个 $N=3129$ 的候选答案集。作者将 VQA 是做一个多标签分类问题。VQA v2 中每一个训练的问题都有一或多个回答，每一回答对应 $[0,1]$ 中的一个 soft accuracy。之所以会出现多种回答和 $[0,1]$ 准确率，是因为人们在标注时的分歧，尤其是模棱两可的问题或者相同意思不同表述的回答。由于标注的模糊性，训练集有 7% 的问题无法从输出词汇表中给出正确答案。作者没有丢弃这部分问题，而发现了一个有意思的现象——这些模糊问题都会将输出词汇表中所有的候选词的预测得分推向零 输出分类器将 $h$ 输入到非线性变换层 $f_o$（由后面知道这个非线性变换层 $f_o$ 表示 gated tanh activations），然后在经过一个线性变换得到预测得分 $\\hat s$，可以描述为如下式子 \\hat s=\\sigma(w_o, f_o(h))其中 $w_o\\in\\mathbb R^{N\\times512}$ 是一个学习矩阵 sigmoid 函数将最终得分归一化到 $(0,1)$，损失函数设计如下所示，类似于交叉熵损失，只不过作者使用了的是 soft target scores L=-\\sum_{i=1}^M\\sum_{j=1}^Ns_{ij}log(\\hat s_{ij})-(1-s_{ij})log(1-\\hat s_{ij})其中 $M$ 表示训练问题的数量，$N$ 表示训练候选答案的数量，$s$ 表示 soft accuracy of ground truth answer 这个损失函数有两个优点： sigmoid 的输出使每个问题可以预测多个答案 使用 soft scores 作为 targets 可以比 binary targets 更利于训练，因为 soft scores 可以捕捉到在 ground truth 标注中出现的一些偶然不确定情况 Pretraining the classifier 训练过程中，学习矩阵 $w_o$ 的一行对应一个候选答案。为了使预测更准确，从两个方面使用候选答案的先验信息初始化 $w_o$ 的行： 从语言学的角度，作者利用了每个答案的 GloVet word embeddings 作为特征。如果出现答案不能和预先训练好的 embedding 精确匹配时，作者会在检查拼写、删除连字符号或从多单词表达中保留单个表达后使用最近匹配，得到的结果记为 $w_o^{text}$，大小为 300 从视觉的角度，利用 Google 搜索与候选答案相关的 10 张图片，然后送入预训练好的 ResNet-101 上提取特征，将 mean-pool 处理后的 10 个特征做平均，产生一个大小为 2048，记为 $w^{img}_o$ 有了预设计好的 $w_o^{text}$ 和 $w_o^{img}$，然后在训练集上使用小学习（0.5 和 0.01）率微调。最终，我们有如下的组合方式 \\hat s=\\sigma\\left(w_o^{text}f_o^{text}(h)+w_o^{img}f_o^{img}(h)\\right) 下图展示了引入 pretrained $w_o^{text}$ 和 $w_o^{img}$ 的结果 Non-linear layers 前面一直提到了一个非线性变换层，通常的做法是使用 FC + ReLU 但是文中作者受 LSTM 和 GRU 中门函数的启发，提出了下面的 gated hyperbolic tangent activatio，每一个非线性变换层 $f_a:x\\in\\mathbb R^m\\rightarrow y\\in\\mathbb R^n$ 定义为 \\tilde y=tanh(Wx+b)\\\\ g=\\sigma(W'x+b')\\\\ y=\\tilde y\\circ g 其中 $g$ 可以被视为作用在中间激活 $\\tilde y$ 上的一个门函数，用于控制信息的有效传递 "},"【论文】Transformer.html":{"url":"【论文】Transformer.html","title":"Transformer","keywords":"","body":" 【论文】Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. （pdf） Why is Transformer transformer 第一个次实现完全依赖于自我注意力机制来计算输入和输出表示的转导模型，完全摒弃了循环和卷积的网络结构 transfomer 的整体结构为 encoder-decoder structure Encoder 将输入序列 $(x_1,x_2,\\cdots,x_n)$ 映射为另一个连续序列表达式 $Z=(z_1, z_2,\\cdots,z_n)$，给定 $Z$，解码器 decoder 一次生成一个元素的符号的输出序列 $(y_1,y_2,\\cdots,y_m)$，模型的每一步都是自动回归的，在生成下一个输出时，会将先前生成的符号用作附加输入 下面是 Transformer 的一个整体框架 Encoder 由 6 个 identical layer 堆叠而成，每一个 identical layer 由 2 个 sub-layer 组成： 第一部分是 multi-head self-attention 第二部分是一个简单的全连接前馈神经网络 在每个 sub-layer 中加入了残差连接（目的是解决深度学习中的退化问题），并使用归一化，即每个 sub-layer 的输出为 $LayerNorm(x+Sublayer(x))$，所有 sub-layer 中向量的维度均为 $d_{model}=512$ decoder 也是由 6 个 identical layer 堆叠而成 第一部分是 multi-head self-attention mechanism 第二部分是 encoder-decoder attention layer 第三部分是一个简单的全连接前馈神经网络 不过，与 encoder 单层两个 sub-layer 不同的是，decoder 的单层中插入了一个 sub-layer：encoder-decoder attention，该层对 encoder 的输出进行 multi-head attention 机制处理。与 encoder 结构相似，每个 sub-layer 都进行了残差连接并归一化 下面是关于 Transformer 更细节上的完整框架 Transformer 输入处理 Transformer 的输入数据首先通过 Word2Vec 等词嵌入方法将输入语料转化成特征向量，论文中使用的词嵌入的维度为 $d_{model}=512$ 在底层的块中，$\\mathbf x$ 将直接作为 Transformer 的输入，而在其他层中，输入则是上一个块的输出，如下图所示 Multi-Head Attention 在介绍 multi-head attention 前，我们先简单的介绍一下一般的注意力机制 我们看个例子，我们把 the 对应的 embedding 作为查询向量 $q$，然后把句子中的所有词作为 $X$。我们先用 $q$ 和 $X$ 做一个内积，也就是计算一下相似度。然后，通过 softmax 进行归一化，就得到了一组注意力的权重 $w_1,\\cdots,w_5$。我们按照这组权重对所有的词进行加权，就得到 the 的上下文表示 我们可以发现这个句子并没有来自外部的查询向量 $q$，这个 $q$ 都是来源于句子本身的每个词，那么就相当于句子自己注意到自己身上，这种模式我们就称为 self-attention 在 transformer 的 self-attention 中每个单词有 3 个不同的向量，分别描述为 Query、Key 和 Value，长度均为 64。它们通过嵌入向量 $\\mathbf X$ 乘以三个不同的权值矩阵 $\\mathbf W^Q$、$\\mathbf W^K$、$\\mathbf W^V$ 得， 3 个权值矩阵的大小为 512 × 64 Scale Dot-Product Attention self-attention 的输出为 Value 的加权和，其中每个 Value 对应的权重由 $Q$ 和对应的 $K$ 经过一系列函数运算得到（如下图左边所示） 具体说来，self-attention 的计算方法分为 7 步：（第一、二步在下图左边没有体现) 将输入单词转换成嵌入向量 根据嵌入向量得到 $Q,K,V$ 三个向量 $Q,K$ 的维度为 $d_k$，$V$ 的维度为 $d_v$ 为输入句子中的每个单词评分，即计算一个 $score = QK^T$ $score$ 决定了我们在某一位置编码时需要把多少注意力放在输入句子的其他部分上。例如，下面在计算位置 #1 Thinking 的得分时，第一个得分通过 $q1\\cdot k1$ 得到，第二个得分通过 $q_2\\cdot k2$ 得到 为了梯度的稳定，对 $score$ 进行归一化 除以 $\\sqrt{d_k}$，得到的所有 socre 都是正数，且相加等于 1，可以防止点乘的值过大落入 softmax 上梯度很小的位置 对归一化后的结果使用 softmax 激活 softmax score 决定了每一个单词在该位置的表达量，显然，在这个位置上的单词将会获得最高的 softmax score，同时我们也可以由此观察其他单词和当前单词的相关程度 将激活的结果点乘上 $V$，得到加权的每个输入向量的评分 这一步的动机是保持我们想要关注的单词的完整值，而忽略不相关的单词 相加之后得到当前位置最终的输出结果 $\\mathbf z=\\sum V$（对应位置相加） 上面 7 步的过程可以总结为下图所示结果，这里以计算 Thinking 为例 实际上过程中的运算都是基于矩阵运算，也就是 整个注意力函数的表达式为 $Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ 形如 现在，我们对 Scale Dot-Product Attention 的结果过程有了一个比较清晰的了解，下面我们要进一步讨论的问题是：是否能直观解释一下 Query、Key、Value 的工作，我们为什么要提出这三者按照上面的操作进行计算？ 我们可以类比信息检索系统，举个例子，如果在某电商平台要搜索某件商品，那么我们需要在搜索引擎上输入的内容便是 Query（例如，红色薄款羽绒服），然后搜索引擎根据 Query 匹配 Key（即商品种类——羽绒服，颜色——红色，描述——薄款），然后根据 Query 和 Key 的相似度匹配得到搜索内容，即 Value Self-attention 和这个搜索过程是类似的，$Q$ 里面的每一行就是一个词，每一个词表示一个查询，而 $K$ 就像我们的 key-value 的数据库一样， $K$ 的每一行用来做索引，我们通过 $Q$ 和 $K$ 的点乘表示在一定程度上我们对当前输入句子的兴趣点在哪儿，然后再乘以 $V$ 找出我们关注的内容 Multi-Head Attention Multi-Head Attention 能让模型考虑到不同位置的 attention，另外 Multi-Head Attention 可以在不同的 representation subspace 表示不一样的关联关系，使用单个 head 的 attention 一般达不到这样的效果，下图以两个 head 为例 论文中使用 8 个并行的 head，这样子就会产生 8 个不同的输出，如何将它们压缩在一起呢？论文提出的方法如下所示 \r \\begin{align}\r &MultiHead(Q,K,V)=Concat(head1,\\cdots,head_h)W^O\\\\\r &where\\ head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)\r \\end{align}\r 完整的 Multi-Head Attention 运行如下： 将数据 $X$ 分别输入到 8 个 self-attention 中，得到 8 个加权后的特征矩阵 $Z_i,i=1\\cdots,8$ 将 8 个 $Z_i$ 按列拼成一个大的特征矩阵 特征矩阵经过一层全连接后得到输出 $Z$ Transformer 中以三种不同的方式使用 Multi-Head Attention： 在 encoder-decoder attention layer 中，Query 来自向前的 decoder 层，并且 Key 和 Value 来自 encoder 的输出，decoder 中的每个位置加入输入序列中的所有位置 在 encoder 的 self-attention layer 中，所有的 Key、Value 和 Query 都来自同一个地方—— encoder 中前一层的输出 类似的，在 decoder 的 self-attention layer 中，所有的 Key、Value 和 Query 都来自同一个地方，但是这里使用了可选的 Mask 操作 Position-wise 前馈神经网络 除了 self-attention layer，encoder 和 decoder 每层还包含一个全连接前馈神经网络，其中包含了两个带有 ReLU 作为激活函数的线性变换 \r FFN(z)=max(0, zW_1+b_1)W_2+b_2\r 两次线性变换在形式上是相同的，但层与层之间的参数不同（即 $W1$ 和 $W_2$ 是不同）。我们也可以把这个前馈理解成两层 $1\\times1$ 卷积，输入 $d{model}=512$，中间层 $d_{ff}=2048$ Final linear and softmax layer 解码器解码之后，解码的特征向量经过一个线性层，它将特征向量转投影到一个更大的向量中，这个更大的向量称为 logits vector 我们假设模型知道了 10k 个唯一的单词（论文中称为 output vocabulary），这 10k 个单词是从训练集学得的。于是，logits vecto 就有 10k 个单元那么宽——每一个单元对应唯一一个单词的评分 之后再经过 softmax 激活得到下一个 token 的预测概率，具有最高概率的单元会被选出来，与这个单元相对应的单词就作为这一时间步的输出 Positional encoding 由于模型中不包含循环和卷积结构，那么无论句子的结构怎么打乱，Transformer 都会得到类似的结果，因为这时的 Transformer 只是一个功能更强大的词袋模型 为了能够利用序列的顺序特征，论文在词向量编码时引入了位置编码特征，具体来说就是，位置编码会在序列中加入不同 tokens 之间相对或绝对位置的信息，这样 Transformer 就能够区分不同位置的单词了 那么怎么编码这个位置信息呢？ 常见的模式有：(1）根据数据学习；（2）自己设计编码规则。在这里作者采用了第二种方式 那么这个位置编码该是什么样子呢？ 通常位置编码是一个长度为 $d_{model}$ 的特征向量，这样便于和词向量进行单位加的操作 位置编码使用了不同频率的正弦和余弦函数 \r PE(pos,2i)=sin(\\frac{pos}{10000^{2i/d_{model}}})\\\\\r PE(pos,2i+1)=cos(\\frac{pos}{10000^{2i/d_{model}}})\r 之所以选择正弦和余弦函数作为位置编码，是因为对于任意固定的偏移量 $k$， $PE{pos+k}$ 都可以表示为 $PE{pos}$ 的线性函数（考虑 $sin(\\alpha+\\beta)= sin\\alpha cos\\beta+cos\\alpha sin\\beta$，$cos(\\alpha+\\beta)$ 同理）。论文也测试了使用预学习的 positional embeddings 作为位置编码，这两种方法测试结果相近，不过论文为了使模型能够推断出比训练期间遇到的序列更长的输入序列，选择了使用正弦函数作为位置编码 Transformer 完整的工作流程 我们现在来看一次完整的 Transformer 工作流程吧，作为最后的回顾 首先，encoder 处理输入序列，然后将顶层 encoder 的输出转换为一组注意力向量 $K$ 和 $V$ 输入到 decoder 的 Encoder-Decoder Attention layer，它们会帮助 decoder 专注于输入序列中的正确位置 下面即即是重复上面第一步，直到遇到一个特殊的符号表示解码器已经完成输出。每一步的输出在下一个时间步被送到 decoder 的底部当做输入，和 encoder 一样，我们会在输出中嵌入位置编码以指示每个单词的位置 我们现在来解释一个地方，在 Scale Dot-Product Attention 中有个可选的步骤 Mask 是用来干嘛的？ encdoer 和 decoder 的 self-attention layer 有一丢丢的不同，就在这里：decoder 只关注于输出序列中更前面的位置，这样我们就可以在计算 softmax 之前将后面的位置遮罩掉（如，设置为 -inf） Transformer 的训练过程 假设我们的输出词汇表只包含 6 个单词（a、am、i、thanks、student、\\ 表示 end of sentence），定义好输出词汇表我们就可以使用 6 个单元宽度的向量来表示词汇表中的每个单词，例如我们对 am 进行 one-hot encoding 接下来，我们就可以研究模型的损失函数了。我们得到一个输出概率分布，通过交叉熵损失或者 KL 散度可以计算输出概率分布和正确概率分布之间的差异 这仅是考虑如何计算一个单词的损失值。实际中，我们输入一个句子 je suis étudiant 输出 i am a student 需要计算连续的输出概率分布，于是有如下的一个过程 每个概率分布都是由一个宽度为 vocab_size 的向量表示 （我们的示例中 vocab_size = 6，实际中可能是 30k 后者 50k） 第一个概率分布在对应于单词 i 的单元格中概率最高 第二个概率分布在对应于单词 am 的单元格中概率最高 依此类推，直到第五个输出分布指示 \\ 符号 在足够大的训练集上训练充分后，我们会看到如下的一个结果 这里，我们仅考虑一次产生一个输出，所以模型所做的工作也就是从概率分布中选择概率最高的单元格对应的单词，摒弃其他所有的单元格，这是一种叫做 『greedy decoding』 的方法 还有一种方法叫做 『beams search』，我们假设保留前两个词 i 和 a，然后在下一步中运行模型两次：一次假设第一个输出位置对应的单词是 i，另一次假设第一个输出位置对应的单词是 a，然后哪一次的错误更小哪种输出方式就会被保留下来。然后对位置 #2 和 #3 重复类似的操作。这里我们取的 beam_size = 2，意味着在任何时间步上，有两个假设（未完成的翻译）保存在内存中，并且我们取的 top_beams 也为 2，表示我们将返回两个翻译 Reference The Illustrated Transformer 20210625；短教程：《Transformers》；特邀讲师：邱锡鹏教授 "},"【论文】TSGV 综述.html":{"url":"【论文】TSGV 综述.html","title":"TSGV 综述","keywords":"","body":" 【论文】Xiaohan Lan, Yitian Yuan, Xin Wang, Zhi Wang, and Wenwu Zhu. 2021. A Survey on Temporal Sentence Grounding in Videos. Temporal Sentence Grounding in Videos (TSGV) 在介绍 TSGV 之前，我们先介绍一个类似的视频任务——action detection（或者称为 temporal action grounding in videos，TAGV），该任务作为一个 video classification problem，检测一个视频片段将其分类为某一类 action 但是 TAGV 受限于定义的 action 种类，一种更自然的想法是用一段描述去表示某种 action，这就是 TSGV 要做的事情+——根据 query 找出预测帧的起始点和终止点，如下图所示，“ “A little girl walks by a little boy and continues to blow the leaves” 为 query，预测的起始点和终止点为 7.11s 和 12.7s TSGV 作为一种下游的 VL task 和 video question answering、video content retrieval 有着很多的联系，例如对于 video qa 可以先通过 TSGV 找出和 question 相关的帧片段，然后在仔细分析得到 answer；再比如可以提供一些简短的 video sentence summaries 定位语义连贯的视频片段组成原始视频的 visual summaries TSGV 的一些挑战和困难： 视频和描述都是在时间上连贯的，视频和描述对齐就非常复杂 对齐的目标片段可以十分灵活，candidate segments 的长度不同、定位不同，如果通过滑动窗口逐一和 query 进行匹配会消耗大量的计算资源，因此获得不同时粒度的 segments 来覆盖目标片段是一个非常大的挑战 视频中的动作往往来说在语义上是相关的、在时间域上是独立的，因此要通过 query 的提示去整合视频的上下文来准确定位不是一件容易的事情 已有的一些 TSGV 的工作可以分为下面几种： two-stage matching-based，将视频通过滑窗或者 proposal generation network 分为一些 candidate segments，然后通过跨模态交互模块 re-rank 选出得分最高的 candidate。这种 scan-and-localize pipeline 的问题就是比较费时，重叠的 candidate 会产生大量冗余计算，同时逐一匹配的方式割裂了视频的上下文信息 end-to-end manner，分为 anchor-based 和 anchor-free 两种方法，anchor-based 方法通过 LSTM 或 CNN 在每个时间点上产生一系列的 multi-taskscale candidates，而 anchor-free 方法则对每一个 video unit（frame-level or clip-level）预测其作为起始点和终止点的可能性，或者更直接地根据跨模态特征回归出起始点坐标和终止点坐标 RL-based，强化学习的方法将 sentence localization 视为是一个 sequential decision process weakly supervised setting Method Overview Two-stage method 第一阶段：在模型计算前采用一个 pre-segmenting 得到 proposal candidates，然后将 candidates 和 query sentence 进行跨模态交互计算得到 target segment localization 典型的如 MCN（2017）和 CTRL（2017）两篇先驱工作，定义了 TSGV 任务和基本的数据集。MCN 通过滑窗采样得到所有的 candidate segments，然后将 segment representation 和 query representation 投射到一个共同的嵌入空间中计算 $\\ell_2$ 距离。考虑 segments 可能来自同一视频（intra-video）也有可能来自不同视频（inter-video），MCN 采用了下面的一组 ranking loss，具体来说，对于样本 $i$，intra-video ranking loss 鼓励句子 $i$ 更接近目标 segment 的 location $\\tau^i$，远离同一视频中的其他 locations；inter-video ranking loss 则在鼓励的同时远离其他视频的同一 location 其中 $\\mathcal L^R(x, y)=max(0,x-y+b)$，$b$ 是 margin CTRL 首次将 RCNN 从目标检测用到了 TSGV 中。同样，CTRL 还是采用了滑窗来得到不同长度的候选片段，然后通过 add、multiply 和 fc 三个操作进行跨模态的交互，最后再通过一个 FC 计算 alignment score 和 location offsets。具体来说，CTRL 采用了下面一组 multi-task loss 来进行训练 $cs{i,j}$ 计算 segment $c_i$ 和 sentence $s_j$ 之间的 alignment score；$\\mathcal L{reg}$ 只在 align pairs 中做 location regression；$R$ 是 smooth-L1 function 之后的一些工作，例如 『Cross-modal Moment Localization in Videos』将 query 分解成几个部分可以根据视频上下文自适应地得到重要的 textual components 『Attentive Moment Retrieval in Videos』引入了一个叫做 ACRN（attentive cross-modal retrieval network）网络通过 query 引导注意力，将一部分权重放在 segment 的上下文上，以此来增加 segment representation 『Cross-modal video moment retrieval with spatial and language-temporal attention』提出的 SLTA 同样是一种 spatial and language-temporal 的注意力模型，通过 query 的引导注意到一些相关的 objects 和他们之间的联系 『Multi-modal Circulant Fusion for Video-to-Language and Backward』引入了一个叫做 MCF（ multi-modal circulant fusion）的模态融合机制来替代 CTRL 中的三个简单操作 『Mac: Mining activity concepts for language-based temporal localization』则通过视频和文本挖掘出一些 activity concepts 作为先验知识，计算 actionness score 来判断 candidate 对动作的置信度，以此提高定位的准确性 但是，基于滑窗的方法存在着大量的计算冗余，因此效率十分低下 一些方法尝试减少 proposal candidates，我们将这类方法称为 proposal-generated method，但是其任然属于 two-stage QSPN（2019）有下面两个部分和一个结合 retrieval task 和 captioning task 的 multi-tasking loss 组成 query-guided SPN 首先将 query 嵌入到 video features 中以此得到每个 temporal location 的注意权重，然后再将得到的注意力权重和卷积得到的特征结合起来，由此可以得到query-aware 的 candidate segment representatio你，接着将 SPN 中得到的特征整合到（b）中 LSTM 的第二层 前面提到 QSPN 有两个任务的 loss，对于检索，采用的是一个 triplet-based loss 其中，$(S, R)$ 表示 positive (sentence, segment) pair，$R'$ 表示负样本的 segment 补充的 captioning task 用于根据检索到的视频 re-generate query 同样地，在 SAP 这篇工作中作者也将 query 的语义信息整合到 activity proposals 的生成中——把从 query 和 视频帧得到的 visual concept 用于计算 visual-semantic correlation score，然后由一组得分高的视频帧组成 activity proposals End-to-end method 前面提到 e2e 的模型两种 anchor-based 和 anchor-free，我们先介绍 anchor-based 下面是列举一些典型工作， TGN 通过 LSTM grounder 实现了一个细粒度的 frame-by-word interaction，在每一个时间点 LSTM grounder 会同时计算一组 candidate segments，这些 segments 有着不同的 temporal scales 但是以当前的时间点为 ending point CMIN 和 TGN 类似同时计算一组 candidate，不同的是 CMIN 采用 sequential BiGRU network，同时作者还采用边界回归来 refine candidate，在模态交互上，CMIN 使用 syntactic GCN 对 query 的句法结构进行分析，同时使用 multi-head self-attention 捕捉视频中的 long-range temporal dependencies，然后进行一个细粒度跨模态的多段交互以产生 cross-modal features CBP 也是利用 sequential LSTM对每个时间点预测 temporal anchors and boundaries，同时 CBP 还利用了一个基于自注意力的模块来收集上下文线索，基于跨模态交互的输出直接计算每一个 contextual elements 对结果的贡献 CSMGAN 则是建立了一个连接图通过迭代消息传送来对 cross-/self-modal relations 进行建模，以此获得更高层次的交互信息。图中每一个结点整合了其邻居结点的信息，然后根据整合的信息和从 ConvGRU 得到的当前状态更新自己的转状态 FIAN 不同于前面的工作，采用了 content-oriented 策略来生成候选片段。即，FIAN 通过一个 refined cross-modal guided attention block 先获得细粒度的跨模态交互，然后通过 symmetrical iterative attention 得到 sentence-aware video representation 和 video-aware sentence representation SCDM 设计了一种分层的 temporal convolutional network 来定位目标片段，其中 semantics-conditioned dynamic modulation 利用 sentence semantics 来构成 sentence-related video contents。如下图所示，multimodal fusion 得到的特征可以表示为 接着在 semantic modulated temporal convolution module 中模型会动态地围绕句子调节 temporal feature maps，具体来说，对于每一个 temporal convolutional layer，记 $\\mathbf A=\\left{\\mathbf a_i\\right}$，feature unit $\\mathbf a_i$ 按照如下公式进行调节 其中，$\\gammai^c$ 和 $\\beta_i^c$ 为 modulation vectors，modulation vectors 根据 sentence representation $\\mathbf S=\\left{\\mathbf s_n\\right}{n=1}^N$ 进行计算 MAN 对整个视频使用分层卷积获得一系列 scale 的candidate，将 text feature 作为 dynamic filter 和前面的 visual representation 进行卷积计算得到定位结果。同时，MAN 引入了图卷积网络进行 temporal reasoning 进一步加强得到的 moment representation SCDM 和 MAN 在使用 temporal convolutional network 的时候只考虑了一维的 temporal feature maps，而 2D-TAN 则通过二维的 map 来建模视频段之间的 temporal relations。如下图所示，视频按照参数 $\\tau$ 分为几段等间距的 video clip，2D temporal feature map 上的 $(i, j)$ 位置表示从 $i\\tau$ 到 $(j+1)\\tau$ 的 candidate moment，同时 2D temporal feature map 不仅记录了不同长度的 moments，还记录了它们的邻接关系。temporal adjacent network 将每个 moment 和 sentence representation 融合后通过 CNN 整合视频上下文信息，最终得到每个 moment 的得分。2D-TAN 采用二元交叉损失，用 scaled IoU 作为监督信号，其范围在 $t{min}$ 到 $t{max}$ 之间，按照如下公式计算 其中，$o_i$ 作为 candidate moment 和 groundtruth moment 之间的 temporal IoU。最终损失函数可以表示为如下，$p_i$ 作为一个 moment 的预测分数 SMIN 对 2D temporal feature map 做更进一步的处理，在探索 moment 的固有结构后可以将其分解为 visual content 和用于跨模态交互、intra-moment 交互的 positional boundary parts 最后还有一篇工作『Multi-Stage Aggregated Transformer Network for Temporal Language Localization in Videos』，比较新，21年的，但是没有公开论文，它沿用 2D-TAN 的 proposal generation approach，后面设计了一个 visual-language transformer 和 multi-stage aggregation module 来获得 discriminative moment representations anchor-based 方法受限于一些人为设定的限制，比如 anchor 的数量和大小。同事，尽管不想 two-stage 那样需要 pre-segmentation，但是模型的性能任然取决于对 proposal candidate 的排序，排序的性能将极大地影响模型结果 下面我们介绍一些 anchor-free 的工作，anchor-free 丢弃了 candidate 的概念，直接从 frame 或 clip 入手，预测每一个 frame 或 clip 作为标记点的可能性，或者直接对整个特征做一个回归预测标记点的位置 下面是一些典型工作， ABLR 首先通过 bidirectional LSTM networks 来获取 query 和 video 的上下文信息，然后使用 a multi-modal co-attention interaction 来生成反应视频全局结构的 video attention 和突出重点细节的 sentence attention，最后基于注意力机制来预测时间坐标（$t^s$ 表示 starting timestep，$t^e$ 表示 ending timestep）。最后的坐标预测分两个：attention weight-based regression 和 attended feature-based regression，regression 的所示描述如下 其中 $R$ 表示 smooth-L1 function。$L{reg}^{ablr}$ 的目的是最小化预测片段和 groundtruth 片段之间的坐标偏差，同时，作者还设计一个 attention calibration loss $L{cal}$ 用于获得更准确的 video attentions—— $L_{cal}$ 会让在 groundtruth 里面的 clips 获得更大的注意力权重 LGI 类似与 ABLR 使用了基于注意力的定位 regression，同时还提出了一个更有有效的 local-global video-text interaction module PMI 建立了一个 channel-gated modality interaction model 以 pairwise 的方式对 channel-level and sequence-level interaction 建模直接预测片段定位 HVTG 预测 frame-level relevance scores，然后根据这个 score 得到定位边界。在跨模态的交互上，HVTG 将视频中的 objects 和句子中的 words 作为结点，建立了一个分层级的 visual-textual graph 来对特征进行编码 ExCL 受 NLP 的 Reading Comprehension task 的启发——从视频中检索一个片段类似于从一篇文章检索一段 text，于是 ExCL 用了三个不同的 start-end frame predictor 来预测每一帧（如下图所示）。橘色的 bidirectional LSTMs 表示 text sentence encoder，蓝色的表示 video encoder，ExCL 有两种模式，取决于训练目标是什么。ExCL-clf 采用分类 loss ExCL-reg 采用回归 loss，通过计算 SoftMax 输出的概率分布期望来预测时间点 VSLNet 采用了 standard span-based Question Answering framework，可以更进一步区分 video sequence 和 text passage，然后使用 query-guided highlighting strategy 所想搜索的范围得到一个粗糙的 highlight region L-Net（暂时略过）『Span-based Localizing Network for Natural Language Video Localization』 (暂时略过)『Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention』 DEBUG 通过对 groundtruth 中的所有帧预测到双向时间边界的距离来实现定位，groundtruth 中的所有 frame 视为正样本，对比只将groundtruth segment boundaries 视为正样本的方法，可以减小正负样例不平衡带来的问题。通常，一个典型的 dense anchor-free model 都会 有一个负责跨模态交互的 backbone，然后再接上做帧预测的 head work，DEBUG 正式这样，它选用 QANet 作为 backbone，然后通过三个分支 head 预测 classification score、boundary distances 和 confidence score 下面还有两篇工作 DRN 和 GDP 都是沿用了 backbone + head 的方式，在具体的选择实现有各自的不同，这里不再详细介绍 『Dense Regression Network for Video Grounding』 『Rethinking the BottomUp Framework for Query-Based Video Localization』 另外，还有一些工作它们不能严格分为 anchor-based 或 anchor-free，而是采取了两者的折中，例如，BPNet 就是采用 anchor-free 的方式生成 proposals，然后通过 anchor-based 方式匹配 segment 和 query。此外，还有例如 DPIN 和 CBLN 这样的工作，这里不一一细讲（『Context-aware Biaffine Localizing Network for Temporal Sentence Grounding』，『Dual Path Interaction Network for Video Moment Localization』） Reinforcement learning-based method 下面基于强化学习和弱监督学习的方法我们快速带过，不作为个人研究重点，不详细讨论 基于强化学习的方法将 TSGV 任务视为是一个 sequential decision process。在 action space 中的每一步都是人为设计好的 temporal transformations R-W-M 是第一篇将 RL 引入 TSGV 的工作，在每一个时间步，observation network 输出当前环境的状态，然后 actor-critic module 根据这个输出生成 action policy，基于 action policy agent 将调整时间边界。迭代的过程在遇到 STOP action 或达到最大步数后结束 Weakly supervised method 弱监督的方法旨在降低人力标注数据集，这里就不详细列举 Datasets and Evaluations table 1 是 TSGV 基本数据集的一个统计表，就不一一细致介绍每个数据集，针对每个数据集说一些需要注意的地方 DiDeMo 在制作时以 5s 为一个 clips unit 导致 groundtruth segments 都是以 5s 为倍数的，在一定程度上就限制了搜索的范围，人为地降低了任务的难度 TACoS 主要是围绕 cooking activities 的数据，视频的平均长度都在 300s 左右，相对来说比其他基本数据大很多 Charades-STA 是从 Charades 这个 activity recognition 数据集得到的，作者在制作时先是从视频描述中解析出 activity label，然后将描述与原始的 label-indicated temporal intervals 对齐，由此就构成了 TSGV 的数据。考虑这样得到的 query 太短作者进一步增加了 query 的复杂度，最终是得到 4,233 个长为 6.3 words 的sentence-segment pairs 和 1,378 个长为 12.4 words 的pairs ActivityNet Captions 中句子的平均长度为 13.48，句子长度成正态分布。因为测试集被官方保留用作比赛，大多数的 TSGV 工作都用两个可得的验证集 val1 和 val2 作为测试集 TSGV 的评价指标有这几个—— R@n, IoU@m（这是一个指标）和 mIoU，类似 object detection 中 IoU 的定义，在 TSGV 中 temporal IoU 按照下图所示的方法进行计算 mIoU 即 mean IoU，是简单的一个评价指标，计算所有样本的平均 temporal IoU R@n, IoU@m 这里不是太明确，我先放上原文的解释，后面再细说吧 大概意思应该是这样，在模型预测的 top-n 中有一个 segment 和 groundtruth 的 temporal IoU $>m$，那么就视样本 $i$ 预测正确，最终计算预测正确样本在全部样本中的占比 下面我们就前面的介绍的几种模型看下它们的 R@1, IoU@m 评估 先看 two-stage 之间的比较，整体来说，模型得分都很低，主要原因有下面几点， 这里列举的大多数 two-stage 工作在跨模态的交互上都很粗糙，所以并没有得到细粒度的交互特征准确定位视频段 将 candidate segment generation 和 sentence-segment matching 分割开的方式本身就存在问题，并不能实现全局最优 单个 segment 和 query 计算 matching relationship 在一定程度上割裂了 segment context 和 global video context 我们关注 TACos，相对于其他数据集，模型在这个数据集上的表现都比较差，原因是 TACos 主要关注的是一些 cooking activities，背景都在厨房不会产生大的变化，而最主要的视觉变化就是一些 cooking objects 的改变，如砧板、刀、面包等。同时，TACos 视频的长度给 SlidingWindow 的方法带来了很大的挑战 关注表格的后面两行可以发现，proposal-generation 的方法在减少 candidate segment 的基础上可以处理更大的数据集（ActivityNet Captions），也能够获得一些更好的分数 接着，我们看一下 e2e，相对于 two-stage 的方法，e2e 涨分明显，说明整体模型一个框架的方式在很大程度上改善了上面说到的 two-stage 问题的后面两点，至于模型间的细致比较我们就不再列出，各种模型有自己的跨模态交互方法，各有好坏 下面关于 RL-based 和 weakly supervised method 的 performance comparison 表格就不再给出，不作为重点关注内容 最后，我们给出 DiDeMo 数据集上的一些特殊评价指标的结果，在 MCN 中将 IoU 的阈值设为 $m=1$ 主要是考虑到 DiDeMo 中的片段都以 5s 为单位，那么基于匹配的方法就可能预测出一个完全对齐的 target moment，所以采用这种几段的 IoU 阈值。在下表中，采用了 R@1,m@1 和 R@5,m@1 两个指标对一些工作在 DiDeMo 上的表现进行了评估 Discussions 现有的数据集（temporal annotation biases 和 ambiguous groundtruth annotations）和评价指标给 TSGV 带来了一定的限制，我们在下面将详细讨论这些限制，并介绍一些新的改进工作。同时，我们还会讨论 TSGV 未来的一些研究 large-scale video corpus moment retrieval、spatio-temporal localization 和 audio-enhanced localization Limitations of Current Benchmarks 最近的一些工作指出 TSGV 当前的数据集和评价指标存在如下的一些问题， 训练集和测试集的目标片段 starting 和 ending timestamps 联合分布过于相似，这样即使不对 video 和 query 充分建模，一些模型仅是很好拟合分布或偏差也能在测试集上得到不错的结果 TSGV 上数据集对 gt segment 定位的标注显得有些不清楚和主观化，这很影响模型的评估 基于前面两点，现在的评价指标并不能客观真实地反应模型的有效性 下面就这些问题作出详细的阐释说明，先说 temporal annotation biases 的问题 『A Closer Look at Temporal Sentence Grounding in Videos: Dataset and Metric』这篇工作 re-organize Charades 和 ActivityNet Captions 两个数据集，设置了两 test sets：一组采用和训练集相同的 temporal location distribution，记为 test-iid；一组则采用和训练集完全不同的分布。如下图所示 作者利用这两个数据集对一些模型做了测试，结果如下，可以发现：大多数模型在 test-ood 上性能下降很明显，它们并没有对 video 和 query 充分建模，而是只学到了这种分布上的偏差。最近也有工作尝试去减小这种欺骗性关系的误导。 接着说下 ambiguous groundtruth annotations 的问题。『Uncovering Hidden Challenges in Query-Based Video Moment Retrieval』这篇工作指出：考虑到一个 query 可能对应多个视频段或者不同标注者对定位的信息标注不同，显然仅采用一个 gt 的方式有些许的不妥。如下图所示，作者重新标注了下 segment，根据 query “a woman is doing somersaults and big jumps alone”，Eugene标注者给出了不同的结果，蓝色所示部分，而且并不和数据集给定的 gt 重合。作者建议是采用 multiple gt，也针对 multiple gt 提出了两个新的评价指标 最后，我们说下 evaluation metrics 的问题。通常， R@n , IoU@m 都会选用一些较小的 IoU 阈值，如 $m\\in\\left{0.1, 0.3, 0.5\\right}$，但是这有非常大的问题。考虑 ActivityNet Captions，当中存在很大一部分的 gt segment 长度都很长，再加上之前说的 annotation biases，在 IoU 阈值较小的情况下预测的准确率就变得很高，这显然是不应该的。还是在『A Closer Look at Temporal Sentence Grounding in Videos: Dataset and Metric』这篇工作中，作者提出了 discounted-R@n, IoU@m 指标。作者认为 hit score $r(n, m, q_i\">$ 不应该只是取 0, 1 两个值，而是取 $\\left[0, 1\\right]$ ，这取决于预测片段和 gt 之间的下你该对距离，于是，hit score 就重新表示为 其中，$\\text{nDis}$ 计算预测边界和 gt 之间的距离然后根据视频长度归一到 $\\left[0, 1\\right]$ ，$(g_i^s, g_i^e)$ 表示预测边界的起始点和终止点 下面我们介绍一下 TSGV 未来的三个研究方向， Large-scale video corpus moment retrieval，简称 VCMR，是 TSGV，旨在从一大堆 video corpus 中找到 target video，然后再在 target video 中定位 segment Spatio-temporal localization 会更细一些，将 referring object/instance 定位为一个连续的 spatialtemporal tube，也就是一连串的 bboxes，如下图所示 Audio-enhanced localization 旨在通过视频中的声音来提供更多的信息，比如球员进球的欢呼声，或者通过 Automated Speech Recognition 技术将视频中人物的对话转换成 text，这些都将给 video grounding 带来更多的信息和细节 "},"【论文】UMT.html":{"url":"【论文】UMT.html","title":"UMT","keywords":"","body":" 【论文】UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection （pdf） QVHighlights 数据集提出了一个新的任务：video moment retrieval and video highlight detection，关于这个任务的进一步解释如 Fig 1 所描述。用数学语言描述如下：输入一个段未剪辑的视频 $V$（有 $Nv$ 个 clips）以及一个 query（有 $N_t$ 个 token），目标是定位所有和 query 高度相关的 moments（用 temporal boundaries $b\\in \\mathbb R^2$ 表示），同时对每个 moment 预测 clip-level saliency scores $\\left{s_i\\right}{i=1}^{N_v}$ 作者认为 QVHighlights 提出的 Moment-DETR 仅是给出了一个 strong baseline，在此更深一步，作者从 multi-modal learning（(visual-audio） 和 flexibility 两个方面出发探索了一种更强的 moment retrieval and highlight detection 方法，取名为 Unified Multi-modal Transformers (UMT)，模型框架如下所示 UMT 包含 5 个部分：uni-modal encoder，cross-modal encoder，query generator，query decoder 和 prediction heads 数据输入先经过各自预训练好的特征提取器 $Ev,E_a, E_t$，然后 visual 和 vaudio features 各自经过一个 uni-modal encoder 利用全局感受野建模上下文信息，之后再将两个模态特征输入到 cross-modal encoder，cross-modal encoder 输出的特征 $\\left{r_i\\right}{i=1}^{Nv}$ 送去 query generator 和 textual feature 共同作用生成 clip-level moment queries $\\left{q_i\\right}{i=1}^{Nv}$，这些 queries 再送入 query decoder 产生最后用以预测的 query-guided video features $\\left{d_i\\right}{i=1}^{N_v}$ uni-modal encoder uni-modal encoder 就是普通的 transformer encoder，作者认为现在大部分的videos/audios 特征提取器都是基于滑窗的策略，这样只建模了局部的的上下文信息，而缺乏全局的上下文信息。所以，在特征提取器后面再接了一个 transformer 来建模全局的上下文信息 cross-modal encoder 作者在 『Attention Bottlenecks for Multimodal Fusion』23的工作上将 bottleneck transformer module 分解为两部分：feature compression 和 expansion，如 Fig 3 所示 按照 23 中的方法引入 bottleneck tokens $\\left{zi\\right}{i=1}^{N_b}$ （其中 ，$N_b$ 是一个比 $N_v$ 小很多的数字），feature compression 在 bottleneck tokens 和 different modalities features 之间通过 multi-head attentions 实现，具体公式如下。与 transformer encoder 公式中唯一不同的是将 $z_i$ 作为 query matrix，旨在将跨模态的信息整合在 bottleneck tokens 中 在 feature compression 之后，作者又通过 feature expansion 将跨模态融合的信息再返回给两个模态的 multi-head attention，这个 multi-head attention 是新加的 作者指出这样做能以一个线性的计算复杂度使两个模态的特征得到很好的融合，同时避免了不必的干扰信息 query generator 作者提出 query generator 的原因是：transformer decoder 的输出序列长度取决于query embedding 的输入序列长度，而 query embedding 是一些随机初始化的可学习向量。在 video highlight detection 任务中，这种机制并不合适。所以作者引入了 query generator 用以产生 temporally aligned moment queries，其中 $\\left{ri\\right}{i=1}^{N_v}$ 做 multihead attention 的 query，textual features 作为 key 和 value 同时，文中还提到了一些没有 text queries 的情况，对于这类情况将 video-audio joint representations 和位置编码相加后用作 moment queries Query Decoder and Prediction Heads query decoder 将 $\\left{ri\\right}{i=1}^{Nv}$ 和 $\\left{q_i\\right}{i=1}^{N_v}$ 作为输入，然后解码视频特征。作者让 query decoder 的输出和 query encoder 的输入长度保持一致，这样做的目的是：1）很容易通过一个线性层就可以得到 cliplevel saliency (highlight) scores；2）动态的输出长度可以方便作者将 moment retrieval 任务视为一种 keypoint detection problem 具体来说，每个 moment 可以由一个 temporal center 和一个 duration (window) 来表示，而 temporal center 可以通过预测 temporal heatmap 和提取 local maxima 来得到，window 进一步就可以通过对 center feature 的回归来得到。Heatmap 上的点是离散的，必定不能和真正的 temporal center 对齐，由此也不可避免地造成 retrieval performances 的下降 作者采用了 4 个线性层分别预测 saliencies，centers，windows 和 offsets，关于训练 loss 的设置详见原文描述 下面是一些实验结果，我们暂时不做详细介绍 23. Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks for multimodal fusion. In Advances in Neural Information Processing Systems (NeurIPS), 2021. ↩ "},"【论文】Unicoder-VL.html":{"url":"【论文】Unicoder-VL.html","title":"Unicoder-VL","keywords":"","body":" 【论文】Li G, Duan N, Fang Y, et al. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training.（pdf） Unicoder-VL 和同期的 UNITER 都属于 one-stream model，与之前的 VisualBERT 相比两个网络都大同小异，Unicoder 的结构如下图所示，但是预训练任务增加到了 3 个： MLM，没有什么改变 MOC，对图像区域做遮挡操作，以 15% 概率选中遮挡区域，并在每次遮挡时以 80% 概率将特征随机替换为全 0 向量，以 10% 概率随机替换成其他区域对应的特征向量，以 10% 概率保持不变，预测遮挡区域的分类标签 VLM，没有什么改变 有趣的是，Unicoder-VL 只在 Image-Retrieval 和 VCR 两个下游任务上做了验证，Image-Retrieval 使用了两个数据集 MSCOCO 和 Flickr30k，具体表现如下表所示 总体感觉，Unicoder-VL 是微软发的一篇水刊，没有价值很高的内容 "},"【论文】UNITER.html":{"url":"【论文】UNITER.html","title":"UNITER","keywords":"","body":" 【论文】Chen, Yen-Chun, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. （pdf） What is UNITER UNITER 主要的不同是给多模态社区带来了新的同于 ViLBERT、LXMERT 的路线——在模态特征融合方面，可以共享一个编码器。虽然 encoder 的输入源不同，但是通过 transformer 的双向连接也能实现跨模态的交互 UNITER 由 3 部分组成： 有 2 种 Embedder 结构，Image Embedder 通过 Faster-RCNN 输出的 ROI feature 和 7-D 的位置特征 $[x_1,y_1,x_2,y_2,w,h,w*h]$（normalized top/left/bottom/right coordinates, width, height, and area）进行融合建模；Text Embedder 参考 BERT 输入， token embedding 和 position embedding 相加之后经过一个 LayerNorm 得到 text feature 建模完的 image feature 和 text feature 直接接入 transformer 进行双向建模，融合两种模态，从而达成目的，不同于 two-stream 预训练模型，这两类模态在 UNITER 中共享同一个 encoder 作者设计了 4 种预训练的任务去训练模型， 前三个任务 Masked Language Modeling（MLM）、MRM（Mask Region Model）和 Image-Text-Matching（ITM）都是一些常见的多模态预训练任务。其中，MRM 有 3 个变体任务：MRC（Mask Region Cls），MRFR（Mask Regin Feature Regress），MRC-KL（MRC + KL divergency） 第四个任务是 WRA（Word Region Alignment），这个任务主要关心的是词和图像区域的对齐，和 ITM 不同，ITM 关心的是句子和图像的对齐 Pre-training Tasks MLM 被遮掉的词记为 $\\mathbf w{\\mathbf m}$，其附近的词为 $\\mathbf w{\\backslash\\mathbf m}$，目标采用最小负对数似然损失，包括对周围词的观察和对所有图像区域的观察 MRFR 输入的 ROI pooled feature 记为 $r(\\mathbf v{\\mathbf m}^{(i)})$，transformer 输出的结果经过一层 FC layer 变换到和 ROI pooled feature 一样的维度大小，记为 $h{\\theta}(\\mathbf v_{\\mathbf m}^{(i)})$ 表示遮罩区域预测的结果，目标采用 L2 损失 MRC transformer 的输出经过一层 FC layer 和一层 softmax 得到一个归一化的预测分布 $g{\\theta}(\\mathbf v{\\mathbf m}^{(i)})$，由于在数据集中并没有提供每个对象的分类标签，所以将被遮掉的区域通过 Faster RCNN 用对象检测器得到置信度分数最高的分类标签，同时得到一个 one-hot vector $c(\\mathbf v_{\\mathbf m}^{(i)})$，目标使用交叉熵损失 MRC-KL MRC 是采用了一种预测 hard label 的方式，即输出 0 or 1，我们也可以考虑采用 soft label 的方式——比较两个输出的分类分布 ITM UNITER 中同样也有 [CLS] 这样的 special token，在 [CLS] 上应用一个 FC layer 计算输入图文对的匹配得分，记为 $s_{\\theta}(\\mathbf v, \\mathbf w)$，很显然 ITM 是一个二分类问题，输出标签记为 $y\\in\\left{0,1\\right}$，目标采用二进制交叉熵损失 WRA WRA 使用了 Optimal Transport，记 $\\mathbf T\\in\\mathbb R^{T\\times K}$ 表示学习得到的 transport plan，它提供了一种最优化 $\\mathbf w$ 和 $\\mathbf v$ 之间对齐的方案 作者主要考虑 OT 下面的 3 点特性非常适合 WRA 任务： self-normalization：$\\mathbf T$ 中所有的元素之和为 1 sparsity：优化结束后，$\\mathbf T$ 至多只有 $2\\cdot max(K,T)-1$ 个非零元素，这样可以得到一个易于解释且稳定的对齐方案 efficiency：与常规的线性规划求解不同，OT 只用到迭代和矩阵向量乘法就可以解决，很适合大规模的模型预训练使用 具体来说，记 $(\\mathbf w,\\mathbf v)$的离散分布为 $\\mathbf\\mu,\\mathbf\\nu$，其中 \r \\mathbf\\mu=\\sum\\limits_{i=1}^T\\mathbf a_i\\delta_{\\mathbf w_i},\\ \\mathbf a=\\left\\{\\mathbf a_i\\right\\}_{i=1}^T\\\\\r \\mathbf\\nu=\\sum\\limits_{j=1}^K\\mathbf b_j\\delta_{\\mathbf v_j},\\ \\mathbf b=\\left\\{\\mathbf b_j\\right\\}_{j=1}^T\r $\\mathbf a,\\mathbf b$ 作为权重向量，$\\sum\\limits{i=1}^T\\mathbf a_i=\\sum\\limits{j=1}^K\\mathbf bj=1$，$\\delta{\\mathbf w_i}$ 表示 $\\mathbf w_i$ 上的 Dirac function 于是，$\\mathbf\\mu,\\mathbf\\nu$ 之间的 OT distance 就可以用于训练图文对 $(\\mathbf w,\\mathbf v)$ 对齐，用数学公式描述为 \r \\mathcal L_{WRA}(\\theta)=\\mathcal D_{ot}(\\mathbf\\mu,\\mathbf\\nu)=\\underset{T\\in\\Pi(\\mathbf a,\\mathbf b)}{min}\\ \\sum_{i=1}^T\\sum_{j=1}^K\\mathbf T_{ij}\\cdot c(\\mathbf w_i,\\mathbf v_j)\r 其中，$\\Pi(\\mathbf a,\\mathbf b)=\\left{\\mathbf T\\in\\mathbb R_+^{T\\times K}|\\mathbf T\\mathbf 1_m=\\mathbf a,\\mathbf T^T\\mathbf 1_n=\\mathbf b\\right}$，$\\mathbf 1_n$ 表示一个维度为 $n$ 的全 1 向量，$c(\\mathbf w_i,\\mathbf v_j)$ 是计算 $\\mathbf w_i$ 和 $\\mathbf v_j$ 之间距离的代价函数，在实验中采用余弦距离，即 $c(\\mathbf w_i,\\mathbf v_j)=1-\\frac{\\mathbf w_i^T\\mathbf v_j}{||\\mathbf w_i||_2||\\mathbf v_j||_2}$ 实际中计算 $\\mathbf T$ 是相当耗费计算资源的，所以采用 IPOT 算法近似求解 $\\mathbf T$ Experiments UNITER 的预训练数据集采用 COCO，Visual Genome (VG)，Conceptual Captions (CC) 和 SBU Captions 四个数据集 论文中训练了两个 UNITER 模型：（1）UNITER-base: L = 12, H = 768, A = 12, Total Parameters = 86M；（2）UNITER-large: L = 24, H = 1024, A = 16, Total Parameters = 303M fine-tunning 阶段共采取了 6 个下游任务和 9 个数据集，下表是 base 和 large 两个模型在各个下游任务上的表现：UNITER-Base 在 9 个数据集上面几乎都好于其他的模型，而 UNITER-Large 则取得了当前最好的效果 Reference UNITER多模态预训练模型原理加代码解读 "},"【论文】VGGNet.html":{"url":"【论文】VGGNet.html","title":"VGGNet","keywords":"","body":"﻿* 【论文】Simonyan K , Zisserman A . Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer ence, 2014.（pdf） 【新颖点】 更深的网络 使用小卷积核 $3\\times3$ 和 $1\\times1$ 替代大卷积核 多尺度训练及预测 网络结构 比较常见的为 VGG-16 和 VGG-19，VGGNet 把网络分成了 5 段，每段都将多个 $3\\times3$ 的卷积网络串联在一起，每段卷积后面接一个最大池化层，最后面是 3 个全连接层和一个 softmax 层 加深比加宽更好 如果要增大感受野，一般的思路是直接增加卷积核的大小，但是这直接增加了参数的数量，假设有 $C$ 个 $7\\times7$ 的卷积核，其参数量为 $7C\\times7C=49C^2$。作者认为用小的卷积核串联不仅能起到增大感受野的作用，而且能够减小参数数量，例如，这里我们可以用 $3$ 个 $3\\times3$ 的卷积核串联同样能达到 $7\\times7$ 的感受野，但参数量却只有 $3\\times3C\\times3C=27C^2$ 至于为什么三个 $3\\times3$ 的卷积核串联的感受野等同于一个 $7\\times7$ 的卷积核，或者两个 $3\\times3$ 的卷积核串联的感受等同于一个 $5\\times5$ 的卷积核简单画个草图就能很容易明白 另外，论文中作者也主张使用 $1\\times1$ 的卷积核来增加线性换，这种方式在保持空间维度不变的情况下，增加了决策函数的非线性性 多尺度训练 VGGNet 使用了 Multi-Scale 的方法增强数据——将原始图像缩放到不同尺寸 $S$，然后在随机剪裁成 224 x 224 的图片，这样可以增加数据量。作者设置 $S$ 在 $[256, 512]$ 这个区间内取值，使用 Multi-Scale 获得多个版本的数据，并将这些数据合在一起训练 其他改进点 证明 LRN 效果不大 训练时，先训练级别简单的 VGGNet，例如上表中的 A 网络，然后再使用 A 网络的权重来初始化后面的复杂模型，加快训练速度 相较于 AlexNet，使用了 $2\\times2$ 的池化核，获取更多的细节信息 "},"【论文】VideoMAE.html":{"url":"【论文】VideoMAE.html","title":"VideoMAE","keywords":"","body":" 【论文】Feichtenhofer C, Fan H, Li Y, et al. Masked Autoencoders As Spatiotemporal Learners.（pdf） 【论文】Tong Z, Song Y, Wang J, et al. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training.（pdf） :mag_right: VideoMAE 做 VideoMAE 的有两篇，一个是何恺明团队的，一个是南京大学的，github 的代码是南大的，但是两篇的内容几乎是一样的，所以就放在一起说，不做单独区分 VideoMAE 和 MAE 的原理基本类似，只是从 2D 变到了 3D，增加一个时间维度，模型结构如下图所示 我们不对论文做详细阅读，主要关注下面一些问题 输入上，在 pre-training 时，采取 $16\\times224\\times224$ 的输入，从原始视频按照 4 的步长采样 16 帧，模型的 temporal batch size = 2，于是，对于 $16\\times224\\times224$ 的输入，会产生 $8\\times14\\times14$ 个 token（spatial patch size $16\\times16$） VideoMAE 结构上虽然是 encoder-decoder，但实现上是两 个 ViT 模型，根据 MAE 的非对称结构，VideoMAE encoder 采用 12 层 ViT，decoder 采用 8 层 ViT （positional embedding） Decoder 在还原时可以预测整个 spacetime patch（$t\\times16\\times16$），但是作者发现只需要预测一个 time slice 即可（$16\\times16$）。Decoder 预测原始的像素值或者每一个 patch 的 normalized values，训练损失采用 MSE 在下游任务 Kinetics action classification finetune 上，对 encoder average pooling 的 token 接一个线性分类器 （输入全部的视频吗？还是要 mask？有 cls token 吗？） "},"【论文】ViLBERT.html":{"url":"【论文】ViLBERT.html","title":"ViLBERT","keywords":"","body":" 【论文】Lu, Jiasen, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. （pdf） What is ViLBERT 以往的 vision-and-language 任务都是先预训练好独立语言和视觉模型，然后针对特定的任务进行学习。这种方法的问题明显，就是缺少两者之间的互动。ViLBERT 的提出也正是解决预训练模型里视觉特征和语言特征的流动性问题 文中借鉴了当前很多模型使用自监督无标注数据学习的方式。例如，在自然语言中，抓取大量无标注数据通过遮罩其中部分信息，使模型实现自监督学习，BERT、GPT 采用了这种方法都取得不错的结果。作者也希望将该方法用到跨模态的模型训练中，那么首先要解决的问题就是合适的数据源——作者考虑最近发布的 Conceptual Captions dataset ViLBERT 结合图片和文本信息特征， 将 BERT 改造成双路结构，并通过 co-attention 方式交互。这样的结构可以适应每个模态不同的处理需求，也能在不同的 representation depth 上进行跨模态的交互 文中用两种任务训练模型：预测遮罩掉的图片区域内容和单词；预测文字与图片是否匹配 ViLBERT: Extending BERT to Jointly Represent Images and Text 有一种最简单的方式是通过聚类离散化视觉输入的空间，然后得到一些和输入文本一致的 visual token，接着就输入到 BERT 里面。这种做发问题是： 通过聚类的离散化会丢失重要的视觉细节信息 忽略了两种模态的 input representation 之间具有不同的抽象程度。例如，两个图像区域之间的联系会弱于两个单词之间的联系，如果强行使用那些适应 visual token 的预训练好的权重到语义特征上，那么这样子就会把 learned BERT language model 带跑偏 所以，作者设计了 two-stream architecture，每一个分支都有一组 transformer block 和 co-attention transformer layer 记 $v1,\\cdots,v_T$ 表示 region features，$w_0,\\cdots,w_T$ 表示文本输入，模型最终的输出为 $h{v0},\\cdots,h{vT}$ 和 $h{w0},\\cdots,h{w_T}$ visual stream： 模型先用 Faster RCNN 从图片中提取多个目标区域的特征，由于图片区域不像文字特征一样含有一定的先后位置信息，因此采用一个长度为 5 的向量给图片位置编码，向量包含左上角坐标、右下角坐标和面积。然后，将位置向量投影到和视觉特征一样的维度，接着对位置向量和视觉特征进行加和 text stream： 在两个分支交互之前，text stream 需要先经过一个 BERT（即下图中的 L-k ×） Co-Attentional Transformer Layers 这部分说起来很简单，co-attention transformer layer 有两个 encoder block，一块处理视觉信息，一块处理文本信息，然后视觉的 keys 和 values 传到处理文本信息的 encoder layer 的 multi-head attention 中，同理文本的 keys 和 values 也这么做。因此，各自的注意力模块会以另一种模态为条件产生 attention-pooled features。为什么可以这么做呢？道理很简单，还记得在 transformer 中吗，key 相当于查询索引，value 相当于查询值，那么在 image-conditional language attenttion 下，就是我们拿着视觉上的信息，看看在输出的回答上我们更注意什么，然后更新对 language attention Training Tasks and Objectives 如图 a 所示，将输入的 15% 进行遮罩，遮掉的可能是图片，也可能是文字，用剩余部分对其进行预测。遮掉图片时 90% 的情况用 0 进行填充，10% 的情况该区域保持不变。遮罩掉的文本和 BERT 中的处理一样。模型根据语义预测图片区域的分布，最小化预测分布和真实分布的 KL 散度。这样做是由于语言常常能反应图片的高级语义，但不太可能重建确切的图像特征 在匹配任务中，模型输入的是 $\\left{IMG,vq,\\cdots,v_T,CLS,w_1,\\cdots,w_T,SEP\\right}$（IMG token 表示图像区域序列的开始，它表征整个图像），输出 $h{IMG}$ 和 $h_{CLS}$ ，计算两者的点乘并通过一个线性层判断二者最终是否匹配。Conceptual Captions dataset 的图像和描述是对应的，为了生成负样本，选择随机替换掉图像或描述中的一个 Experiment ViLBERT 的下游任务有四个，分别如下图所示 VQA： 在 VQA 2.0 上进行训练，fine-tunning 时使用 $h{IMG}$ 和 $h{CLS}$ 进行点乘，然后经过一个两层的 MLP 映射为 3129 个可能的答案。VQA 可以视为多标签的分类问题，那么每个预测的答案就可以根据它与 10 个人工标注答案的相关性计算一个 soft target score，然后根据 soft target score 计算 binary cross-entropy loss。预测时，只需要一个 softmax 就可以搞定了 VCR： 给出一张图像，完成两个任务：问答（Q → A）和回答理由（Q → AR），答案都是多项选择。VCR 数据集包括 290k 个 QA 问题，它们来自 110k 个电影场景。具体实现是将问题和每个可能的答案连接起来，形成四个不同的文本输入，并将每个文本输入和图像一起传入 ViLBERT，在 VILBERT 基础上加入一个线性层，给每组图文对打分 Grounding Referring Expressions： 根据文字描述在图中框出对应物品，使用 RefCOCO+ dataset 训练。文中直接使用 Mask R-CNN 在 COCO 数据集上训练得到的目标区域，并在最后的表征层后加入一个线性层，预测各个区域与文字匹配的得分，将最高分区域作为最终的预测结果 Caption-Based Image Retrieval： 根据文字描述在图片池中搜索图片，Flickr30k dataset 包含 31k 张图片，这些图片和文字的对应关系优于自动抓取的图片。使用一对四的方式训练，对每个数据随机抽取三个干扰项：一个随机的标题，一个随机图像以及一个从 100 个最近邻中选取的 hard negative。实际预测时计算 caption-image pair 的匹配度并选择得分最高的图片作为预测结果。为了提高效率，作者在第一个 Co-TRM layer 之前使用了 cache 机制存储 linguistic stream representation Zero-shot Caption-Based Image Retrieval： Zero-shot 指检索在训练集中未见过的数据，之前的几个任务都使用数据集精调模型，此任务中直接使用预训练的模型在 Flickr30k dataset 上进行检索，以评测预训练模型提取特征的能力 Analysis 下图是论文中的消融实验结果 有如下几点发现： two-stream 对于模型性能提升有很大的帮助。作者认为像 Videobert 这样的 single-stream model，同时输入两个模态到 transformer blocks 中，会加深对视觉特征的处理，且使两个模态过早产生交互。这样由于两个模态的特征在整个处理过程中一直都在交互，我们无法获得任何的 representations 来提高模型的效率。而在 ViLBERT 的实验中证明，将两个模态分开处理再融合的方法是有效的 预训练的代理任务能够提升 visiolinguistic representations，ViLBERT vs ViLBERT† pretraining + fine-tuning 的策略对于下游任务的解决很有帮助 另外，作者也研究了 CO-TRM→TRM 层数对下游任务的影响 以及预训练数据集大小对模型性能的影响 Referencee 论文阅读_跨模态模型VILBERT "},"【论文】VILLA.html":{"url":"【论文】VILLA.html","title":"VILLA","keywords":"","body":" 【论文】Gan, Zhe, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-Scale Adversarial Training for Vision-and-Language Representation Learning. （pdf） What is VILLA VILLA（Vision-and-Language Large-scale Adversarial training）第一次在跨模态预训练的任务中引入了对抗训练。整个模型框架如下图所示，VILLA 包括两个训练阶段： 与任务无关的对抗性训练（APT） 特定于任务的对抗性 fine-tunning（AFT） 为什么作者会想到提出这样的 one-stage training 呢？ 首先，直观地说，如果能有一个训练得非常好的预训练模型（充分理解了图像和语言之间的联系），那么这个预训练模型和下游的 V&L 任务模型将会产生很好的共鸣 于是，那么现在的问题就是考虑如何增强与任务无关的预训练模型。作者没有在训练任务上做花样，采取了常用的 3 种预训练方法 Masked language modeling、Masked region modeling 和 Image-Text matching 。但是，通过对抗性学习，作者希望在 APT 阶段提高模型的泛化能力，这样就很容易在 AFT 阶段迁移到不同的的任务上得到良好的表现 同时，由于大规模预训练模型的容量巨大，而下游任务中标记的数据量却有限，过于的 fine-tunning 容易陷入过拟合中，所以引入了 AFT 阶段的对抗性学习使得下游任务模型具有更加稳健的表现 下面我们具体说一下作者是怎么加入对抗性学习的 为了在训练时能方便地引入对抗样本，作者建议在每个模态的嵌入空间中进行对抗训练，而不是在图像像素和文本标记上添加对抗性扰动——对于 text modality，在 word embedding 中添加对抗扰动；对于 image modality，直接在提取得到的图像区域特征上添加对抗性扰动，因为我们目标只是提高 V&L 模型的泛化能力，不是生成对抗性图像示例，所以和以往的对抗性学习采取的方式不同 对抗性学习的引入会大大增加计算的消耗，为了实现大规模训练，作者采用 free adversarial training strategy，该方案可以在计算输入梯度时以一种不增加额外开销的方式得到参数梯度。同时，作者还引入了基于 KL 散度的正则化来强制使得预测结果的置信度水平相互接近，这不仅能够提高训练目标的平滑度，在实践也已证明是重要的正则化方法，可有效提高模型性能 下面我们将详细介绍作者在论文中具体工作是如何实现的 Perturbations in the Embedding Space 作者只在 image embedding 和 word embedding 上添加对抗性扰动，对多模态特征的其他内容保持不变，例如图像、文本的位置编码这些都不做扰动 另外，同一时刻只对一个模态添加扰动，记两个模态各自的抵抗性扰动为 $\\delta{img},\\delta{txt}$，那么输出预测要是 $\\hat y=f\\theta(x{img}+\\delta{img},x{txt})$，要么是 $\\tilde y=f\\theta(x{img},x{txt}+\\delta{txt})$。为了保留 embedding 的原始语义，$\\delta{img}$ 和 $\\delta{txt}$ 的范数被限制得很小 “Free” Multimodal Adversarial Training 在详细介绍 VILLA 的训练目标之前，我们先按照作者在文中的数学符号简单梳理一下跨模态预训练的内容 记下游任务为 $\\mathcal Tf$，对应的数据集 $\\mathcal D_f$ 由 $(x{img}, x_{txt}, y)$ 构成，$y$ 表示数据集的标签，例如在 VQA 中，$y$ 表示候选池中的 ground-truth answer；在 VCR 中 $y$ 就是四选一的分类标签 于是，pretraining 和 fine-tunning 统一起来的训练任务就可以用如下的公式描述，不同的 $y$ 实例化了每一个下游任务，其中 $\\theta$ 表示学习的参数集在 fine-tunning 中用预训练得到的权重初始化 \r \\underset{\\theta}{min}\\ \\mathbb E_{(x_{img},x_{txt},y)\\sim\\mathcal D}\\left[L(f_\\theta(x_{img},x_{txt}),y)\\right]\r 现在来看 VILLA 总的训练目标 \r \\underset{\\theta}{min}\\ \\mathbb E_{(x_{img},x_{txt},y)\\sim\\mathcal D}\\left[\\mathcal L_{std}(\\theta)+\\mathcal R_{at}(\\theta)+\\alpha\\cdot\\mathcal R_{kl}(\\theta)\\right]\r 对于上面的第一项很清楚，就是传统的跨模态预训练的损失，即 $\\mathcal L{std}(\\theta)=L(f\\theta(x{img},x{txt}),y)$ 重点关注后面两项，$\\mathcal R{at}(\\theta)$ 表示 label-preserving AT loss，$\\mathcal R{kl}(\\theta)$ 表示 finer-grained adversarial regularization $\\mathcal R{at}(\\theta)$ 完成了 lebal-preserving 的对抗性攻击，其定义为 \r \\begin{align}\r \\mathcal R_{at}(\\theta)&=\\underset{||\\delta_{img}||\\leq\\epsilon}{max}\\ L(f_\\theta(x_{img}+\\delta_{img},x_{txt}),y)\\\\\r &+\\underset{||\\delta_{txt}||\\leq\\epsilon}{max}\\ L(f_\\theta(x_{img},x_{txt}+\\delta_{txt}),y)\r \\end{align}\r 其中，$L$ 表示对抗性 embedding 上的交叉熵损失。我们使用 Frobenius norm 来限制 $\\delta{img}$ 和 $\\delta_{txt}$ $\\mathcal R{kl}(\\theta)$ 则进一步要求预测结果的置信度水平都十分接近，这些预测结果都是一些在 simplex $\\Delta_n$ （$n$ 是类别数）上的概率向量，具体来说，$\\mathcal R{kl}(\\theta)$ 定义为 \r \\begin{align}\r \\mathcal R_{kl}(\\theta)&=\\underset{||\\delta_{img}||\\leq\\epsilon}{max}\\ L_{kl}(f_\\theta(x_{img}+\\delta_{img},x_{txt}),f_\\theta(x_{img},x_{txt}))\\\\\r & +\\underset{||\\delta_{txt}||\\leq\\epsilon}{max}\\ L_{kl}(f_\\theta(x_{img},x_{txt}+\\delta_{txt}),(f_\\theta(x_{img},x_{txt}))\r \\end{align}\r 其中，$L_{kl}(p,q)=KL(p||q)+KL(q||p)$ 讲完了训练目标，我们现在来看一下如何求解吧 在优化上，总目标的最小化可以通过 SGD 解决，$\\mathcal R{at}$ 的最大化则可以通过 PGD，以 $\\delta{img}$ 为例， \r \\delta_{img,t+1}=\\Pi_{||\\delta_{img}||\\leq\\epsilon}(\\delta_{img,t}+\\alpha g(\\delta_{img,t})\\ /\\ ||\\delta_{img,t}||_F)\r $g(\\delta{img,t})=\\triangledown{\\delta{img}} L(f\\theta(x{img}+\\delta{img},x{txt}),y)$ 表示 损失梯度中相对于 $\\delta{img}$ 的部分，$\\Pi{||\\delta{img}||\\leq\\epsilon}$ performs a projection onto the $\\epsilon$-ball \"Free\" AT Strategy K-step PGD 会有 K 次的前向传播和反向传播，这样的计算消耗是很大的。同时，在 K-step PGD 中只有在最后一步对抗性扰动才会在模型训练中被用到。考虑这两点问题，作者采用 FreeLB 中的方法通过多次的 PGD 迭代来精修对抗性 embedding，同时在每一次迭代中累积 free parameter gradients $\\triangledown_\\theta L$，这样根据累积的梯度就可以一次性更新完模型参数 $\\theta$，实际上产生了一个虚拟的 K 倍的 mini-batch 下图的算法描述了 Free Multimodal Adversarial Training 的完整过程 Experiments 下图是 VILLA 在不同下游任务和目前表现不错的模型的比较 论文中还给出了 VILLA 和 UNITER 注意力可视化的对比 "},"【论文】ViLT.html":{"url":"【论文】ViLT.html","title":"ViLT","keywords":"","body":" 【论文】Kim W, Son B, Kim I. Vilt: Vision-and-language transformer without convolution or region supervision.（pdf） Background 在回答 What is ViLT 这个问题之前，相信你应该读过不少的关于 VLP 的论文，但是不知道你对这些论文是否有一个总结。如果没有，很好 ViLT 论文中给出了一些综述，不放先看看 Taxonomy of Vision-and-Language Models 作者依据（1）在参数或者计算上两种模态是否保持平衡（2）在网络的后面层两种模态是否相互作用，将 VLP model 分为如下的 4 类 （a）对图像和文本独立使用 encoder，图像的计算更多，在跨模态交互中使用简单的点积或者浅层 attention layers 来表示两种模态的相似性，典型的如一些比较早的 VSE、VSE++ 和 SCAN （b）每个模态使用相同开销的 transformer encoder 进行 embedding，然后通过点积进行浅层的跨模态交互，典型的是 CLIP，尽管 CLIP 在 image-to-text retrieval 上的 zero-shot 表现不错，但是在其他 V & L tasks 上表现却一般 （c）使用深层 transformer 实现跨模态的交互，但是由于 visual embedding 仍然使用比较笨重的卷积网络抽取特征，计算量依然还是很大，典型的如 ViLBERT、UNITER 和 Pixel-BERT （d）以 ViLT 为例，visual embedding 和 text embedding 一样轻量，模型的主要计算集中在模态交互上 Modality Interaction Schema 跨模态交互有两种方式：一种是 single-stream，如 BERT 和 UNITER，对图像和文本 concate 后进行跨模态交互；另一种是 two-stream，如 ViLBERT 和 LEXMERT，在输入的时候不会将图像和文本 concate 在一起，两个是各自分开的 ViLT 沿用了 single-stream的交互方式，避免引入额外的计算量 Visual Embedding Schema 现有的 VLP 模型的 text embedding 基本上都使用 BERT 结构，但是 visual embedding 各有不同。在多数情况下，visual embedding 是现有 VLP 模型的瓶颈 visual embedding 的方法总共有三大类， region feature 方法通常采用 Faster R-CNN 二阶段检测器提取检测区域的特征 grid feature方法直接使用 CNN 提取 grid feature patch projection 方法来源于 ViT，将输入图片切片然后通过线性映射提取特征。ViLT 沿用该方法来完成 visual embedding，采用 $32\\times32$ 的 patch projection ViLT ViLT 的整体结构如下图所示，使用 ViT 的参数初始化，这样做的目的是让交互层在没有独立的 deeper visual embdder 的情况下也可以很好的处理视觉特征 其余的内容就和 ViT 比较像了， 文本特征输入部分，将文本看成一个词序列 $t$，通过 word embedding matrix $T$ 后加上一个 position embedding $T^{pos}$ 得到嵌入后的结果 $\\bar t$，再加上对应的 modal-type embedding $t^{type}$ 图像特征输入部分也是如此，$v$ 是 flattened patch，$V$ 是 linear projection transformer 的输入是 text embedding 和 image embedding 连接的结果 $z^0$，经过 $D$ 层的 encoder 得到 final contextualized sequence $z^D$ $p$ 是 pooled representation，经过一个线性映射 $W_{pool}$ 和 tanh 激活 值得一提的是 word embedding 和 visual embedding 都分别嵌入了一个额外通过学习得到的 [class]，方便和下游任务的对接 Pre-training Objectives ViLT 的预训练代理任务有两个： ITM，判断图文对是否匹配。另外，ViLT 还设计了一个 word patch alignment（WPA）代理任务，采用 IPOT 来计算 textual subset 和 visual subset 的对齐分数 MLM，ViLT 使用了一个叫 whole word masking 的技巧——将整个单词的 wordpiece tokens 全部遮掉，让模型能更好的利用另外一个模态的信息来预测被掩盖的词。举个例子，giraffe 这个词，tokenize 之后变成了 3 个 woedpiece tokens [\"gi\", \"##raf\", \"##fe\"]，如果不使用 whole word masking 的话就是像这样完成遮罩 [\"gi\", \"[MASK]\", \"##fe\"]，那么模型可能仅仅依靠 [\"gi\", \"##fe\"] 就预测除了被遮掉的部分，这样就违背了 MLM 设计的初衷 Experiments 下面是 ViLT 的预训练数据集和一些下游任务的表现 可见 ViLT 在下游任务上的表现虽然略次一些，但是其计算却有着非常大的优势 下图是 ViLT WPA 的可视化结果 Reference ViLT：最简单的多模态Transformer "},"【论文】VisualBERT.html":{"url":"【论文】VisualBERT.html","title":"VisualBERT","keywords":"","body":" 【论文】Li L H, Yatskar M, Yin D, et al. Visualbert: A simple and performant baseline for vision and language.（pdf） 这里我们对 VisualBert 做一个简单的介绍，如果你已经详细阅读过 UNITER，那么 VisualBERT 对你来说应该不会太过陌生。VisualBERT 应该是多模态预训练中的第一个 one-stream model，借助与 transformer 中的注意力机制来挖掘文本和图像之间的关系 VisualBERT 的结构如下图所示，引入 visual embedding $F$ 对图像进行建模，图像中每个 bounding region 的嵌入记为 $f\\in F$，$f$ 融合了三个内容：（1）经 CNN 得到的 bounding region 的特征 $f_o$;（2）segment embedding $f_s$，在 BERT 中用于区分 embedding 属于两句话中的那一句 ，这里用于区分是图像的 embedding 还是文本的 embedding；（3）position embedding $f_p$，用于为 transformer 提供位置感知 VisualBERT 采用两种方式进行预训练，这两种方式都和常见的预训练任务有一点点区别： MLM，类似于 BERT，不同的是加入了视觉特征；与 ViLBERT 不同的是 VisualBERT 并没有对图像区域进行遮罩 Sentence-image prediction，由于 VisualBERT 使用的数据集 COCO captions 对一张图片有 5 个独立的描述，因此作者构建一个由两段 caption 组成的文本语段，其中一个是对对应图片的 caption，另外一个则 50% 是随机的 caption，50% 是对应图片的另外一个不同的 caption VisualBERT 的 transformer encoder 与 BERT~BASE~ 的配置一致：12 层，hidden_size = 768，attention head = 12，同时用 BERT~BASE~ 的参数进行初始化 下面是 VisualBERT 在一些下游任务上和其他模型的对比 下图是论文中注意力权重的可视化结果 "},"【论文】ViT.html":{"url":"【论文】ViT.html","title":"ViT","keywords":"","body":" 【论文】Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale.（pdf） What is Vision Transformer（ViT） ViT 的第一个特点是告诉大家 CNN 在视觉中并不是必须的，下面是 ViT 的模型架构 Patch Embedding ViT 希望尽可能地遵循原始的 transformer 结构，在 NLP 中 transformer 的输入是一维的词序列，而图片却是二维的，那怎么办呢？ ViT 的做法很简单，将图片划分为多个不重叠的 patch，记输入的图像为 $\\mathbf x\\in\\mathbb R^{H\\times W\\times C}$，patch 的大小为 $P\\times P$，那么一幅图像就被分为了 $N=HW/P^2$ 个 patch，每个 patch 压平后可以表示为 $\\mathbf x_p\\in\\mathbb R^{N\\times(P^2\\cdot C)}$，即这里的每个 patch 已经变为一维的，大小为 $P^2\\cdot C$，于是，一个 patch 就类比于 NLP 中的一个 word 接着，我们需要做的是对输入进行嵌入，得到 patch embedding，ViT 提供了两种方式： Linear Projection：通过一个简单的线性变换将 patch 映射到 $D$ 维，我们将线性变换记为 $\\mathbf E\\in\\mathbb R^{(P^2\\cdot C)\\times D}$ Hybrid Architecture：作为原始 image patches 的替代，可以将 CNN 得到的特征图压平后通过 $\\mathbf E$ 映射得到输入序列，这种情况下就相当于 patch 的大小为 $1\\times1$ 在原码中可以发现，这里的 linear projection 等同于对 $\\mathbf x_p$ 做一个 size = $P\\times P$、stride = $P$ 的卷积操作 [class] Token 和 BERT 操作一样，在输入序列前面加入一个 learnable embedding $\\mathbf z0^0=\\mathbf x{class}$，对应的 transformer encoder 的输出记为 $\\mathbf z_L^0$，表示 image representation $\\mathbf y$，后续在预训练或 fine-tunning 中完成分类任务就是在 $\\mathbf z_L^0$ 上添加一个 classification head Position Embedding 同样因为 transformer 的 permutation-invariant，需要在输入的时候嵌入位置信息，不同的是 ViT 中 position encoding 是可以学习的（图 Fig 10所示） 作者对比了几种不同的 position embedding： no positional information：将输入看做是一个 patches bag 1-D positional embedding：将输入看做具有前后顺序的 patches sequence 2-D postional embedding：将输入看做是二维的网格图，每一个 patch 需要两个坐标 embedding（X-embedding，Y-embedding） Relative positional embedding：将 patches 之间的相对距离用作空间信息的编码 发现后面 3 种 positional embedding 的差别并不是很大，这主要是因为 ViT 的输入是相对较大的 patchs 而不是pixels，所以学习位置信息相对容易很多。所以，作者在文中的实验默认使用 1-D positional embedding Transformer Encoder \r \\begin{align}\r &\\mathbf z_0=[\\mathbf x_{class};\\mathbf x_p^1\\mathbf E;\\mathbf x_p^2\\mathbf E;\\cdots;\\mathbf x_p^N\\mathbf E]+\\mathbf E_{pos}\\\\\r &\\mathbf z_{\\ell}'=MSA(LN(\\mathbf z_{\\ell-1}))+\\mathbf z_{\\ell-1}\\\\\r &\\mathbf z_{\\ell}=MLP(LN(\\mathbf z_{\\ell}'))+\\mathbf z_{\\ell}'\\\\\r &\\mathbf y=LN(\\mathbf z_L^0)\r \\end{align}\r 其中，MSA 表示 multi-head selfattention Experiment 下面我们来看看 ViT 的第二个特点：超大数据集训练 在数据集上选用了三个规模递增的数据集：mageNet（1k classes，1.3M images），ImageNet-21k（21k classes，14M images）和 JFT（18k classes，303M high-resolution images） 同时，作者设计了三个版本的 ViT，如下表所示。同时，patch 大小也有三种方案 $14\\times14、16\\times16、32\\times32$，因此记 ViT-B/16，表示使用 ViT-Base model，patch大小为 $16\\times16$ 在预训练阶段，图片大小统一是 $224\\times224$ ，所以如果patch大小为 $14\\times14$，则每张图片对应于一个 $16\\times16$ 的序列，这大概就是论文题目的来源 通过下图我们发现，对中等规模的数据集（例如，ImageNet）进行训练时，此类模型所产生的适度精度要比同等规模的 ResNet 低几个百分点。 这种看似令人沮丧的结果是预料之中的：transformer 缺乏 CNN 固有的一些归纳偏置，例如平移不变性和局部感知性，因此在训练不足的数据量时不能很好地概括 但是，如果在更大的数据集上训练模型（14M - 300M图像），则模型表现会发生变化。 我们发现 ViT 经过足够的预训练并转移到数据点较少的任务时，可以获得出色的结果。在 ImageNet-21k 或 JFT-300M 上进行预训练时，ViT 在多个图像识别基准上达到或超越了最新水平。特别是，最佳模型在 ImageNet 上达到 88.55％ 的精度，在 ImageNet-ReaL 上达到 90.72％ 的精度，在 CIFAR-100 上达到 94.55％ 的精度，在 19 个任务的 VTAB 上达到 77.63％ 的精度（如下表所示） ViT 这样的表现得益于 transformer 的一个特色——scalability，当模型和数据量提升时，性能持续提升。所以，在大数据面前，ViT 会发挥出更大的优势 此外，论文中也对 ViT 做了进一步分析，如分析了不同层的平均 attention distance，这个类比于 CNN 的感受野。可以发现前面层的『感受野』虽然差异很大，但是总体相比后面层『感受野』较小，而模型后半部分『感受野』基本覆盖全局，和 CNN 比较类似，说明 ViT 最后也学习到了类似的范式 最后，在NLP 领域中，BERT、GPT 等都使用了自监督学习进行预训练，而本文的 ImageNet-21k、JFT 都属于有标签数据，通过实验也证明了监督预训练得到的 ViT 效果不错。那么更进一步，如果用自监督学习训练 ViT，效果会不会如何呢？ 作者也进行了一点尝试，自监督学习的重点是设计一个合理的代理任务，比如 BERT 使用的掩码语言模型任务（MLM），作者参考了BERT，提出了 masked patch prediction 任务，也就是对某些patch进行mask，然后使用其他 patch 对其预测 在 JFT 数据集上进行 masked patch prediction，然后在 ImageNet 上微调，结果怎么样呢？一个坏消息和一个好消息： 坏消息是，相比于监督预训练，效果低了 4 个点 好消息是，相比于直接在 ImageNet 上训练模型，效果高了 2 个点 由于这个模块不是 ViT 的重点，作者也提出后续可以尝试对比学习进行预训练，总之大概就是这样，结束啦 Reference [论文笔记] ViT ViT: 简简单单训练一个Transformer Encoder做个图像分类 \"未来\"的经典之作ViT：transformer is all you need! 一文带你掌（放）握（弃）ViT(Vision Transformer)（原理解读+实践代码） ViT "},"【论文】VL-BERT.html":{"url":"【论文】VL-BERT.html","title":"VL-BERT","keywords":"","body":" 【论文】Su W, Zhu X, Cao Y, et al. Vl-bert: Pre-training of generic visual-linguistic representations.（pdf） 从 VL-BERT 的结构来看应该还是属于 one-stream model，但是和我们见过的 UNITER、VisualBERT 相比，VL-BERT 在输入上做了一个极大的创新——将图像和文本的 embedding 融合在一起 在输入上 VL-BERT 做出的改进有如下几点： 为了解决图片和文本无法直接对齐的问题，作者直接暴力地将整张图片的 embedding 作为输入，与 token embedding 进行融合，如下图中的 1-6 考虑到整张图片的粒度远大于文本 token，一次性输入整张图片的 embbeding 显然不利于图像和文本信息的交互。 所以，又使用了 Faster RCNN 的 detector 对图片进行目标检测，提取图像中的 RoI，加上 [IMG] 标识，输入到模型中 为了不失掉全局信息，在 [END] 对应的位置又加上了整张图像的 embedding token embedding 的设置同 BERT；position embedding 用于区分输入的顺序，主要是针对文本序列，对于不同的图像区域输入没有顺序可言，故 position embedding 一样；segment embedding 分三类 A、B、C，区分 embedding 的来源——句子 A、句子 B 和图像。 注意到 1-6 和 7 的 visual feature embedding 不同，分两种情况：（1）对于输入 RoI 的元素，使用 Faster RCNN 得到的每个 RoI feature 做嵌入；（2）对输入整张图像的元素，使用覆盖整个图像的 RoI feature 做嵌入。Geometry Embedding 就相当于图像区域的 position embedding，是一个 4 维的向量，记录 bbox 左上和右下的相对坐标位置 VL-BERT 中只设计了两种预训练任务： Masked Language Model with Visual Clues 根据文本 + 图像信息预测 masked token，比如上图中的例子，通过图片给出的视觉信息预测被遮掉的词是 bottle Masked RoI Classification with Linguistic Clues 根据文本 + 图像信息预测 masked RoIs 的类别。需要注意的是，由于模型在 token embedding 中会接收整张图片的 embedding，为了避免信息泄露，对 1-6 中的整张图片也要将相应的部分遮罩掉（如上图中遮掉 kitten 对应的 RoI）。最后，模型根据文本信息和剩下的图片信息预测遮掉区域所属类别 特别的是，作者发现如果在 VL-BERT 中引入 sentence-image relation prediction 任务会降低 VL-BERT 在下游任务的表现。论文中作者给出的解释是 sentence-image relation prediction 中会引入图像文本对不匹配的负样本，这些负样本的引入会影响模型在其他任务上的表现（那这么说 VL-BERT 只能在好的样本间学习 generic visual-linguistic representation，而无法使用学到的视觉语言关系去区分不好的样本，也就是看着答案做题还行，变通一下就出问题？） 下图是 VL-BERT 在 3 个下游任务上的具体工作 Reference 当NLPer爱上CV：后BERT时代生存指南之VL-BERT篇 "},"【论文】VSLNet.html":{"url":"【论文】VSLNet.html","title":"VSLNet","keywords":"","body":" 【论文】Zhang H, Sun A, Jing W, et al. Span-based localizing network for natural language video localization（pdf） 有两种方法处理 temporal grounding 任务， 一种做 ranking task，采用 multimodal matching 的结构 一种做 regression task，直接回归 target 作者在标准的 span-based QA framework 基础上引入了一个 query-guided highlighting（QGH）strategy来处理 temporal grounding 和 span-based QA 之间大的差别 QA 就是根据给定的 language query 去 untrimmed video 中找一个 video moment 作为 answer，作者认为这里主要有两个问题： 视频帧和帧之间的联系性会很紧密，而句子两个相邻的词也可能存在很大的差别 我们对帧之间的差异并不是很敏感，一些很小的 offset 并不影响对视频内容的理解，但是句子中一个词的变化可能会有完全不同的含义 作者先在标准的 span-based QA framework 上跑了一个 baseline（VSLBase），但是 VSLBase 并没有解决上面的两点问题，所以进一步提出了 VSLNet。在 QGH strategy 中，作者定义和 target moment 比邻的内容为 foreground，其他的则是 background，于是 foreground 的往往会比 answer span 大一些。首先，long region 能够提供一些有效的内容定位 answer span，然后 highlighted region 就可以局限在这一个片段中区分不同帧之间的信息，得到更准确的定位结果 为了让时间和 visual feature sequence 能对应起来，要先对 moment annotations 进行一点处理，设 $\\tau^s,\\tau^e$ 表示起始时间和终止时间，visual feature $\\textbf V=\\left{\\textbf vi\\right}{i=1}^n$，转换后的 start (end) span index 记为 $a^{s(e)}=\\left$，其中 $\\mathcal T$ 表示视频的总时长， $\\left$ 表示取整操作，于是，在推理的时候，预测的 span boundary 就可以很容易转换为时间点 $\\tau^{s(e)}=a^{s(e)}/n\\times\\mathcal T$ span-based QA framework 输入采取一个三元组 $ (Context,Question,Answer)$，于是 VSLBase 的输入就可以表示为 $(\\textbf V, Q, \\textbf A)$，$\\textbf V=[\\textbf v1, \\textbf v_2,\\cdots,\\textbf v_n]$，$\\textbf A=[\\textbf v{a^s}, \\textbf v{a^s+1},\\cdots,\\textbf v{a^e}]$ 下面是 VSLBase 和 VSLNet 的结构，从 feature extractor 得到的特征 $\\mathbf V$ 和 $\\mathbf Q$ 先经过一个映射层得到相同维度的 token，$\\mathbf V'\\in\\mathbb R^{n\\times d},\\mathbf Q'\\in\\mathbb R^{m\\times d}$，然后再输入到 feature encoder 中，feature encoder 采用 QANet，这里只采用了一层 encoder block Context-Query Attention 作者采用 context-query attention (CQA) 模块来进行跨模态的交互，CQA 先计算相似度得分，$\\mathcal S\\in\\mathbb R^{n\\times m}$，然后生成 context-to-query ($\\mathcal A$) and query-to-context ($\\mathcal B$) attention weights $\\mathcal S_r, \\mathcal S_c$ 分别是 row- and column-wise normalization of $\\mathcal S$ by SoftMax 最终，CQA 的输出就可以记为 Conditioned Span Predictor Conditioned span predictor 由两个 unidirectional LSTMs 和两个全连接层组成，作者这里采用 unidirectional LSTM 的原因是比起 bidirectional LSTM 并没有太大的表现差距，但是参数能少一些。两个 LSTM 串联，这样第二个 LSTM 在预测终止点的时候就可以用上起始点的信息。最终，将两个 LSTM 的 hidden states 输入全连接层预测结果 时间边界的概率分布 $P_{s(e)}=\\text{SoftMax}(\\textbf S^{s(e)})\\in\\mathbb R^n$，模型 loss 如下所示 $Y_s,Y_e$ 表示时间边界转化后的帧序号 $a^s, a^e$ 推理的时候，预测的 answer span $(\\hat a^s, \\hat a^e)$ 取联合概率最大的结果，即 Query-Guided Highlighting 前面介绍了 foreground 和 background 的概念，QGH 通过超参数 $\\alpha$ 决定，foreground 比 answer span 多出来的部分。foreground 置 1 background 置 0，就可以得到一个 0-1 序列 $Y_h$，那么 QGH 就可以视作一个二分类的模块，预测 visual feature 属于 foreground 或 background 的置信度 QGH 的结构如下图所示，首先把 $\\tilde{ \\mathbf Q}$ 映射为一个 sentence representation $\\mathbf h_Q$，然后将 $\\mathbf h_Q$ 和 $\\mathbf V^q$ cat 在一起得到 $\\bar{\\mathbf V}^q$，于是，这里的 highlighting score 就是 $\\mathcal S_h=\\sigma\\left(\\text{Conv1D}(\\bar{\\mathbf V}^q)\\right)$，最终的输出为 $\\tilde{\\mathbf V}^q=\\mathcal S_h\\cdot\\bar{\\mathbf V}^q$ 前面讲过 QGH 就是一个分类器，其 loss 记为 $\\mathcal L{QGH}=f{CE}(Sh, Y_h)$，整个模型的 loss 就是 $\\mathcal L=\\mathcal L{span} + \\mathcal L_{QGH}$ 下面是一些实验结果，我们先不做细致讨论 "},"【论文】VT-T5.html":{"url":"【论文】VT-T5.html","title":"VT-T5","keywords":"","body":" 【论文】Cho J, Lei J, Tan H, et al. Unifying vision-and-language tasks via text generation.（pdf） 作者的思路很简单，希望通过一种方法将所有的下游任务用一个模型解决。不同于以往针对下游任务微调 task head 的方法，作者通过 generate labels in text 通过同一个目标函数（maximum likelihood estimation）训练得到一个 unified language modeling architecture 解决所有问题（i.e. visual question answering，referring expression comprehension，natural language visual reasoning，visual commonsense reasoning 和 image captioning） generate labels in text 的方法如 Fig 2. 所示，特别地，针对 grounding task，作者采用直接预测其 region id 的方式，如 即是一种表示 region id 的特殊 text token 那么，将所有任务的预测形式统一了，怎么让一个统一的模型能感知到不同问题需要不同的预测输出呢？这里，作者采用了一个 prefix prompt，如 Fig 2. 中的 『visual grounding :』，可以通过改变前缀让模型感知到针对不同任务的预测输出 作者的两个创新点基本就介绍完了，我们对照模型框架来具体了解一下作者的做法 考虑前面 grounding 提到的办法，作者在 visual embedding 的时候编码了 4 部分内容：（1）RoI features；（2） bbox coordinates；（3）image ids $\\in\\left{1,2\\right}$；（4）region ids $\\in\\left{1,\\cdots,n\\right}$ 其中，RoI features 和 bbox coordinates 通过线性层进行编码，image ids 和 region ids 通过 BERT 中采用的 learnable embedding 进行编码。考虑到 NLVR 输入会有两张图片，为了区分图像区域是来自哪张图片，所以引进了 image ids 关于 text embedding，因为作者选用的 T5 和 BART 两种 LM 对 position 的编码方式不同（T5 采用相对位置编码，BART 采用绝对位置编码），作者保留各自的编码方式，用到哪一种 LM 时采用其相对应的 position embedding 介于 region id 的引入，作者在 T5 和 BART 原始词汇表的基础上加入了一些 visual sentinel tokens，即 $\\left{\\text{},\\dots,\\text{}\\right}$，这些 tokens 的 text embedding 结果就作为 region ids的编码结果 另外，值得注意的是 VL-T5（VT-T5/BART 即作者自己的模型）采用的是 T5 的相对位置编码，图像区域之间没有所谓的先后顺序之分所以它们的位置编码设为 0 至于 LM 的 encoder-decoder 结构我们就不再介绍了。我们介绍一下模型的目标函数，针对预训练和下游任务，VL-T5 都使用一个统一的目标函数——通过最小化 label text $y$ 和输入文本 $x$、图像 $v$ 的 negative log-likelihood 来优化参数 $\\theta$，即 \r \\mathcal L_{\\theta}^{GEN}=-\\sum_{j=1}^{|y|}\\log P_{\\theta}(y_j|y_{ 如下图所示，与以往 task head 的方式不同，VT-T5 直接输出预测的 label text Table 1. 展示了 VL-T5 预训练任务和下游任务在 generate labels in text 方法下的一些例子 EXperiments VL-T5 的表现和其他采用 task head 模型的表现持平，其他就一般般吧，没有特别的亮点。我们甚至不认为 VT-T5 是一种通过多模态预训练模型，而更像是一种根据下游任务数据特地训练出来的一种任务通用模型 "},"【论文】Where does it exist.html":{"url":"【论文】Where does it exist.html","title":"Where does it exist","keywords":"","body":" 【论文】Zhang Z, Zhao Z, Zhao Y, et al. Where does it exist: Spatio-temporal video grounding for multi-form sentences.（pdf） 文章提出一种新的任务叫做 Spatio-Temporal Video Grounding for Multi-Form Sentences (STVG)，如下图所示，给定一个 untrimmed video 和一句描述物体的陈述句或疑问句（clarative/interrogative sentence），STVG 旨在定位出 queried object 的 spatio-temporal tube STVG 与以往任务不同的是， 定位是在 untrimmed videos 上进行，因此 queired object 只会存在在视频中很短的片段中，因此 temporal grounding 的有效性是非常重要的 STVG 采用了 multi-form sentences，不仅通过陈述句定位一些 explicit objects，同时还通过疑问句定位一些 unknown objects，而需要通过 unknown objects 与其他 objects 的关系来推理出所需要定位的内容 于是，以往基于 trimmed videos 提取 spatio-temporal tubes 的方法可能并不适用于 STVG，为了解决这个任务，作者提出了一个新的 Spatio-Temporal Graph Reasoning Network（STGRN），用以捕捉 temporal object dynamics 之间的 region 关系，同时不基于任何的 tube pre-generation 来定位 spatio-temporal tubes 首先，STGRN 将视频变为一个 spatio-temporal region graph，考虑到 text 描述中会涉及到 object actions，所以进一步引入了一个 temporal subgraph 来捕捉 temporal dynamics information，于是完整的 spatio-temporal region graph 共包含两个部分： patial Relation Graph：包含 implicit and explicit spatial subgraphs 捕捉 region-level relationship Temporal Dynamic Graph：捕捉帧之间的 dynamics and transformation 然后，作者将文本线索（textual clue）加入到 spatio-temporal region graph 作为一种引导，再通过在 spatio-temporal graph encoder 中经过 multi-step cross-modal graph reasoning 获得良好的 cross-modal understanding，最后 spatio-temporal localizer 直接定位 object tube，具体来说，先通过一个 temporal localizer 定位时间边界，然后再通过一个 spatial localizer 定位 object Video and Text Encoder 首先用 pre-trained Faster R-CNN 对视频每一帧提取 $K$ 个 region，即对 $t$-th frame 得到 region 表示 $\\left{r^ti\\right}^K{i=1}$，其中对应 region $r^ti$ 的 visual feature 表示为 $\\textbf r_i^t\\in\\mathbb R^{d_r}$，bbox 表示为 $\\textbf b_i^t=[x_i^t, y_i^t, w_i^t, h_i^t]$；同时，还通过 Faster R-CNN 获取整个帧的 frame feature 记为 $\\left{\\textbf f^{\\ t}\\right}^N{i=1}$ 对于句子 $s$，通过 BiGRU 可以得到 word semantic features $\\left{\\textbf si\\right}^L{i=1}\\in\\mathbb R^{d_s}$，也即文中提到的 textual clue，其中 $\\textbf s_i$ 是 step $i$ forward and backward hidden states 的 catenation。接下来，我们要获取一个 query representation 用于定位（即 Fig 2. 中 Language Extractor 出来的特征），方法是选取一个 entity feature 和 textual clue 按照一种注意力的方式聚合在一起由此来形成 query representation，亦即下面的 entity-aware feature 由于 STVG 会给出两种形式的句子，entity feature 的选取方式不同，对于陈述句，从 $\\left{\\textbf si\\right}^L{i=1}$ 选择一个作为 entity feature $\\textbf s^e$ 表示 queried object，例如上图表示 \"boy\" 的 $\\textbf s_3$ ；对于疑问句，选择 \"who\" 或者 \"what\" 的 feature 作为 entity feature Spatio-Temporal Graph Encoder 输入的 video 先被处理成一个 spatio-temporal region graph，如前所述包含每一帧的 implicit spatial subgraph $\\mathcal G{imp}=(\\mathcal V,\\mathcal E{imp})$ 和 explicit spatial subgraph $\\mathcal G{exp}=(\\mathcal V,\\mathcal E{exp})$，以及多帧交互的 temporal dynamic subgraph $\\mathcal G{tem}=(\\mathcal V,\\mathcal E{tem})$ 将每一帧的全连接 region graph 作为 implicit spatial graph，其中 $\\mathcal E_{imp}$ 包含 $K\\times K$ 条无向边（不带 label） explicit spatial subgraph 则采用三元组 $\\lefti,p^t{ij},r^tj\\right>$ 来构建图，其中 $r^t$ 表示 frame $t$ 中的两个 region，$p^t{ij}$ 表示两个 region 的关系，每一个三元组就表示一条边。于是，explicit graph 的构建就可以视为一个关系分类任务。具体来说，给定两个 region $[\\textbf r^ti;\\textbf b_i^t],[\\textbf r^t_j;\\textbf b_j^t]$，以及联合的 region $[\\textbf r^t{ij};\\textbf b{ij}^t]$，然后将三个 region 的 feature 经过不同的 linear layer，然后 cat 在一起再通过一个 classification layer 预测关系。Classifier 在 VG 上训练好，取训练集中最常用的 top-50 predicates，另外增加一个 no_relation class。最终，$\\mathcal E{exp}$ 有 3 种边 $i\\text{-to-}j,j\\text{-to-}j,i\\text{-to-}i$ 和 51 种 label 在 temporal dynamic subgraph 中将不同帧相同帧的相同 object 连起来，对于 frame $t$，把它和相邻的 $2M$ 帧（前面 $M$ 帧，后面 $M$ 帧）连接起来。考虑到时间间隔越大的帧相关性越小，作者还设计了一个 linking score $s(r^t_i,r^k_j)$ 来计算两个帧相同 object 的联系性 其中 $\\text{cos(}\\cdot\\text{)}$ 表示余弦相似度，$\\epsilon$ 是一个平衡参数。在这个公式中同时考虑两个 region 的 appearance similarity 和 spatial overlap ratio，$|k-t|$ 用于控制 IoU score，对于时间间隔较大的帧，linking score 就主要取决于 appearance similarity，于是，对于 $r^ti$ 选取 linking score 最大创建一条边。在 temporal dynamic subgraph 中每个 region 就有 $2M+1$ 条边（这句话是照着原文翻的，但是感觉应该原文写错了）。同样，在 $\\mathcal E{tem}$ 中有 3 种边：: forward, backward and self-loop 讲完建图，下面就是怎么进行 multi-step cross-modal graph reasoning spatio-temporal graph encoder 中包含三个部分：cross-modal fusion、spatial graph convolution 和 temporal graph convolution cross-modal fusion 用于将 textual clues 揉到 spatio-temporal graph 中，具体来说对 region $r^ti$，通过 word features $\\left{\\textbf s_i\\right}^L{i=1}$ 按照如下方式计算注意力权重 接着建立 textual gate 用 text information 作为引导来激活 text-irrelevant regions 最后，将 filtered region feature 和 textual feature cat 起来构成 cross-model region features $\\left{\\left{\\textbf v^ti\\right}{i=1}^K\\right}_{t=1}^N$，cross-modal fusion 之后通过 $T$ spatio-temporal convolution layers 进行 multi-step graph reasoning 在 spatio-temporal convolution 的每一层中先通过 spatial graph convolution 来捕捉每一帧的 visual relationships，具体来说，对于 $\\left{\\left{\\textbf v^ti\\right}{i=1}^K\\right}{t=1}^N$，先对 $\\mathcal G{imp}$ 应用 implicit graph convolution 同样也有 explicit graph convolution，与 original undirected GCN 不同的是这里需要考虑边的方向和 label temporal graph convolution 作用在 $\\mathcal G_{tem}$ 上，方法如下 Spatio-Temporal Localizer Temporal localizer 估计一组 candidates clips，然后调整他们的边界获得最终的 temporal grounding 结果 每一帧的 region feature 可以通过下面的计算方式得到，其中 $\\textbf m^t$ 表示 frame $t$ 的 relation-aware feature 将上面得到的特征和之前的 global frame features $\\left{\\textbf f^{\\ t}\\right}^N{i=1}$ cat 在一起，再通过一个 BiGRU 得到最终的 frame features $\\left{\\textbf h^{t}\\right}^N{i=1}$ 接着，作者在每个时间步 $t$ 定义了一组 multi-scale candidate clips $B^t=\\left{(s^ti, e^t_i)\\right}^P{i=1}$，其中 $(s^t_i, e^t_i)=(t-w_i/2,t+w_i/2)$，$w_i$ 表示 temporal length。最终，通过一个线性层对所有的 candidate clips 进行回归和预测 offsets Temporal localizer 有两个损失： 一个用于 clip 选择的 alignment loss，计算 temporal IoU $\\hat C_i^t$ 一个回归 boundary adjustments，微调具有最高 $\\hat C_i^t$ 的 clip，其中 $\\hat \\delta_s=s-\\hat s$，同理 $\\hat\\delta_e$ 对于 spatial localizer，直接将 region feature $\\left{\\textbf mi^t\\right}{i=1}^K$ 和 query representation $\\textbf s^q$、frame feature $\\textbf h^t$ 整合在一起，计算 matching scores loss 就计算 IoU score $\\hat S^t_i$ 即可，其中 $\\mathcal S_t$ 表示 temporal gt 所对应的帧 最终整个模型的 loss 就是 Dynamic Selection Method 在推理的时候，先定位 tube 的 temporal boundaries $(T_s,T_e)$，然后再逐帧定位 grounded region，基于贪心的方法是选择 $S_i^t$ 最高的，但是这么做有一个问题，生成的 tube 不是太流畅，两帧之间的 bboxes 可能会有很大的偏移，因此，为了让生成的 tube 更流畅一些，作者采取了一种 dynamic selection method 还是采用 linking score 的思路，计算相邻两帧 $t,t+1$ regions 之间的联系程度 然后通过最大化 $E$ 来生成最终的 spatio-temporal tube $Y$ VidSTG 数据集介绍，视频的平均长度在 28.01s，tube 的 temporal length 在 9.68s，declarative sentences 的平均长度是 11.12，interrogative sentences 的平均长度为 8.98 "}}